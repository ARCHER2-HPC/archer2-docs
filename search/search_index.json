{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"ARCHER2 User Documentation ARCHER2 is the next generation UK National Supercomputing Service. You can find more information on the service and the research it supports on the ARCHER2 website . The ARCHER2 Service is a world class advanced computing resource for UK researchers. ARCHER2 is provided by UKRI , EPCC , Cray (an HPE company) and the University of Edinburgh . What the documentation covers This is the documentation for the ARCHER2 service and includes: Quick Start Guide The ARCHER2 quick start guide provides the minimum information for new users. ARCHER2 User and Best Practice Guide Covers all aspects of use of the ARCHER2 supercomputing service. This includes fundamentals (required by all users to use the system effectively), best practice for getting the most out of ARCHER2, and other advanced technical topics. Research Software Information on each of the centrally-installed research software packages. Software Libraries Information on the centrally-installed software libraries. Most libraries work as expected so no additional notes are required however a small number require specific documentation Data Analysis and Tools Information on data analysis tools and other useful utilities. Other Software Useful information on software that is not officially supported by the ARCHER2 service but that will be useful to users of that software. Essential Skills This section provides information and links on essential skills required to use ARCHER2 efficiently: e.g. using Linux command line, accessing help and documentation. ARCHER2 and publications This section describes how to acknowledge the use of ARCHER2 in your published work and how to use the ARCHER2 publications database. Contributing to the documentation The source for this documentation is publicly available in the ARCHER2 documentation Github repository so that anyone can contribute to improve the documentation for the service. Contributions can be in the form of improvements or addtions to the content and/or addtion of Issues providing suggestions for how it can be improved. Full details of how to contribute can be found in the README.md file of the repository. Credits This documentation draws on the Cirrus Tier-2 HPC Documentation , Sheffield Iceberg Documentation and the ARCHER National Supercomputing Service Documentation .","title":"Documentation overview"},{"location":"#archer2-user-documentation","text":"ARCHER2 is the next generation UK National Supercomputing Service. You can find more information on the service and the research it supports on the ARCHER2 website . The ARCHER2 Service is a world class advanced computing resource for UK researchers. ARCHER2 is provided by UKRI , EPCC , Cray (an HPE company) and the University of Edinburgh .","title":"ARCHER2 User Documentation"},{"location":"#what-the-documentation-covers","text":"This is the documentation for the ARCHER2 service and includes: Quick Start Guide The ARCHER2 quick start guide provides the minimum information for new users. ARCHER2 User and Best Practice Guide Covers all aspects of use of the ARCHER2 supercomputing service. This includes fundamentals (required by all users to use the system effectively), best practice for getting the most out of ARCHER2, and other advanced technical topics. Research Software Information on each of the centrally-installed research software packages. Software Libraries Information on the centrally-installed software libraries. Most libraries work as expected so no additional notes are required however a small number require specific documentation Data Analysis and Tools Information on data analysis tools and other useful utilities. Other Software Useful information on software that is not officially supported by the ARCHER2 service but that will be useful to users of that software. Essential Skills This section provides information and links on essential skills required to use ARCHER2 efficiently: e.g. using Linux command line, accessing help and documentation. ARCHER2 and publications This section describes how to acknowledge the use of ARCHER2 in your published work and how to use the ARCHER2 publications database.","title":"What the documentation covers"},{"location":"#contributing-to-the-documentation","text":"The source for this documentation is publicly available in the ARCHER2 documentation Github repository so that anyone can contribute to improve the documentation for the service. Contributions can be in the form of improvements or addtions to the content and/or addtion of Issues providing suggestions for how it can be improved. Full details of how to contribute can be found in the README.md file of the repository.","title":"Contributing to the documentation"},{"location":"#credits","text":"This documentation draws on the Cirrus Tier-2 HPC Documentation , Sheffield Iceberg Documentation and the ARCHER National Supercomputing Service Documentation .","title":"Credits"},{"location":"archer-migration/","text":"ARCHER to ARCHER2 migration This section of the documentation is a guide for user migrating from ARCHER to ARCHER2. It covers: Migrating your account from ARCHER to ARCHER2 Migrating data from ARCHER to ARCHER2 Main differences between ARCHER and ARCHER2 Tip If you need help or have questions on ARCHER to ARCHER2 migration, please contact the ARCHER2 service desk","title":"ARCHER to ARCHER2 migration"},{"location":"archer-migration/#archer-to-archer2-migration","text":"This section of the documentation is a guide for user migrating from ARCHER to ARCHER2. It covers: Migrating your account from ARCHER to ARCHER2 Migrating data from ARCHER to ARCHER2 Main differences between ARCHER and ARCHER2 Tip If you need help or have questions on ARCHER to ARCHER2 migration, please contact the ARCHER2 service desk","title":"ARCHER to ARCHER2 migration"},{"location":"archer-migration/account-migration/","text":"Migrating your account from ARCHER to ARCHER2 This section covers the following questions: When will I be able to access ARCHER2? Has my project has been migrated from ARCHER to ARCHER2? How much resource will my project have on ARCHER2? How do I set up an ARCHER2 account? How do I log into ARCHER2 for the first time? Tip If you need help or have questions on ARCHER to ARCHER2 migration, please contact the ARCHER2 service desk When will I be able to access ARCHER2? We anticipate that users will have access during the week beginning 11th January 2021. Notification of activation of ARCHER2 projects will be sent to the project leaders/PIs and the project users. Has my project been migrated to ARCHER2? If you have an active ARCHER allocation at the end of the ARCHER service then your project will very likely be migrated to ARCHER2. If your project is migrated to ARCHER2 then it will have the same project code as it had on ARCHER. Some further information that may be useful: If you are a member of the EPSRC or NERC consortia on ARCHER then all of these consortia have been migrated to ARCHER2. A list of the consortia and their ARCHER/ARCHER2 codes can be found on the ARCHER2 website. If your project code begins with i or d then you are a member of an industrial project or a Director's Time project, these projects will be contacted individually to discuss arrangements. Please speak to your project leader or PI in the first instance. How much resource will my project have on ARCHER2? The unit of allocation on ARCHER2 is called the ARCHER2 Compute Unit (CU) and, in general, 1 CU will be worth 1 ARCHER2 node hour. UKRI have determined the conversion rates which will be used to transfer existing ARCHER allocations onto ARCHER2. These will be: 1.5156 kAU = 4.21 ARCHER node hour = 1 ARCHER2 node hour = 1 CU In identifying these conversion rates UKRI has endeavoured to ensure that no user will be disadvantaged by the transfer of their allocation from ARCHER to ARCHER2. A nominal allocation will be provided to all projects during the initial no-charging period. Users will be notified before the no-charging period ends. When the ARCHER service ends, any unused ARCHER allocation in kAUs will be converted to ARCHER2 CUs and transferred to ARCHER2 project allocation. How do I set up an ARCHER2 account? Once you have been notified that you can go ahead and setup an ARCHER2 account you will do this through SAFE. Note that you should use the new unified SAFE interface rather than the ARCHER SAFE. The correct URL for the new SAFE is: https://safe.epcc.ed.ac.uk Your access details for this SAFE are the same as those for the ARCHER SAFE. You should log in in exactly the same way as you did on the ARCHER SAFE. Important You should make sure you request the same account name in your project on ARCHER2 as you have on ARCHER. This is to ensure that you have seamless access to your ARCHER /home data on ARCHER2. See the ARCHER to ARCHER2 Data Migration page for details on data transfer from ARCHER to ARCHER2 Once you have logged into SAFE, you will need to complete the following steps before you can log into ARCHER2 for the first time: Request an ARCHER2 account through SAFE See: How to request a machine account (SAFE documentation) (Optional) Create a new SSH key pair and add it to your ARCHER2 account in SAFE See: SSH key pairs (ARCHER2 documentation) If you do not add a new SSH key to your ARCHER2 account, then your account will use the same key as your ARCHER account Collect your initial, one-shot password from SAFE See: Intial passwords (ARCHER2 documentation) How do I log into ARCHER2 for the first time? The ARCHER2 documentation covers logging in to ARCHER from a variety of operating systems: Logging in to ARCHER2 from macOS/Linux Logging in to ARCHER2 from Windows","title":"Migrating your account from ARCHER to ARCHER2"},{"location":"archer-migration/account-migration/#migrating-your-account-from-archer-to-archer2","text":"This section covers the following questions: When will I be able to access ARCHER2? Has my project has been migrated from ARCHER to ARCHER2? How much resource will my project have on ARCHER2? How do I set up an ARCHER2 account? How do I log into ARCHER2 for the first time? Tip If you need help or have questions on ARCHER to ARCHER2 migration, please contact the ARCHER2 service desk","title":"Migrating your account from ARCHER to ARCHER2"},{"location":"archer-migration/account-migration/#when-will-i-be-able-to-access-archer2","text":"We anticipate that users will have access during the week beginning 11th January 2021. Notification of activation of ARCHER2 projects will be sent to the project leaders/PIs and the project users.","title":"When will I be able to access ARCHER2?"},{"location":"archer-migration/account-migration/#has-my-project-been-migrated-to-archer2","text":"If you have an active ARCHER allocation at the end of the ARCHER service then your project will very likely be migrated to ARCHER2. If your project is migrated to ARCHER2 then it will have the same project code as it had on ARCHER. Some further information that may be useful: If you are a member of the EPSRC or NERC consortia on ARCHER then all of these consortia have been migrated to ARCHER2. A list of the consortia and their ARCHER/ARCHER2 codes can be found on the ARCHER2 website. If your project code begins with i or d then you are a member of an industrial project or a Director's Time project, these projects will be contacted individually to discuss arrangements. Please speak to your project leader or PI in the first instance.","title":"Has my project been migrated to ARCHER2?"},{"location":"archer-migration/account-migration/#how-much-resource-will-my-project-have-on-archer2","text":"The unit of allocation on ARCHER2 is called the ARCHER2 Compute Unit (CU) and, in general, 1 CU will be worth 1 ARCHER2 node hour. UKRI have determined the conversion rates which will be used to transfer existing ARCHER allocations onto ARCHER2. These will be: 1.5156 kAU = 4.21 ARCHER node hour = 1 ARCHER2 node hour = 1 CU In identifying these conversion rates UKRI has endeavoured to ensure that no user will be disadvantaged by the transfer of their allocation from ARCHER to ARCHER2. A nominal allocation will be provided to all projects during the initial no-charging period. Users will be notified before the no-charging period ends. When the ARCHER service ends, any unused ARCHER allocation in kAUs will be converted to ARCHER2 CUs and transferred to ARCHER2 project allocation.","title":"How much resource will my project have on ARCHER2?"},{"location":"archer-migration/account-migration/#how-do-i-set-up-an-archer2-account","text":"Once you have been notified that you can go ahead and setup an ARCHER2 account you will do this through SAFE. Note that you should use the new unified SAFE interface rather than the ARCHER SAFE. The correct URL for the new SAFE is: https://safe.epcc.ed.ac.uk Your access details for this SAFE are the same as those for the ARCHER SAFE. You should log in in exactly the same way as you did on the ARCHER SAFE. Important You should make sure you request the same account name in your project on ARCHER2 as you have on ARCHER. This is to ensure that you have seamless access to your ARCHER /home data on ARCHER2. See the ARCHER to ARCHER2 Data Migration page for details on data transfer from ARCHER to ARCHER2 Once you have logged into SAFE, you will need to complete the following steps before you can log into ARCHER2 for the first time: Request an ARCHER2 account through SAFE See: How to request a machine account (SAFE documentation) (Optional) Create a new SSH key pair and add it to your ARCHER2 account in SAFE See: SSH key pairs (ARCHER2 documentation) If you do not add a new SSH key to your ARCHER2 account, then your account will use the same key as your ARCHER account Collect your initial, one-shot password from SAFE See: Intial passwords (ARCHER2 documentation)","title":"How do I set up an ARCHER2 account?"},{"location":"archer-migration/account-migration/#how-do-i-log-into-archer2-for-the-first-time","text":"The ARCHER2 documentation covers logging in to ARCHER from a variety of operating systems: Logging in to ARCHER2 from macOS/Linux Logging in to ARCHER2 from Windows","title":"How do I log into ARCHER2 for the first time?"},{"location":"archer-migration/archer2-differences/","text":"Main differences between ARCHER and ARCHER2 This section provides an overview of the main differences between ARCHER and ARCHER2 along with links to more information where appropriate. For all users You use the new SAFE rather than the ARCHER SAFE You can add multiple SSH keys to your ARCHER2 account using SAFE There are 128 cores on an ARCHER2 compute node rather than 24 ARCHER2 usage is charged in CUs (Compute Units) rather than kAU Generally: 1.5156 kAU = 4.21 ARCHER node hour = 1 ARCHER2 node hour = 1 CU ARCHER2 uses the Slurm scheduler instead of PBS Pro See: Running jobs on ARCHER2 Parallel applications are launched using srun rather than aprun You cannot currently query your budget on ARCHER2 itself, you can view your budget using SAFE For users compiling and developing software on ARCHER2 The Intel compilers are not available on ARCHER2 ARCHER2 supports the Cray, Gnu and AMD compilers See: Application development environment Intel MKL libraries are not available on ARCHER2 Use Cray LibSci for BLAS/LAPACK/ScaLAPACK Use FFTW for FFTs All binaries on ARCHER2 are dynamically linked Static linking is currently not possible on ARCHER2 This means that all binaries must be installed on the /work file systems as these are the only file systems available on the compute nodes","title":"Main differences between ARCHER and ARCHER2"},{"location":"archer-migration/archer2-differences/#main-differences-between-archer-and-archer2","text":"This section provides an overview of the main differences between ARCHER and ARCHER2 along with links to more information where appropriate.","title":"Main differences between ARCHER and ARCHER2"},{"location":"archer-migration/archer2-differences/#for-all-users","text":"You use the new SAFE rather than the ARCHER SAFE You can add multiple SSH keys to your ARCHER2 account using SAFE There are 128 cores on an ARCHER2 compute node rather than 24 ARCHER2 usage is charged in CUs (Compute Units) rather than kAU Generally: 1.5156 kAU = 4.21 ARCHER node hour = 1 ARCHER2 node hour = 1 CU ARCHER2 uses the Slurm scheduler instead of PBS Pro See: Running jobs on ARCHER2 Parallel applications are launched using srun rather than aprun You cannot currently query your budget on ARCHER2 itself, you can view your budget using SAFE","title":"For all users"},{"location":"archer-migration/archer2-differences/#for-users-compiling-and-developing-software-on-archer2","text":"The Intel compilers are not available on ARCHER2 ARCHER2 supports the Cray, Gnu and AMD compilers See: Application development environment Intel MKL libraries are not available on ARCHER2 Use Cray LibSci for BLAS/LAPACK/ScaLAPACK Use FFTW for FFTs All binaries on ARCHER2 are dynamically linked Static linking is currently not possible on ARCHER2 This means that all binaries must be installed on the /work file systems as these are the only file systems available on the compute nodes","title":"For users compiling and developing software on ARCHER2"},{"location":"archer-migration/data-migration/","text":"Data migration from ARCHER to ARCHER2 This short guide explains how to move data from the ARCHER service to the ARCHER2 service. We have also created a walkthrough video to guide you. Note This section assumes that you have an active ARCHER and ARCHER2 account, and that you have successfully logged in to both accounts. Tip Unlike normal access, ARCHER to ARCHER2 transfer has been set up to require only one form of authentication. You will not need to generate a new SSH key pair to transfer data from ARCHER to ARCHER2 as your password will suffice. First, login to the ARCHER(1) (making sure to change auser to your username): ssh auser@login.archer.ac.uk Then, combine important research data into a single archive file using the following command: tar -czf all_my_files.tar.gz file1.txt file2.txt directory1/ Please be selective -- the more data you want to transfer, the more time it will take. From ARCHER in particular, in order to get the best transfer performance, we need to access a newer version of the SSH program. We do this by loading the openssh module: module load openssh Transferring data using rsync (recommended) Begin the data transfer from ARCHER to ARCHER2 using rsync : rsync -Pv -e\"ssh -c aes128-gcm@openssh.com\" \\ ./all_my_files.tar.gz a2user@transfer.dyn.archer2.ac.uk:/work/t01/t01/a2user Important Notice that the hostname for data transfer from ARCHER to ARCHER2 is not the usual login address. Instead, you use transfer.dyn.archer2.ac.uk . This address has been configured to allow higher performance data transfer and to allow access to ARCHER with password only with no SSH key required. When running this command, you will be prompted to enter your ARCHER2 password. Enter it and the data transfer will begin. Also, remember to replace a2user with your ARCHER2 username, and t01 with the budget associated with that username. The use of the -P flag to allow partial transfer -- the same command could be used to restart the transfer after a loss of connection. The -e flag allows specification of the ssh command - we have used this to add the location of the identity file. The -c option specifies the cipher to be used as aes128-gcm which has been found to increase performance. Unfortunately the ~ shortcut is not correctly expanded, so we have specified the full path. We move our research archive to our project work directory on ARCHER2. Transferring data using scp If you are unconcerned about being able to restart an interrupted transfer, you could instead use the scp command, scp -c aes128-gcm@openssh.com all_my_files.tar.gz \\ a2user@transfer.dyn.archer2.ac.uk:/work/t01/t01/a2user/ but rsync is recommended for larger transfers. Important Notice that the hostname for data transfer from ARCHER to ARCHER2 is not the usual login address. Instead, you use transfer.dyn.archer2.ac.uk . This address has been configured to allow higher performance data transfer and to allow access to ARCHER with password only with no SSH key required.","title":"Data migration from ARCHER to ARCHER2"},{"location":"archer-migration/data-migration/#data-migration-from-archer-to-archer2","text":"This short guide explains how to move data from the ARCHER service to the ARCHER2 service. We have also created a walkthrough video to guide you. Note This section assumes that you have an active ARCHER and ARCHER2 account, and that you have successfully logged in to both accounts. Tip Unlike normal access, ARCHER to ARCHER2 transfer has been set up to require only one form of authentication. You will not need to generate a new SSH key pair to transfer data from ARCHER to ARCHER2 as your password will suffice. First, login to the ARCHER(1) (making sure to change auser to your username): ssh auser@login.archer.ac.uk Then, combine important research data into a single archive file using the following command: tar -czf all_my_files.tar.gz file1.txt file2.txt directory1/ Please be selective -- the more data you want to transfer, the more time it will take. From ARCHER in particular, in order to get the best transfer performance, we need to access a newer version of the SSH program. We do this by loading the openssh module: module load openssh","title":"Data migration from ARCHER to ARCHER2"},{"location":"archer-migration/data-migration/#transferring-data-using-rsync-recommended","text":"Begin the data transfer from ARCHER to ARCHER2 using rsync : rsync -Pv -e\"ssh -c aes128-gcm@openssh.com\" \\ ./all_my_files.tar.gz a2user@transfer.dyn.archer2.ac.uk:/work/t01/t01/a2user Important Notice that the hostname for data transfer from ARCHER to ARCHER2 is not the usual login address. Instead, you use transfer.dyn.archer2.ac.uk . This address has been configured to allow higher performance data transfer and to allow access to ARCHER with password only with no SSH key required. When running this command, you will be prompted to enter your ARCHER2 password. Enter it and the data transfer will begin. Also, remember to replace a2user with your ARCHER2 username, and t01 with the budget associated with that username. The use of the -P flag to allow partial transfer -- the same command could be used to restart the transfer after a loss of connection. The -e flag allows specification of the ssh command - we have used this to add the location of the identity file. The -c option specifies the cipher to be used as aes128-gcm which has been found to increase performance. Unfortunately the ~ shortcut is not correctly expanded, so we have specified the full path. We move our research archive to our project work directory on ARCHER2.","title":"Transferring data using rsync (recommended)"},{"location":"archer-migration/data-migration/#transferring-data-using-scp","text":"If you are unconcerned about being able to restart an interrupted transfer, you could instead use the scp command, scp -c aes128-gcm@openssh.com all_my_files.tar.gz \\ a2user@transfer.dyn.archer2.ac.uk:/work/t01/t01/a2user/ but rsync is recommended for larger transfers. Important Notice that the hostname for data transfer from ARCHER to ARCHER2 is not the usual login address. Instead, you use transfer.dyn.archer2.ac.uk . This address has been configured to allow higher performance data transfer and to allow access to ARCHER with password only with no SSH key required.","title":"Transferring data using scp"},{"location":"archer2-migration/","text":"ARCHER2 4-cabinet system to ARCHER2 full system migration This section of the documentation is a guide for user migrating from the ARCHER2 4-cabinet system to the ARCHER2 full system. It covers: Migrating your account from ARCHER2 4-cab to full ARCHER2 Migrating data from ARCHER2 4-cab to full ARCHER2 Porting applications from ARCHER2 4-cab to full ARCHER2 Main differences between ARCHER2 4-cab and full ARCHER2 Tip If you need help or have questions on ARCHER2 4-cab to full ARCHER2 migration please contact the ARCHER2 service desk","title":"ARCHER2 4-cabinet system to ARCHER2 full system migration"},{"location":"archer2-migration/#archer2-4-cabinet-system-to-archer2-full-system-migration","text":"This section of the documentation is a guide for user migrating from the ARCHER2 4-cabinet system to the ARCHER2 full system. It covers: Migrating your account from ARCHER2 4-cab to full ARCHER2 Migrating data from ARCHER2 4-cab to full ARCHER2 Porting applications from ARCHER2 4-cab to full ARCHER2 Main differences between ARCHER2 4-cab and full ARCHER2 Tip If you need help or have questions on ARCHER2 4-cab to full ARCHER2 migration please contact the ARCHER2 service desk","title":"ARCHER2 4-cabinet system to ARCHER2 full system migration"},{"location":"archer2-migration/account-migration/","text":"Accessing the ARCHER2 full system This section covers the following questions: When will I be able to access ARCHER2 full system? Has my project been enabled on ARCHER2 full system? How much resource will my project have on ARCHER2 full system? How do I set up an account on the full system? How do I log into the different ARCHER2 systems? Tip If you need help or have questions on using ARCHER2 4-cabinet system and ARCHER2 full system please contact the ARCHER2 service desk When will I be able to access ARCHER2 full system? We anticipate that users will have access from mid-late November. Users will have access to both the ARCHER2 4-cabinet system and ARCHER2 full system for at least 30 days. UKRI will confirm the dates and these will be communicated to you as they are confirmed. There will be at least 14 days notice before access to the ARCHER2 4-Cabinet system is removed. Has my project been enabled on ARCHER2 full system? If you have an active ARCHER2 4-cabinet system allocation on 1st October 2021 then your project will be enabled on the ARCHER2 full system. The project code is the same on the full service as it is on ARCHER2 4-cabinet system. Some further information that may be useful: If you are a member of the EPSRC or NERC consortia on ARCHER2 then all of these consortia have been enabled on ARCHER2 full system. A list of the consortia and their ARCHER2 codes can be found on the ARCHER2 website. How much resource will my project have on ARCHER2 full system? The unit of allocation on ARCHER2 is called the ARCHER2 Compute Unit (CU) and 1 CU is equivalent to 1 ARCHER2 node hour. Your time budget will be shared on both systems. This means that any existing allocation available to your project on the 4-cabinet system will also be available on the full system. There will be a period of at least 30 days where users will have access to both the 4-cabinet system and the full system. During this time, use on the full system will be uncharged (though users must still have access to a valid, positive budget to be able to submit jobs) and use on the 4-cabinet system will be a charged in the usual way. Users will be notified before the no-charging period ends. How do I set up an account on the full system? You will keep the same usernames, passwords and SSH keys that you use on the 4-cabinet system on the full system. You do not need to do anything to enable your account, these will be made available automatically once access to the full system is available. You will connect to the full system in the same way as you connect to the 4-cabinet system except for switching the ordering of the credentials: 4-cabinet system: enter machine account password then passphrase for SSH key pair Full system: enter passphrase for SSH key pair then machine account password How do I log into the different ARCHER2 systems? The ARCHER2 documentation covers logging in to ARCHER2 from a variety of operating systems: - Logging in to ARCHER2 from macOS/Linux - Logging in to ARCHER2 from Windows Login addresses: ARCHER2 4-cabinet system: login-4c.archer2.ac.uk ARCHER2 full system: login.archer2.ac.uk Tip When logging into the ARCHER2 full system for the first time, you may see an error from SSH that looks like @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ @ WARNING: POSSIBLE DNS SPOOFING DETECTED! @ @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ The ECDSA host key for login.archer2.ac.uk has changed, and the key for the corresponding IP address 193.62.216.43 has a different value. This could either mean that DNS SPOOFING is happening or the IP address for the host and its host key have changed at the same time. Offending key for IP in /Users/auser/.ssh/known_hosts:11 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ @ WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED! @ @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY! Someone could be eavesdropping on you right now (man-in-the-middle attack)! It is also possible that a host key has just been changed. The fingerprint for the ECDSA key sent by the remote host is SHA256:UGS+LA8I46LqnD58WiWNlaUFY3uD1WFr+V8RCG09fUg. Please contact your system administrator. If you see this, you should delete the offending host key from your ~/.ssh/known_hosts file (in the example above the offending line is line #11) What will happen to ARCHER2 data? There are three file systems associated with the ARCHER2 Service: home file systems The home file systems will be mounted on both the 4-cabinet system and the full system; so users\u2019 directories are shared across the two systems. Users will be able to access the home file systems from both systems and no action is required to move data. The home file systems will be read and writeable on both services during the transition period. work file systems There are different work file systems for the 4-cabinet system and the full system. The work file system on the 4-cabinet system (labelled \u201carcher2-4c-work\u201d in SAFE) will remain available on the 4-cabinet system during the transition period. There will be new work file systems on the full system and you will have new directories on the new work file systems. Your initial quotas will typically be double your quotas for the 4-cabinet work file system. Important: you are responsible for transferring any required data from the 4-cabinet work file systems to your new directories on the work file systems on the full system. The work file system on the 4-cabinet system will be available for you to transfer your data from for at least 30 days from the start of the ARCHER2 full system access and 14 days notice will be given before the 4-cabinet work file system is removed. RDFaaS file systems For users who have access to the RDFaaS, your RDFaaS data will be available on both the 4-cabinet system and the full system during the transition period and will be readable and writeable on both systems.","title":"Accessing the ARCHER2 full system"},{"location":"archer2-migration/account-migration/#accessing-the-archer2-full-system","text":"This section covers the following questions: When will I be able to access ARCHER2 full system? Has my project been enabled on ARCHER2 full system? How much resource will my project have on ARCHER2 full system? How do I set up an account on the full system? How do I log into the different ARCHER2 systems? Tip If you need help or have questions on using ARCHER2 4-cabinet system and ARCHER2 full system please contact the ARCHER2 service desk","title":"Accessing the ARCHER2 full system"},{"location":"archer2-migration/account-migration/#when-will-i-be-able-to-access-archer2-full-system","text":"We anticipate that users will have access from mid-late November. Users will have access to both the ARCHER2 4-cabinet system and ARCHER2 full system for at least 30 days. UKRI will confirm the dates and these will be communicated to you as they are confirmed. There will be at least 14 days notice before access to the ARCHER2 4-Cabinet system is removed.","title":"When will I be able to access ARCHER2 full system?"},{"location":"archer2-migration/account-migration/#has-my-project-been-enabled-on-archer2-full-system","text":"If you have an active ARCHER2 4-cabinet system allocation on 1st October 2021 then your project will be enabled on the ARCHER2 full system. The project code is the same on the full service as it is on ARCHER2 4-cabinet system. Some further information that may be useful: If you are a member of the EPSRC or NERC consortia on ARCHER2 then all of these consortia have been enabled on ARCHER2 full system. A list of the consortia and their ARCHER2 codes can be found on the ARCHER2 website.","title":"Has my project been enabled on ARCHER2 full system?"},{"location":"archer2-migration/account-migration/#how-much-resource-will-my-project-have-on-archer2-full-system","text":"The unit of allocation on ARCHER2 is called the ARCHER2 Compute Unit (CU) and 1 CU is equivalent to 1 ARCHER2 node hour. Your time budget will be shared on both systems. This means that any existing allocation available to your project on the 4-cabinet system will also be available on the full system. There will be a period of at least 30 days where users will have access to both the 4-cabinet system and the full system. During this time, use on the full system will be uncharged (though users must still have access to a valid, positive budget to be able to submit jobs) and use on the 4-cabinet system will be a charged in the usual way. Users will be notified before the no-charging period ends.","title":"How much resource will my project have on ARCHER2 full system?"},{"location":"archer2-migration/account-migration/#how-do-i-set-up-an-account-on-the-full-system","text":"You will keep the same usernames, passwords and SSH keys that you use on the 4-cabinet system on the full system. You do not need to do anything to enable your account, these will be made available automatically once access to the full system is available. You will connect to the full system in the same way as you connect to the 4-cabinet system except for switching the ordering of the credentials: 4-cabinet system: enter machine account password then passphrase for SSH key pair Full system: enter passphrase for SSH key pair then machine account password","title":"How do I set up an account on the full system?"},{"location":"archer2-migration/account-migration/#how-do-i-log-into-the-different-archer2-systems","text":"The ARCHER2 documentation covers logging in to ARCHER2 from a variety of operating systems: - Logging in to ARCHER2 from macOS/Linux - Logging in to ARCHER2 from Windows Login addresses: ARCHER2 4-cabinet system: login-4c.archer2.ac.uk ARCHER2 full system: login.archer2.ac.uk Tip When logging into the ARCHER2 full system for the first time, you may see an error from SSH that looks like @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ @ WARNING: POSSIBLE DNS SPOOFING DETECTED! @ @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ The ECDSA host key for login.archer2.ac.uk has changed, and the key for the corresponding IP address 193.62.216.43 has a different value. This could either mean that DNS SPOOFING is happening or the IP address for the host and its host key have changed at the same time. Offending key for IP in /Users/auser/.ssh/known_hosts:11 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ @ WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED! @ @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY! Someone could be eavesdropping on you right now (man-in-the-middle attack)! It is also possible that a host key has just been changed. The fingerprint for the ECDSA key sent by the remote host is SHA256:UGS+LA8I46LqnD58WiWNlaUFY3uD1WFr+V8RCG09fUg. Please contact your system administrator. If you see this, you should delete the offending host key from your ~/.ssh/known_hosts file (in the example above the offending line is line #11)","title":"How do I log into the different ARCHER2 systems?"},{"location":"archer2-migration/account-migration/#what-will-happen-to-archer2-data","text":"There are three file systems associated with the ARCHER2 Service:","title":"What will happen to ARCHER2 data?"},{"location":"archer2-migration/account-migration/#home-file-systems","text":"The home file systems will be mounted on both the 4-cabinet system and the full system; so users\u2019 directories are shared across the two systems. Users will be able to access the home file systems from both systems and no action is required to move data. The home file systems will be read and writeable on both services during the transition period.","title":"home file systems"},{"location":"archer2-migration/account-migration/#work-file-systems","text":"There are different work file systems for the 4-cabinet system and the full system. The work file system on the 4-cabinet system (labelled \u201carcher2-4c-work\u201d in SAFE) will remain available on the 4-cabinet system during the transition period. There will be new work file systems on the full system and you will have new directories on the new work file systems. Your initial quotas will typically be double your quotas for the 4-cabinet work file system. Important: you are responsible for transferring any required data from the 4-cabinet work file systems to your new directories on the work file systems on the full system. The work file system on the 4-cabinet system will be available for you to transfer your data from for at least 30 days from the start of the ARCHER2 full system access and 14 days notice will be given before the 4-cabinet work file system is removed.","title":"work file systems"},{"location":"archer2-migration/account-migration/#rdfaas-file-systems","text":"For users who have access to the RDFaaS, your RDFaaS data will be available on both the 4-cabinet system and the full system during the transition period and will be readable and writeable on both systems.","title":"RDFaaS file systems"},{"location":"archer2-migration/archer2-differences/","text":"Main differences between ARCHER2 4-cabinet system and ARCHER2 full system This section provides an overview of the main differences between the ARCHER2 4-cabinet system that all users have been using up until now and the full ARCHER2 system along with links to more information where appropriate. For all users There are 5860 compute nodes in total on the full ARCHER2 system rather than just the 1024 on the 4-cabinet ARCHER2 system. Of the 5860 compute nodes, 584 are high memory nodes , these nodes have 512 GiB of memory rather than the 256 GiB available on standard memory nodes. There are two data analysis nodes available and these are shared by multiple users. When using these nodes, users typically request the number of cores and the amount of memory they require (unlike compute nodes where you always have access to all the cores and all the memory on a node and do not share with any other users). See the Data Analysis section of the User Guide for more information. Software is provided by the Lmod module system (on the 4-cabinet system, TCL environment modules are used instead). Many commands are similar but there are some differences, see the Software Environment section of the User Guide for more information. Not all versions of all software have been ported over to the full system from the 4-cabinet system, some older versions of software have not been installed. The scheduler layout has been expanded to provide more functionality and flexibility. You can find details of the new QoS available in the Submitting Jobs on ARCHER2 section of the User Guide . You no longer need to specify --reservation=shortqos when using the short QoS. Jobs running in a reservation can now run for longer than the maximum wall time available in any of the normal QoS defined in the scheduler. Reservations must use the reservation QoS. You should no longer add the module load epcc-job-env command to job submission scripts. For users compiling and developing software on ARCHER2 The HPE Programming Environment (PE) version has been updated to the 21.04 release with the 21.09 release also available. These releases make newer versions of compilers and MPI libraries available. The change to Lmod modules means that some modules are hidden by default until dependencies have been loaded (for example, you will not be able to load the cray-netcdf or cray-netcdf-hdf5parallel modules until you have loaded the appropriate cray-hdf5 or cray-hdf5-parallel modules). You can use the module spider command to see all available modules, including hidden ones. There are two MPI transport layers available: OpenFabrics (OFI) and UCX. In some cases, you may see performance and/or scaling improvements by switching to UCX rather than the default OFI transport layer. For more information on when to switch and how to switch, see the Application Development Environment section of the User Guide","title":"Main differences between ARCHER2 4-cabinet system and ARCHER2 full system"},{"location":"archer2-migration/archer2-differences/#main-differences-between-archer2-4-cabinet-system-and-archer2-full-system","text":"This section provides an overview of the main differences between the ARCHER2 4-cabinet system that all users have been using up until now and the full ARCHER2 system along with links to more information where appropriate.","title":"Main differences between ARCHER2 4-cabinet system and ARCHER2 full system"},{"location":"archer2-migration/archer2-differences/#for-all-users","text":"There are 5860 compute nodes in total on the full ARCHER2 system rather than just the 1024 on the 4-cabinet ARCHER2 system. Of the 5860 compute nodes, 584 are high memory nodes , these nodes have 512 GiB of memory rather than the 256 GiB available on standard memory nodes. There are two data analysis nodes available and these are shared by multiple users. When using these nodes, users typically request the number of cores and the amount of memory they require (unlike compute nodes where you always have access to all the cores and all the memory on a node and do not share with any other users). See the Data Analysis section of the User Guide for more information. Software is provided by the Lmod module system (on the 4-cabinet system, TCL environment modules are used instead). Many commands are similar but there are some differences, see the Software Environment section of the User Guide for more information. Not all versions of all software have been ported over to the full system from the 4-cabinet system, some older versions of software have not been installed. The scheduler layout has been expanded to provide more functionality and flexibility. You can find details of the new QoS available in the Submitting Jobs on ARCHER2 section of the User Guide . You no longer need to specify --reservation=shortqos when using the short QoS. Jobs running in a reservation can now run for longer than the maximum wall time available in any of the normal QoS defined in the scheduler. Reservations must use the reservation QoS. You should no longer add the module load epcc-job-env command to job submission scripts.","title":"For all users"},{"location":"archer2-migration/archer2-differences/#for-users-compiling-and-developing-software-on-archer2","text":"The HPE Programming Environment (PE) version has been updated to the 21.04 release with the 21.09 release also available. These releases make newer versions of compilers and MPI libraries available. The change to Lmod modules means that some modules are hidden by default until dependencies have been loaded (for example, you will not be able to load the cray-netcdf or cray-netcdf-hdf5parallel modules until you have loaded the appropriate cray-hdf5 or cray-hdf5-parallel modules). You can use the module spider command to see all available modules, including hidden ones. There are two MPI transport layers available: OpenFabrics (OFI) and UCX. In some cases, you may see performance and/or scaling improvements by switching to UCX rather than the default OFI transport layer. For more information on when to switch and how to switch, see the Application Development Environment section of the User Guide","title":"For users compiling and developing software on ARCHER2"},{"location":"archer2-migration/data-migration/","text":"Data migration from the ARCHER2 4-cabinet system to the ARCHER2 full system This short guide explains how to move data from from the work file system on the ARCHER2 4-cabinet system to the ARCHER2 full system. Your space on the home file system is shared between the ARCHER2 4-cabinet system and the ARCHER2 full system so everything from your home directory is already effectively transferred. Note This section assumes that you have an active ARCHER2 4-cabinet system and ARCHER2 full system account, and that you have successfully logged in to both accounts. Tip Unlike normal access, ARCHER2 4-cabinet system to ARCHER2 full system transfer has been set up to require only one form of authentication. You will only need one factor to authenticate from the 4-cab to the full system or vice versa. This factor can be either an SSH key (that has been registered against your account in SAFE) or you can use your passowrd. If you have a large amount of data to transfer you may want to setup a passphrase-less SSH key on ARCHER2 full system and use the data analysis nodes to run transfers via a Slurm job. Transferring data interactively from the 4-cabinet system to the full system First, login to the ARCHER2 4-cabinet system (making sure to change auser to your username): ssh auser@login-4c.archer2.ac.uk Then, combine important research data into a single archive file using the following command: tar -czf all_my_files.tar.gz file1.txt file2.txt directory1/ Please be selective -- the more data you want to transfer, the more time it will take. Unpack the archive file in the destination directory tar -xzf all_my_files.tar.gz Transferring data using rsync (recommended) Begin the data transfer from the ARCHER2 4-cabinet system to the ARCHER2 full system using rsync : rsync -Pv all_my_files.tar.gz a2user@login.archer2.ac.uk:/work/t01/t01/a2user When running this command, you will be prompted to enter your ARCHER2 password -- this is the same password for the ARCHER2 4-cabinet system and the ARCHER2 full system. Enter it and the data transfer will begin. Remember to replace a2user with your ARCHER2 username, and t01 with the budget associated with that username. We use the -P flag to allow partial transfer -- the same command could be used to restart the transfer after a loss of connection. We move our research archive to our project work directory on the ARCHER2 full system. Transferring data using scp If you are unconcerned about being able to restart an interrupted transfer, you could instead use the scp command, scp all_my_files.tar.gz a2user@login.archer2.ac.uk:/work/t01/t01/a2user/ but rsync is recommended for larger transfers. Transferring data via the serial queue It may be convenient to submit long data transfers to the serial queue. In this case, a number of simple preparatory steps are required to authenticate: On the full system, create a new ssh key pair without passphrase (just press return when prompted). Add the new public key to SAFE against your machine account. Use this key pair for ssh/scp commands in the serial queue to authenticate. As it has been arranged that only one of ssh key/password are required between the serial nodes and the 4-cabinet system, this is sufficient. An example serial queue script using rsync might be: #!/bin/bash # Slurm job options (job-name, job time) #SBATCH --partition=serial #SBATCH --qos=serial #SBATCH --time=02:00:00 #SBATCH --ntasks=1 # Replace [budget code] below with your budget code #SBATCH --account=[budget code] # Issue appropriate rsync command rsync -av --stats --progress --rsh=\"ssh -i ${HOME}/.ssh/id_rsa_batch\" \\ user-01@login-4c.archer2.ac.uk:/work/proj01/proj01/user-01/src \\ /work/proj01/proj01/user-01/destination where ${HOME}/.ssh/id_rsa_batch is the new ssh key. Note that the ${HOME} directory is visible from the serial nodes on the full system, so ssh key pairs in ${HOME}/.ssh are available.","title":"Data migration from the ARCHER2 4-cabinet system to the ARCHER2 full system"},{"location":"archer2-migration/data-migration/#data-migration-from-the-archer2-4-cabinet-system-to-the-archer2-full-system","text":"This short guide explains how to move data from from the work file system on the ARCHER2 4-cabinet system to the ARCHER2 full system. Your space on the home file system is shared between the ARCHER2 4-cabinet system and the ARCHER2 full system so everything from your home directory is already effectively transferred. Note This section assumes that you have an active ARCHER2 4-cabinet system and ARCHER2 full system account, and that you have successfully logged in to both accounts. Tip Unlike normal access, ARCHER2 4-cabinet system to ARCHER2 full system transfer has been set up to require only one form of authentication. You will only need one factor to authenticate from the 4-cab to the full system or vice versa. This factor can be either an SSH key (that has been registered against your account in SAFE) or you can use your passowrd. If you have a large amount of data to transfer you may want to setup a passphrase-less SSH key on ARCHER2 full system and use the data analysis nodes to run transfers via a Slurm job.","title":"Data migration from the ARCHER2 4-cabinet system to the ARCHER2 full system"},{"location":"archer2-migration/data-migration/#transferring-data-interactively-from-the-4-cabinet-system-to-the-full-system","text":"First, login to the ARCHER2 4-cabinet system (making sure to change auser to your username): ssh auser@login-4c.archer2.ac.uk Then, combine important research data into a single archive file using the following command: tar -czf all_my_files.tar.gz file1.txt file2.txt directory1/ Please be selective -- the more data you want to transfer, the more time it will take. Unpack the archive file in the destination directory tar -xzf all_my_files.tar.gz","title":"Transferring data interactively from the 4-cabinet system to the full system"},{"location":"archer2-migration/data-migration/#transferring-data-using-rsync-recommended","text":"Begin the data transfer from the ARCHER2 4-cabinet system to the ARCHER2 full system using rsync : rsync -Pv all_my_files.tar.gz a2user@login.archer2.ac.uk:/work/t01/t01/a2user When running this command, you will be prompted to enter your ARCHER2 password -- this is the same password for the ARCHER2 4-cabinet system and the ARCHER2 full system. Enter it and the data transfer will begin. Remember to replace a2user with your ARCHER2 username, and t01 with the budget associated with that username. We use the -P flag to allow partial transfer -- the same command could be used to restart the transfer after a loss of connection. We move our research archive to our project work directory on the ARCHER2 full system.","title":"Transferring data using rsync (recommended)"},{"location":"archer2-migration/data-migration/#transferring-data-using-scp","text":"If you are unconcerned about being able to restart an interrupted transfer, you could instead use the scp command, scp all_my_files.tar.gz a2user@login.archer2.ac.uk:/work/t01/t01/a2user/ but rsync is recommended for larger transfers.","title":"Transferring data using scp"},{"location":"archer2-migration/data-migration/#transferring-data-via-the-serial-queue","text":"It may be convenient to submit long data transfers to the serial queue. In this case, a number of simple preparatory steps are required to authenticate: On the full system, create a new ssh key pair without passphrase (just press return when prompted). Add the new public key to SAFE against your machine account. Use this key pair for ssh/scp commands in the serial queue to authenticate. As it has been arranged that only one of ssh key/password are required between the serial nodes and the 4-cabinet system, this is sufficient. An example serial queue script using rsync might be: #!/bin/bash # Slurm job options (job-name, job time) #SBATCH --partition=serial #SBATCH --qos=serial #SBATCH --time=02:00:00 #SBATCH --ntasks=1 # Replace [budget code] below with your budget code #SBATCH --account=[budget code] # Issue appropriate rsync command rsync -av --stats --progress --rsh=\"ssh -i ${HOME}/.ssh/id_rsa_batch\" \\ user-01@login-4c.archer2.ac.uk:/work/proj01/proj01/user-01/src \\ /work/proj01/proj01/user-01/destination where ${HOME}/.ssh/id_rsa_batch is the new ssh key. Note that the ${HOME} directory is visible from the serial nodes on the full system, so ssh key pairs in ${HOME}/.ssh are available.","title":"Transferring data via the serial queue"},{"location":"archer2-migration/porting/","text":"Porting applications to full ARCHER2 system Porting applications to the full ARCHER2 system has generally proven straightforward if they are running successfully on the ARCHER2 4-cabinet system. You should be able to use the same (or very similar) compile processes on the the full system as you used on ARCHER2. During testing of the ARCHER2 full system, the CSE team at EPCC have seen that application binaries compiled on the 4-cabinet system can usually be copied over to the full system and work well and give good performance. However, if you run into issues with executables taken from the 4-cabinet system on the full system you should recompile in the first instance. Information on compiling applications on the full system can be found in the Application Development Environment section of the User and Best Practice Guide.","title":"Porting applications to full ARCHER2 system"},{"location":"archer2-migration/porting/#porting-applications-to-full-archer2-system","text":"Porting applications to the full ARCHER2 system has generally proven straightforward if they are running successfully on the ARCHER2 4-cabinet system. You should be able to use the same (or very similar) compile processes on the the full system as you used on ARCHER2. During testing of the ARCHER2 full system, the CSE team at EPCC have seen that application binaries compiled on the 4-cabinet system can usually be copied over to the full system and work well and give good performance. However, if you run into issues with executables taken from the 4-cabinet system on the full system you should recompile in the first instance. Information on compiling applications on the full system can be found in the Application Development Environment section of the User and Best Practice Guide.","title":"Porting applications to full ARCHER2 system"},{"location":"data-tools/","text":"Data Analysis and Tools This section provides information on each of the centrally installed data analysis tools: versions available, how to get access, good practice for getting best performance, links to associated training and webinars, links to associated technical reports (eCSE final reports, white papers), links to instruction manuals and further information. Tip You may also find the Data analysis section of the User and Best Practice Guide useful. The tools currently available in this section are: ParaView : A data visualisation and analysis package R : The R statistical language VisiData : An interactive multitool for tabular data","title":"Overview"},{"location":"data-tools/#data-analysis-and-tools","text":"This section provides information on each of the centrally installed data analysis tools: versions available, how to get access, good practice for getting best performance, links to associated training and webinars, links to associated technical reports (eCSE final reports, white papers), links to instruction manuals and further information. Tip You may also find the Data analysis section of the User and Best Practice Guide useful. The tools currently available in this section are: ParaView : A data visualisation and analysis package R : The R statistical language VisiData : An interactive multitool for tabular data","title":"Data Analysis and Tools"},{"location":"data-tools/cray-r/","text":"R for statistical computing R is a software environment for statistical computing and graphics. It provides a wide variety of statistical and graphical techniques (linear and nonlinear modelling, statistical tests, time-series analysis, classification, clustering, and so on). Note When you log onto ARCHER2, no R module is loaded by default. You need to load the cray-R module to access the functionality described below. The recommended version of R to use on ARCHER2 is the HPE Cray R distribution, which can be loaded using: module load cray-R The HPE Cray R distribution includes a range of common R packages, including all of the base packages, plus a few others. To see what packages are available, run the R command library() --from the R command prompt. At the time of writing, the HPE Cray R distribution included the following packages: Full System Packages in library \u2018/opt/R/4.0.3.0/lib64/R/library\u2019: base The R Base Package boot Bootstrap Functions (Originally by Angelo Canty for S) class Functions for Classification cluster \"Finding Groups in Data\": Cluster Analysis Extended Rousseeuw et al. codetools Code Analysis Tools for R compiler The R Compiler Package datasets The R Datasets Package foreign Read Data Stored by 'Minitab', 'S', 'SAS', 'SPSS', 'Stata', 'Systat', 'Weka', 'dBase', ... graphics The R Graphics Package grDevices The R Graphics Devices and Support for Colours and Fonts grid The Grid Graphics Package KernSmooth Functions for Kernel Smoothing Supporting Wand & Jones (1995) lattice Trellis Graphics for R MASS Support Functions and Datasets for Venables and Ripley's MASS Matrix Sparse and Dense Matrix Classes and Methods methods Formal Methods and Classes mgcv Mixed GAM Computation Vehicle with Automatic Smoothness Estimation nlme Linear and Nonlinear Mixed Effects Models nnet Feed-Forward Neural Networks and Multinomial Log-Linear Models parallel Support for Parallel computation in R rpart Recursive Partitioning and Regression Trees spatial Functions for Kriging and Point Pattern Analysis splines Regression Spline Functions and Classes stats The R Stats Package stats4 Statistical Functions using S4 Classes survival Survival Analysis tcltk Tcl/Tk Interface tools Tools for Package Development utils The R Utils Package 4-cabinet system Packages in library \u2018/opt/R/4.0.2.0/lib64/R/library\u2019: base The R Base Package boot Bootstrap Functions (Originally by Angelo Canty for S) class Functions for Classification cluster \"Finding Groups in Data\": Cluster Analysis Extended Rousseeuw et al. codetools Code Analysis Tools for R compiler The R Compiler Package datasets The R Datasets Package foreign Read Data Stored by 'Minitab', 'S', 'SAS', 'SPSS', 'Stata', 'Systat', 'Weka', 'dBase', ... graphics The R Graphics Package grDevices The R Graphics Devices and Support for Colours and Fonts grid The Grid Graphics Package KernSmooth Functions for Kernel Smoothing Supporting Wand & Jones (1995) lattice Trellis Graphics for R MASS Support Functions and Datasets for Venables and Ripley's MASS Matrix Sparse and Dense Matrix Classes and Methods methods Formal Methods and Classes mgcv Mixed GAM Computation Vehicle with Automatic Smoothness Estimation nlme Linear and Nonlinear Mixed Effects Models nnet Feed-Forward Neural Networks and Multinomial Log-Linear Models parallel Support for Parallel computation in R rpart Recursive Partitioning and Regression Trees spatial Functions for Kriging and Point Pattern Analysis splines Regression Spline Functions and Classes stats The R Stats Package stats4 Statistical Functions using S4 Classes survival Survival Analysis tcltk Tcl/Tk Interface tools Tools for Package Development utils The R Utils Package Running R on the compute nodes In this section, we provide an example R job submission scripts for using R on the ARCHER2 compute nodes. Serial R submission script #!/bin/bash --login #SBATCH --job-name=r_test #SBATCH --nodes=1 #SBATCH --tasks-per-node=1 #SBATCH --cpus-per-task=1 #SBATCH --time=00:10:00 # Replace [budget code] below with your project code (e.g., t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Setup the batch environment module load epcc-job-env # Load the R module module load cray-R # Run your R progamme Rscript serial_test.R On completion, the output of the R script will be available in the job output file.","title":"R"},{"location":"data-tools/cray-r/#r-for-statistical-computing","text":"R is a software environment for statistical computing and graphics. It provides a wide variety of statistical and graphical techniques (linear and nonlinear modelling, statistical tests, time-series analysis, classification, clustering, and so on). Note When you log onto ARCHER2, no R module is loaded by default. You need to load the cray-R module to access the functionality described below. The recommended version of R to use on ARCHER2 is the HPE Cray R distribution, which can be loaded using: module load cray-R The HPE Cray R distribution includes a range of common R packages, including all of the base packages, plus a few others. To see what packages are available, run the R command library() --from the R command prompt. At the time of writing, the HPE Cray R distribution included the following packages: Full System Packages in library \u2018/opt/R/4.0.3.0/lib64/R/library\u2019: base The R Base Package boot Bootstrap Functions (Originally by Angelo Canty for S) class Functions for Classification cluster \"Finding Groups in Data\": Cluster Analysis Extended Rousseeuw et al. codetools Code Analysis Tools for R compiler The R Compiler Package datasets The R Datasets Package foreign Read Data Stored by 'Minitab', 'S', 'SAS', 'SPSS', 'Stata', 'Systat', 'Weka', 'dBase', ... graphics The R Graphics Package grDevices The R Graphics Devices and Support for Colours and Fonts grid The Grid Graphics Package KernSmooth Functions for Kernel Smoothing Supporting Wand & Jones (1995) lattice Trellis Graphics for R MASS Support Functions and Datasets for Venables and Ripley's MASS Matrix Sparse and Dense Matrix Classes and Methods methods Formal Methods and Classes mgcv Mixed GAM Computation Vehicle with Automatic Smoothness Estimation nlme Linear and Nonlinear Mixed Effects Models nnet Feed-Forward Neural Networks and Multinomial Log-Linear Models parallel Support for Parallel computation in R rpart Recursive Partitioning and Regression Trees spatial Functions for Kriging and Point Pattern Analysis splines Regression Spline Functions and Classes stats The R Stats Package stats4 Statistical Functions using S4 Classes survival Survival Analysis tcltk Tcl/Tk Interface tools Tools for Package Development utils The R Utils Package 4-cabinet system Packages in library \u2018/opt/R/4.0.2.0/lib64/R/library\u2019: base The R Base Package boot Bootstrap Functions (Originally by Angelo Canty for S) class Functions for Classification cluster \"Finding Groups in Data\": Cluster Analysis Extended Rousseeuw et al. codetools Code Analysis Tools for R compiler The R Compiler Package datasets The R Datasets Package foreign Read Data Stored by 'Minitab', 'S', 'SAS', 'SPSS', 'Stata', 'Systat', 'Weka', 'dBase', ... graphics The R Graphics Package grDevices The R Graphics Devices and Support for Colours and Fonts grid The Grid Graphics Package KernSmooth Functions for Kernel Smoothing Supporting Wand & Jones (1995) lattice Trellis Graphics for R MASS Support Functions and Datasets for Venables and Ripley's MASS Matrix Sparse and Dense Matrix Classes and Methods methods Formal Methods and Classes mgcv Mixed GAM Computation Vehicle with Automatic Smoothness Estimation nlme Linear and Nonlinear Mixed Effects Models nnet Feed-Forward Neural Networks and Multinomial Log-Linear Models parallel Support for Parallel computation in R rpart Recursive Partitioning and Regression Trees spatial Functions for Kriging and Point Pattern Analysis splines Regression Spline Functions and Classes stats The R Stats Package stats4 Statistical Functions using S4 Classes survival Survival Analysis tcltk Tcl/Tk Interface tools Tools for Package Development utils The R Utils Package","title":"R for statistical computing"},{"location":"data-tools/cray-r/#running-r-on-the-compute-nodes","text":"In this section, we provide an example R job submission scripts for using R on the ARCHER2 compute nodes.","title":"Running R on the compute nodes"},{"location":"data-tools/cray-r/#serial-r-submission-script","text":"#!/bin/bash --login #SBATCH --job-name=r_test #SBATCH --nodes=1 #SBATCH --tasks-per-node=1 #SBATCH --cpus-per-task=1 #SBATCH --time=00:10:00 # Replace [budget code] below with your project code (e.g., t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Setup the batch environment module load epcc-job-env # Load the R module module load cray-R # Run your R progamme Rscript serial_test.R On completion, the output of the R script will be available in the job output file.","title":"Serial R submission script"},{"location":"data-tools/paraview/","text":"ParaView ParaView ParaView is a data visualisation and analysis package. Whilst ARCHER2 compute or login nodes do not have graphics cards installed in them paraview is installed so the visualisation libraries and applications can be used to post-process simulation data. The ParaView server ( pvserver ), batch application ( pvbatch ) and the Python interface ( pvpython ) are all available. Users are able to run the server on the compute nodes and connect to a local ParaView client running on their own computer. Useful links Paraview webpage ParaView quick-start guide Using ParaView on ARCHER2 ParaView is available through the paraview module. module load paraview Once the module has been added the ParaView executables, tools, and libraries will be able available. Connecting to pvserver on ARCHER2 For doing visualisation you should connect to pvserver from a local paraview client running on your own computer. Note You should make sure the version of ParaView you have installed locally is the same as the one on ARCHER2 (version 5.9.1). The following instructions are for running pvserver in an interactive job. Start an iteractive job using: srun --nodes=1 --exclusive --time=00:20:00 \\ --partition=standard --qos=short --reservation=shortqos \\ --pty /bin/bash Once the job starts the command prompt will change to show you are now on the compute node e.g. auser@nid001023:/work/t01/t01/auser> Then load the ParaView module and start pvserver with the srun command, auser@nid001023:/work/t01/t01/auser> module load paraview auser@nid001023:/work/t01/t01/auser> srun --oversubscribe -n 4 pvserver --mpi --force-offscreen-rendering Waiting for client... Connection URL: cs://nid001023:11111 Accepting connection(s): nid001023:11111 In a separate terminal you can now set up an ssh tunnel with the node ID and port number which the pvserver is using, e.g. ssh -L 11111:nid001023:11111 auser@login.archer2.ac.uk enter your password and phasephrase as usual. You can then connect from your local client using the following connection settings: Name: archer2 Server Type: Client/Server Host: localhost Port: 11111 Note The Host from the local client should be set to \"localhost\" when using the ssh tunnel. The \"Name\" field can be set to a name of your choosing. 11111 is the default port for pvserver. If it has connected correctly you should see the following: Waiting for client... Connection URL: cs://nid001023:11111 Accepting connection(s): nid001023:11111 Client connected. Using batch-mode (pvbatch) A pvbatch script can be run in a standard job script. For example the following will run on a single node: #!/bin/bash # Slurm job options (job-name, compute nodes, job time) #SBATCH --job-name=example_paraview_job #SBATCH --time=0:20:00 #SBATCH --nodes=1 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 # Replace [budget code] below with your budget code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard module load epcc-job-env module load paraview srun --mpi=pmi2 pvbatch pvbatchscript.py Compiling ParaView The latest instructions for building ParaView on ARCHER2 may be found in the GitHub repository of build instructions: Build instructions for Paraview on GitHub","title":"ParaView"},{"location":"data-tools/paraview/#paraview","text":"ParaView ParaView is a data visualisation and analysis package. Whilst ARCHER2 compute or login nodes do not have graphics cards installed in them paraview is installed so the visualisation libraries and applications can be used to post-process simulation data. The ParaView server ( pvserver ), batch application ( pvbatch ) and the Python interface ( pvpython ) are all available. Users are able to run the server on the compute nodes and connect to a local ParaView client running on their own computer.","title":"ParaView"},{"location":"data-tools/paraview/#useful-links","text":"Paraview webpage ParaView quick-start guide","title":"Useful links"},{"location":"data-tools/paraview/#using-paraview-on-archer2","text":"ParaView is available through the paraview module. module load paraview Once the module has been added the ParaView executables, tools, and libraries will be able available.","title":"Using ParaView on ARCHER2"},{"location":"data-tools/paraview/#connecting-to-pvserver-on-archer2","text":"For doing visualisation you should connect to pvserver from a local paraview client running on your own computer. Note You should make sure the version of ParaView you have installed locally is the same as the one on ARCHER2 (version 5.9.1). The following instructions are for running pvserver in an interactive job. Start an iteractive job using: srun --nodes=1 --exclusive --time=00:20:00 \\ --partition=standard --qos=short --reservation=shortqos \\ --pty /bin/bash Once the job starts the command prompt will change to show you are now on the compute node e.g. auser@nid001023:/work/t01/t01/auser> Then load the ParaView module and start pvserver with the srun command, auser@nid001023:/work/t01/t01/auser> module load paraview auser@nid001023:/work/t01/t01/auser> srun --oversubscribe -n 4 pvserver --mpi --force-offscreen-rendering Waiting for client... Connection URL: cs://nid001023:11111 Accepting connection(s): nid001023:11111 In a separate terminal you can now set up an ssh tunnel with the node ID and port number which the pvserver is using, e.g. ssh -L 11111:nid001023:11111 auser@login.archer2.ac.uk enter your password and phasephrase as usual. You can then connect from your local client using the following connection settings: Name: archer2 Server Type: Client/Server Host: localhost Port: 11111 Note The Host from the local client should be set to \"localhost\" when using the ssh tunnel. The \"Name\" field can be set to a name of your choosing. 11111 is the default port for pvserver. If it has connected correctly you should see the following: Waiting for client... Connection URL: cs://nid001023:11111 Accepting connection(s): nid001023:11111 Client connected.","title":"Connecting to pvserver on ARCHER2"},{"location":"data-tools/paraview/#using-batch-mode-pvbatch","text":"A pvbatch script can be run in a standard job script. For example the following will run on a single node: #!/bin/bash # Slurm job options (job-name, compute nodes, job time) #SBATCH --job-name=example_paraview_job #SBATCH --time=0:20:00 #SBATCH --nodes=1 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 # Replace [budget code] below with your budget code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard module load epcc-job-env module load paraview srun --mpi=pmi2 pvbatch pvbatchscript.py","title":"Using batch-mode (pvbatch)"},{"location":"data-tools/paraview/#compiling-paraview","text":"The latest instructions for building ParaView on ARCHER2 may be found in the GitHub repository of build instructions: Build instructions for Paraview on GitHub","title":"Compiling ParaView"},{"location":"data-tools/visidata/","text":"VisiData VisiData is an interactive multitool for tabular data. It combines the clarity of a spreadsheet, the efficiency of the terminal, and the power of Python, into a lightweight utility which can handle millions of rows with ease. Useful links VisiData online tutorial 10 ways to use VisiData VisiData documentation Using VisiData to analyse HPC code performance : The ARCHER2 /Understanding Package Performance/ course uses VisiData to analyse the performance of an HPC application. VisiData on ARCHER2 You can access VisiData on ARCHER2 by loading the visidata module: module load visidata Once the module has been loaded, VisiData is available via the vd command. Visidata can also be used in scripts by saving a command log and replaying it. See the VisiData documentation on saving and restoring VisiData sessions.","title":"VisiData"},{"location":"data-tools/visidata/#visidata","text":"VisiData is an interactive multitool for tabular data. It combines the clarity of a spreadsheet, the efficiency of the terminal, and the power of Python, into a lightweight utility which can handle millions of rows with ease.","title":"VisiData"},{"location":"data-tools/visidata/#useful-links","text":"VisiData online tutorial 10 ways to use VisiData VisiData documentation Using VisiData to analyse HPC code performance : The ARCHER2 /Understanding Package Performance/ course uses VisiData to analyse the performance of an HPC application.","title":"Useful links"},{"location":"data-tools/visidata/#visidata-on-archer2","text":"You can access VisiData on ARCHER2 by loading the visidata module: module load visidata Once the module has been loaded, VisiData is available via the vd command. Visidata can also be used in scripts by saving a command log and replaying it. See the VisiData documentation on saving and restoring VisiData sessions.","title":"VisiData on ARCHER2"},{"location":"essentials/","text":"Essential Skills This section provides information and links on essential skills required to use ARCHER2 efficiently: e.g. using Linux command line, accessing help and documentation. Terminal In order to access HPC machines such as ARCHER2 you will need to use a Linux command line terminal window Options for Linux, MacOS and Windows are described under our Connecting to ARCHER2 guide Linux Command Line A guide to using the Unix Shell for complete novices For those already familiar with the basics there is also a lesson on shell extras Basic Slurm commands Slurm is the scheduler used on ARCHER2 and we provide a guide to using the basic Slurm commands including how to find out: what resources are available to you how to submit jobs to the scheduler the status of jobs submitted Text Editors The following text editors are available on ARCHER2 Name Description Examples emacs A widely used editor with a focus on extensibility. emacs -nw sharpen.pbs CTRL+X CTRL+C quits CTRL+X CTRL+S saves nano A small, free editor with a focus on user friendliness. nano sharpen.pbs CTRL+X quits CTRL+O saves vi A mode based editor with a focus on aiding code development. vi cfd.f90 :q in command mode quits :q! in command mode quits without saving :w in command mode saves i in command mode switches to insert mode ESC in insert mode switches to command mode If you are using MobaXterm on Windows you can use the inbuilt MobaTextEditor text file editor. You can edit on your local machine using your preferred text editor, and then upload the file to ARCHER2. Make sure you can save the file using Linux line-endings. Notepad, for example, will support Unix/Linux line endings (LF), Macintosh line endings (CR), and Windows Line endings (CRLF) Quick Reference Sheet We have produced this Quick Reference Sheet which you may find useful.","title":"Essential Skills"},{"location":"essentials/#essential-skills","text":"This section provides information and links on essential skills required to use ARCHER2 efficiently: e.g. using Linux command line, accessing help and documentation.","title":"Essential Skills"},{"location":"essentials/#terminal","text":"In order to access HPC machines such as ARCHER2 you will need to use a Linux command line terminal window Options for Linux, MacOS and Windows are described under our Connecting to ARCHER2 guide","title":"Terminal"},{"location":"essentials/#linux-command-line","text":"A guide to using the Unix Shell for complete novices For those already familiar with the basics there is also a lesson on shell extras","title":"Linux Command Line"},{"location":"essentials/#basic-slurm-commands","text":"Slurm is the scheduler used on ARCHER2 and we provide a guide to using the basic Slurm commands including how to find out: what resources are available to you how to submit jobs to the scheduler the status of jobs submitted","title":"Basic Slurm commands"},{"location":"essentials/#text-editors","text":"The following text editors are available on ARCHER2 Name Description Examples emacs A widely used editor with a focus on extensibility. emacs -nw sharpen.pbs CTRL+X CTRL+C quits CTRL+X CTRL+S saves nano A small, free editor with a focus on user friendliness. nano sharpen.pbs CTRL+X quits CTRL+O saves vi A mode based editor with a focus on aiding code development. vi cfd.f90 :q in command mode quits :q! in command mode quits without saving :w in command mode saves i in command mode switches to insert mode ESC in insert mode switches to command mode If you are using MobaXterm on Windows you can use the inbuilt MobaTextEditor text file editor. You can edit on your local machine using your preferred text editor, and then upload the file to ARCHER2. Make sure you can save the file using Linux line-endings. Notepad, for example, will support Unix/Linux line endings (LF), Macintosh line endings (CR), and Windows Line endings (CRLF)","title":"Text Editors"},{"location":"essentials/#quick-reference-sheet","text":"We have produced this Quick Reference Sheet which you may find useful.","title":"Quick Reference Sheet"},{"location":"faq/","text":"ARCHER2 Frequently Asked Questions This section documents some of the questions raised to the Service Desk on ARCHER2, and the advice and solutions. User accounts Username already in use Q. I created a machine account on ARCHER2 for a training course, but now I want to use that machine username for my main ARCHER2 project, and the system will not let me, saying \"that name is already in use\". How can I re-use that username. A. Send an email to the service desk , letting us know the username and project that you set up previously, and asking for that account and any associated data to be deleted. Once deleted, you can then re-use that username to request an account in your main ARCHER2 project. Data ARCHER /home Data Q. What is happening to data on /home on ARCHER and how can I access it from ARCHER2? A. Our systems team have completed the final copy of the ARCHER /home filesystem and this is now available on ARCHER2. If your home directory on ARCHER was: /home/[project]/[group]/[username] Then this data should be available on ARCHER2 at: /home/archer-home-backup/[project]/[group]/[username] This data is read-only and cannot be edited. You should be able to read the data provided you requested the same username and the same project when creating your ARCHER2 account. If this is not the case and you still need some of this data please contact the Service Desk, support@archer2.ac.uk . This backup will not be retained indefinitely so please copy any files you need to your ARCHER2 home directory as soon as you can. ARCHER /work Data Q. What is happening to data on /work on ARCHER? A. The hardware is being dismantled. All users are responsible for the transfer of any ARCHER /work data that they wish to keep. There will be no access to /work after 0800 on Wednesday 27th January 2021. RDF Data Q. What is happening to data on the RDF ( /epsrc and /general ) and how can I access data on the RDF from ARCHER2? A. The RDF data is now available on RDF as a Service (RDFaaS). ARCHER2 users can access the data from the ARCHER2 User Access Nodes (UANs) using cd /epsrc/your_project_code cd /general/your_project_code The data was initially read-only but RDFaaS is now available for read and write access. Note : as previously notified, /nerc is no longer available If the data you wish to access is from an old ARCHER project which is not on ARCHER2, then please contact the Service Desk ( support@archer2.ac.uk ) and we can make arrangements so that you are able to access the data. Undeleteable file .nfsXXXXXXXXXXX Q. I have a file called .nfsXXXXXXXXXXX (where XXXXXXXXXXX is a long hexadecimal string) in my /home folder but I can't delete it. A. This file will have been created during a file copy which failed. Trying to delete it will give an error \"Device or resource busy\", even though the copy has ended and no active task is locking it. echo -n >.nfsXXXXXXXXXXX will remove it. Running on ARCHER2 OOM error on ARCHER2 Q. Why is my code, which worked fine on ARCHER, failing on ARCHER2 with an out of memory (OOM) error? A. While each ARCHER2 node has more memory than an ARCHER node, the large number of processor on ARCHER2 nodes means that there is slightly less memory per processor. This can result in jobs that ran fine on ARCHER failing because they use up all of the node memory. We recommend that you try running the same job on underpopulated nodes. This can be done by editing reducing the --tasks-per-node in your Slurm submission script. Please lower it to half of its value when it fails (so if you have --tasks-per-node=128 , reduce it to --tasks-per-node=64 ). If the problem persists on underpopulated node, this may be a result of a known issue with the underlying libfabric on ARCHER2. You can find more information on this issue (including a temporary workaround) in the Known Issues section. qstat, qsub 'Command not found' Q. Commands such as qstat and qsub don't work - I get a \"command not found\" message - what am I doing wrong? A. qstat and qsub were commands to access the PBS queue management system on ARCHER. ARCHER2 uses Slurm instead of PBS - you can do all the same kinds of things but the commands are different. The Running jobs documentation includes an introduction to Slurm commands . Checking budgets Q. How can I check which budget code(s) I can use? A. You can check in SAFE by selecting Login accounts from the menu, select the login account you want to query. Under Login account details you will see each of the budget codes you have access to listed e.g. e123 resources and then under Resource Pool to the right of this, a note of the remaining budget. When logged in to the machine you can also use the command sacctmgr show assoc where user=$LOGNAME format=user,Account%12,MaxTRESMins,QOS%40 This will list all the budget codes that you have access to (but not the amount of budget available) e.g. User Account MaxTRESMins QOS -------- ------------ ------------ ----------------------------------- userx e123-test largescale,long,short,standard userx e123 cpu=0 largescale,long,short,standard This shows that userx is a member of budgets e123-test and e123 . However, the cpu=0 indicates that the e123 budget is empty or disabled. This user can submit jobs using the e123-test budget. You can only check the amount of available budget via SAFE - see above . Estimated start time of queued jobs Q. I\u2019ve checked the estimated start time for my queued jobs using \u201csqueue -u $USER --start\u201d. Why does the estimated start time keep changing? A. ARCHER2 uses the Slurm scheduler to queue jobs for the compute nodes. Slurm attempts to find a better schedule as jobs complete and new jobs are added to the queue. This helps to maximise the use of resources by minimising the number of idle compute nodes, in turn reducing your wait time in the queue. However, If you periodically check the estimated start time of your queued jobs, you may notice that the estimate changes or even disappears. This is because Slurm only assigns the top entries in the queue with an estimated start time. As the schedule changes, your jobs could move in and out of this top region and thus gain or lose an estimated start time.","title":"ARCHER2 Frequently Asked Questions"},{"location":"faq/#archer2-frequently-asked-questions","text":"This section documents some of the questions raised to the Service Desk on ARCHER2, and the advice and solutions.","title":"ARCHER2 Frequently Asked Questions"},{"location":"faq/#user-accounts","text":"","title":"User accounts"},{"location":"faq/#username-already-in-use","text":"Q. I created a machine account on ARCHER2 for a training course, but now I want to use that machine username for my main ARCHER2 project, and the system will not let me, saying \"that name is already in use\". How can I re-use that username. A. Send an email to the service desk , letting us know the username and project that you set up previously, and asking for that account and any associated data to be deleted. Once deleted, you can then re-use that username to request an account in your main ARCHER2 project.","title":"Username already in use"},{"location":"faq/#data","text":"","title":"Data"},{"location":"faq/#archer-home-data","text":"Q. What is happening to data on /home on ARCHER and how can I access it from ARCHER2? A. Our systems team have completed the final copy of the ARCHER /home filesystem and this is now available on ARCHER2. If your home directory on ARCHER was: /home/[project]/[group]/[username] Then this data should be available on ARCHER2 at: /home/archer-home-backup/[project]/[group]/[username] This data is read-only and cannot be edited. You should be able to read the data provided you requested the same username and the same project when creating your ARCHER2 account. If this is not the case and you still need some of this data please contact the Service Desk, support@archer2.ac.uk . This backup will not be retained indefinitely so please copy any files you need to your ARCHER2 home directory as soon as you can.","title":"ARCHER /home Data"},{"location":"faq/#archer-work-data","text":"Q. What is happening to data on /work on ARCHER? A. The hardware is being dismantled. All users are responsible for the transfer of any ARCHER /work data that they wish to keep. There will be no access to /work after 0800 on Wednesday 27th January 2021.","title":"ARCHER /work Data"},{"location":"faq/#rdf-data","text":"Q. What is happening to data on the RDF ( /epsrc and /general ) and how can I access data on the RDF from ARCHER2? A. The RDF data is now available on RDF as a Service (RDFaaS). ARCHER2 users can access the data from the ARCHER2 User Access Nodes (UANs) using cd /epsrc/your_project_code cd /general/your_project_code The data was initially read-only but RDFaaS is now available for read and write access. Note : as previously notified, /nerc is no longer available If the data you wish to access is from an old ARCHER project which is not on ARCHER2, then please contact the Service Desk ( support@archer2.ac.uk ) and we can make arrangements so that you are able to access the data.","title":"RDF Data"},{"location":"faq/#undeleteable-file-nfsxxxxxxxxxxx","text":"Q. I have a file called .nfsXXXXXXXXXXX (where XXXXXXXXXXX is a long hexadecimal string) in my /home folder but I can't delete it. A. This file will have been created during a file copy which failed. Trying to delete it will give an error \"Device or resource busy\", even though the copy has ended and no active task is locking it. echo -n >.nfsXXXXXXXXXXX will remove it.","title":"Undeleteable file .nfsXXXXXXXXXXX"},{"location":"faq/#running-on-archer2","text":"","title":"Running on ARCHER2"},{"location":"faq/#oom-error-on-archer2","text":"Q. Why is my code, which worked fine on ARCHER, failing on ARCHER2 with an out of memory (OOM) error? A. While each ARCHER2 node has more memory than an ARCHER node, the large number of processor on ARCHER2 nodes means that there is slightly less memory per processor. This can result in jobs that ran fine on ARCHER failing because they use up all of the node memory. We recommend that you try running the same job on underpopulated nodes. This can be done by editing reducing the --tasks-per-node in your Slurm submission script. Please lower it to half of its value when it fails (so if you have --tasks-per-node=128 , reduce it to --tasks-per-node=64 ). If the problem persists on underpopulated node, this may be a result of a known issue with the underlying libfabric on ARCHER2. You can find more information on this issue (including a temporary workaround) in the Known Issues section.","title":"OOM error on ARCHER2"},{"location":"faq/#qstat-qsub-command-not-found","text":"Q. Commands such as qstat and qsub don't work - I get a \"command not found\" message - what am I doing wrong? A. qstat and qsub were commands to access the PBS queue management system on ARCHER. ARCHER2 uses Slurm instead of PBS - you can do all the same kinds of things but the commands are different. The Running jobs documentation includes an introduction to Slurm commands .","title":"qstat, qsub 'Command not found'"},{"location":"faq/#checking-budgets","text":"Q. How can I check which budget code(s) I can use? A. You can check in SAFE by selecting Login accounts from the menu, select the login account you want to query. Under Login account details you will see each of the budget codes you have access to listed e.g. e123 resources and then under Resource Pool to the right of this, a note of the remaining budget. When logged in to the machine you can also use the command sacctmgr show assoc where user=$LOGNAME format=user,Account%12,MaxTRESMins,QOS%40 This will list all the budget codes that you have access to (but not the amount of budget available) e.g. User Account MaxTRESMins QOS -------- ------------ ------------ ----------------------------------- userx e123-test largescale,long,short,standard userx e123 cpu=0 largescale,long,short,standard This shows that userx is a member of budgets e123-test and e123 . However, the cpu=0 indicates that the e123 budget is empty or disabled. This user can submit jobs using the e123-test budget. You can only check the amount of available budget via SAFE - see above .","title":"Checking budgets"},{"location":"faq/#estimated-start-time-of-queued-jobs","text":"Q. I\u2019ve checked the estimated start time for my queued jobs using \u201csqueue -u $USER --start\u201d. Why does the estimated start time keep changing? A. ARCHER2 uses the Slurm scheduler to queue jobs for the compute nodes. Slurm attempts to find a better schedule as jobs complete and new jobs are added to the queue. This helps to maximise the use of resources by minimising the number of idle compute nodes, in turn reducing your wait time in the queue. However, If you periodically check the estimated start time of your queued jobs, you may notice that the estimate changes or even disappears. This is because Slurm only assigns the top entries in the queue with an estimated start time. As the schedule changes, your jobs could move in and out of this top region and thus gain or lose an estimated start time.","title":"Estimated start time of queued jobs"},{"location":"known-issues/","text":"ARCHER2 Known Issues This section highlights known issues on ARCHER2, their potential impacts and any known workarounds. Many of these issues are under active investigation by HPE Cray and the wider service. Open Issues OOM due to memory leak in libfabric (Added: 2022-02-23) There is an underlying memory leak in the version of libfabric on ARCHER2 (that comes as part of the underlying SLES operating system) which can cause jobs to fail with an OOM (Out Of Memory) error. This issue will be addressed in a future upgrade of the ARCHER2 operating system. You can workaround this issue by setting the following environment variable in your job submission scripts: export FI_MR_CACHE_MAX_COUNT=0 This may come with a performance penalty (though in many cases, we have not seen a noticeable performance impact). If you continue to see OOM errors after setting this environment variable and do not believe that your application should be requesting too much memory then please contact the service desk Default FFTW library points to Intel Haswell version rather than AMD Rome at runtime (Added: 2022-02-23) By default, and at runtime, the standard FFTW library version (from the module cray-fftw ) will link to a version of the FFTW library optimised for the Intel Haswell architecture rather than the AMD EPYC architecture. This does not cause errors as the instruction set is compatible between the two architectures but may not provide optimal performance. The performance differences observed have been small (< 5%) but if you want to ensure that applications using the cray-fftw module use the correct version of the libraries at runtime, you should add the following lines to your job submission script: module load cray-fftw export LD_LIBRARY_PATH=$CRAY_LD_LIBRARY_PATH:$LD_LIBRARY_PATH The issue arises because the compilers encode a default path to libraries ( /opt/cray/pe/lib64 ) into the RUNPATH of the executable so that you do not need to load all the library module dependencies at runtime. The libraries that the executable finds at /opt/cray/pe/lib64 are soft links to the default versions of the libraries. There is an error in the soft link to the FFTW libraries in this directory such they point to the Haswell version of the FFTW libraries rather than the AMD EPYC (Rome) versions of the libraries. Occasionally user jobs can cause compute nodes to crash (Added: 2021-11-22) In rare circumstances, it is possible for a user job to crash the compute nodes on which it is running. This is only evident to the user as a failed job: there is no obvious sign that the nodes have crashed. Therefore, if we identify a user whose jobs are causing nodes to crash, we may need to work with them to stop this happening. The underlying issue is resolved in an update to the compute-node operating system (Shasta Version 1.5), which is expected to be rolled out to the main system early in 2022. In the meantime, users who experience this issue are advised to try disabling XPMEM in their application. The ARCHER2 CSE team can provide advice on how to do this. You can contact the CSE team via the service desk . Dask Python package missing dependencies (Added: 2021-11-22) The Dask Python package is missing some dependencies on the latest Programming Environment (21.09). This can be worked around either by using the default Programming Environment (21.04), or by following the instructions to install dask in your own user space. Warning when compiling Fortran code with CCE and MPI_F08 interface (Added: 2021-11-18) When you compile Fortran code using the MPI F08 interface (i.e. use mpi_f08 ) using the default version of CCE (11.0.4) you will see warnings similar to: use mpi_f08 ^ ftn-1753 crayftn: WARNING INTERFACE_MPI, File = interface_mpi_mod.f90, Line = 8, Column = 7 File \"/opt/cray/pe/mpich/8.1.4/ofi/cray/9.1/include/MPI_F08.mod\" containing [sub]module information for \"MPI_F08\" was created with a previous compiler release. It will not be supported by the next major release. It is version 110 from release 9.0. These warnings can be safely ignored as they do not affect the functioning of the code. If you wish to avoid the warnings, you can compile using the more recent CCE version (12.0.3) on the system. To switch to this version, use module load cpe/21.09 from the default environment on ARCHER2. Research Software There are several outstanding issues for the centrally installed Research Software: PyChemShell is not yet available. We are working with the code developers to address this. PLUMED is not yet available. Currently, we recommend affected users to install a local version of the software. Users should also check individual software pages, for known limitations/ caveats, for the use of software on the Cray EX platform and Cray Linux Environment. Issues with RPATH for non-default library versions When you compile applications against non-default versions of libraries within the HPE Cray software stack and use the environment variable CRAY_ADD_RPATH=yes to try and encode the paths to these libraries within the binary this will not be respected at runtime and the binaries will use the default versions instead. The workaround for this issue is to ensure that you set: export LD_LIBRARY_PATH=$CRAY_LD_LIBRARY_PATH:$LD_LIBRARY_PATH at both compile and runtime. For more details on using non-default versions of libraries, see the description in the User and Best Practice Guide MPI UCX ERROR: ivb_reg_mr If you are using the UCX layer for MPI communication you may see an error such as: [1613401128.440695] [nid001192:11838:0] ib_md.c:325 UCX ERROR ibv_reg_mr(address=0xabcf12c0, length=26400, access=0xf) failed: Cannot allocate memory [1613401128.440768] [nid001192:11838:0] ucp_mm.c:137 UCX ERROR failed to register address 0xabcf12c0 mem_type bit 0x1 length 26400 on md[4]=mlx5_0: Input/output error (md reg_mem_types 0x15) [1613401128.440773] [nid001192:11838:0] ucp_request.c:269 UCX ERROR failed to register user buffer datatype 0x8 address 0xabcf12c0 len 26400: Input/output error MPICH ERROR [Rank 1534] [job id 114930.0] [Mon Feb 15 14:58:48 2021] [unknown] [nid001192] - Abort(672797967) (rank 1534 in comm 0): Fatal error in PMPI_Isend: Other MPI error, error stack: PMPI_Isend(160)......: MPI_Isend(buf=0xabcf12c0, count=3300, MPI_DOUBLE_PRECISION, dest=1612, tag=4, comm=0x84000004, request=0x7fffb38fa0fc) failed MPID_Isend(416)......: MPID_isend_unsafe(92): MPIDI_UCX_send(95)...: returned failed request in UCX netmod(ucx_send.h 95 MPIDI_UCX_send Input/output error) aborting job: Fatal error in PMPI_Isend: Other MPI error, error stack: PMPI_Isend(160)......: MPI_Isend(buf=0xabcf12c0, count=3300, MPI_DOUBLE_PRECISION, dest=1612, tag=4, comm=0x84000004, request=0x7fffb38fa0fc) failed MPID_Isend(416)......: MPID_isend_unsafe(92): MPIDI_UCX_send(95)...: returned failed request in UCX netmod(ucx_send.h 95 MPIDI_UCX_send Input/output error) [1613401128.457254] [nid001192:11838:0] mm_xpmem.c:82 UCX WARN remote segment id 200002e09 apid 200002e3e is not released, refcount 1 [1613401128.457261] [nid001192:11838:0] mm_xpmem.c:82 UCX WARN remote segment id 200002e08 apid 100002e3e is not released, refcount 1 You can add the following line to your job submission script before the srun command to try and workaround this error: export UCX_IB_REG_METHODS=direct Note Setting this flag may have an impact on code performance. AOCC compiler fails to compile with NetCDF (Added: 2021-11-18) There is currently a problem with the module file which means cray-netcdf-hdf5parallel will not operate correctly in PrgEnv-aocc. An example of the error seen is: F90-F-0004-Corrupt or Old Module file /opt/cray/pe/netcdf-hdf5parallel/4.7.4.3/crayclang/9.1/include/netcdf.mod (netcdf.F90: 8) The current workaround for this is to load module epcc-netcdf-hdf5parallel instead if PrgEnv-aocc is required. Slurm --export option does not work in job submission script The option --export=ALL propagates all the environment variables from the login node to the compute node. If you include the option in the job submission script, it is wrongly ignored by Slurm. The current workaround is to include the option when the job submission script is launched. For instance: sbatch --export=ALL myjob.slurm Recently Resolved Issues Intel MKL libraries: FFTW gives incorrect results (Added: 2022-03-28) Until recently, the Intel MKL library modules installed on ARCHER2 set environment variables to enable high-performance code paths. Unfortunately, recent investigations have revealed that these code paths resulted in incorrect results from the FFTW compenents of MKL. The modules were changed on 21 March 2022 to remove these environment variables and we have verified that the FFTW components of the MKL libraries now give the expected results.","title":"ARCHER2 Known Issues"},{"location":"known-issues/#archer2-known-issues","text":"This section highlights known issues on ARCHER2, their potential impacts and any known workarounds. Many of these issues are under active investigation by HPE Cray and the wider service.","title":"ARCHER2 Known Issues"},{"location":"known-issues/#open-issues","text":"","title":"Open Issues"},{"location":"known-issues/#oom-due-to-memory-leak-in-libfabric-added-2022-02-23","text":"There is an underlying memory leak in the version of libfabric on ARCHER2 (that comes as part of the underlying SLES operating system) which can cause jobs to fail with an OOM (Out Of Memory) error. This issue will be addressed in a future upgrade of the ARCHER2 operating system. You can workaround this issue by setting the following environment variable in your job submission scripts: export FI_MR_CACHE_MAX_COUNT=0 This may come with a performance penalty (though in many cases, we have not seen a noticeable performance impact). If you continue to see OOM errors after setting this environment variable and do not believe that your application should be requesting too much memory then please contact the service desk","title":"OOM due to memory leak in libfabric (Added: 2022-02-23)"},{"location":"known-issues/#default-fftw-library-points-to-intel-haswell-version-rather-than-amd-rome-at-runtime-added-2022-02-23","text":"By default, and at runtime, the standard FFTW library version (from the module cray-fftw ) will link to a version of the FFTW library optimised for the Intel Haswell architecture rather than the AMD EPYC architecture. This does not cause errors as the instruction set is compatible between the two architectures but may not provide optimal performance. The performance differences observed have been small (< 5%) but if you want to ensure that applications using the cray-fftw module use the correct version of the libraries at runtime, you should add the following lines to your job submission script: module load cray-fftw export LD_LIBRARY_PATH=$CRAY_LD_LIBRARY_PATH:$LD_LIBRARY_PATH The issue arises because the compilers encode a default path to libraries ( /opt/cray/pe/lib64 ) into the RUNPATH of the executable so that you do not need to load all the library module dependencies at runtime. The libraries that the executable finds at /opt/cray/pe/lib64 are soft links to the default versions of the libraries. There is an error in the soft link to the FFTW libraries in this directory such they point to the Haswell version of the FFTW libraries rather than the AMD EPYC (Rome) versions of the libraries.","title":"Default FFTW library points to Intel Haswell version rather than AMD Rome at runtime (Added: 2022-02-23)"},{"location":"known-issues/#occasionally-user-jobs-can-cause-compute-nodes-to-crash-added-2021-11-22","text":"In rare circumstances, it is possible for a user job to crash the compute nodes on which it is running. This is only evident to the user as a failed job: there is no obvious sign that the nodes have crashed. Therefore, if we identify a user whose jobs are causing nodes to crash, we may need to work with them to stop this happening. The underlying issue is resolved in an update to the compute-node operating system (Shasta Version 1.5), which is expected to be rolled out to the main system early in 2022. In the meantime, users who experience this issue are advised to try disabling XPMEM in their application. The ARCHER2 CSE team can provide advice on how to do this. You can contact the CSE team via the service desk .","title":"Occasionally user jobs can cause compute nodes to crash (Added: 2021-11-22)"},{"location":"known-issues/#dask-python-package-missing-dependencies-added-2021-11-22","text":"The Dask Python package is missing some dependencies on the latest Programming Environment (21.09). This can be worked around either by using the default Programming Environment (21.04), or by following the instructions to install dask in your own user space.","title":"Dask Python package missing dependencies (Added: 2021-11-22)"},{"location":"known-issues/#warning-when-compiling-fortran-code-with-cce-and-mpi_f08-interface-added-2021-11-18","text":"When you compile Fortran code using the MPI F08 interface (i.e. use mpi_f08 ) using the default version of CCE (11.0.4) you will see warnings similar to: use mpi_f08 ^ ftn-1753 crayftn: WARNING INTERFACE_MPI, File = interface_mpi_mod.f90, Line = 8, Column = 7 File \"/opt/cray/pe/mpich/8.1.4/ofi/cray/9.1/include/MPI_F08.mod\" containing [sub]module information for \"MPI_F08\" was created with a previous compiler release. It will not be supported by the next major release. It is version 110 from release 9.0. These warnings can be safely ignored as they do not affect the functioning of the code. If you wish to avoid the warnings, you can compile using the more recent CCE version (12.0.3) on the system. To switch to this version, use module load cpe/21.09 from the default environment on ARCHER2.","title":"Warning when compiling Fortran code with CCE and MPI_F08 interface (Added: 2021-11-18)"},{"location":"known-issues/#research-software","text":"There are several outstanding issues for the centrally installed Research Software: PyChemShell is not yet available. We are working with the code developers to address this. PLUMED is not yet available. Currently, we recommend affected users to install a local version of the software. Users should also check individual software pages, for known limitations/ caveats, for the use of software on the Cray EX platform and Cray Linux Environment.","title":"Research Software"},{"location":"known-issues/#issues-with-rpath-for-non-default-library-versions","text":"When you compile applications against non-default versions of libraries within the HPE Cray software stack and use the environment variable CRAY_ADD_RPATH=yes to try and encode the paths to these libraries within the binary this will not be respected at runtime and the binaries will use the default versions instead. The workaround for this issue is to ensure that you set: export LD_LIBRARY_PATH=$CRAY_LD_LIBRARY_PATH:$LD_LIBRARY_PATH at both compile and runtime. For more details on using non-default versions of libraries, see the description in the User and Best Practice Guide","title":"Issues with RPATH for non-default library versions"},{"location":"known-issues/#mpi-ucx-error-ivb_reg_mr","text":"If you are using the UCX layer for MPI communication you may see an error such as: [1613401128.440695] [nid001192:11838:0] ib_md.c:325 UCX ERROR ibv_reg_mr(address=0xabcf12c0, length=26400, access=0xf) failed: Cannot allocate memory [1613401128.440768] [nid001192:11838:0] ucp_mm.c:137 UCX ERROR failed to register address 0xabcf12c0 mem_type bit 0x1 length 26400 on md[4]=mlx5_0: Input/output error (md reg_mem_types 0x15) [1613401128.440773] [nid001192:11838:0] ucp_request.c:269 UCX ERROR failed to register user buffer datatype 0x8 address 0xabcf12c0 len 26400: Input/output error MPICH ERROR [Rank 1534] [job id 114930.0] [Mon Feb 15 14:58:48 2021] [unknown] [nid001192] - Abort(672797967) (rank 1534 in comm 0): Fatal error in PMPI_Isend: Other MPI error, error stack: PMPI_Isend(160)......: MPI_Isend(buf=0xabcf12c0, count=3300, MPI_DOUBLE_PRECISION, dest=1612, tag=4, comm=0x84000004, request=0x7fffb38fa0fc) failed MPID_Isend(416)......: MPID_isend_unsafe(92): MPIDI_UCX_send(95)...: returned failed request in UCX netmod(ucx_send.h 95 MPIDI_UCX_send Input/output error) aborting job: Fatal error in PMPI_Isend: Other MPI error, error stack: PMPI_Isend(160)......: MPI_Isend(buf=0xabcf12c0, count=3300, MPI_DOUBLE_PRECISION, dest=1612, tag=4, comm=0x84000004, request=0x7fffb38fa0fc) failed MPID_Isend(416)......: MPID_isend_unsafe(92): MPIDI_UCX_send(95)...: returned failed request in UCX netmod(ucx_send.h 95 MPIDI_UCX_send Input/output error) [1613401128.457254] [nid001192:11838:0] mm_xpmem.c:82 UCX WARN remote segment id 200002e09 apid 200002e3e is not released, refcount 1 [1613401128.457261] [nid001192:11838:0] mm_xpmem.c:82 UCX WARN remote segment id 200002e08 apid 100002e3e is not released, refcount 1 You can add the following line to your job submission script before the srun command to try and workaround this error: export UCX_IB_REG_METHODS=direct Note Setting this flag may have an impact on code performance.","title":"MPI UCX ERROR: ivb_reg_mr"},{"location":"known-issues/#aocc-compiler-fails-to-compile-with-netcdf-added-2021-11-18","text":"There is currently a problem with the module file which means cray-netcdf-hdf5parallel will not operate correctly in PrgEnv-aocc. An example of the error seen is: F90-F-0004-Corrupt or Old Module file /opt/cray/pe/netcdf-hdf5parallel/4.7.4.3/crayclang/9.1/include/netcdf.mod (netcdf.F90: 8) The current workaround for this is to load module epcc-netcdf-hdf5parallel instead if PrgEnv-aocc is required.","title":"AOCC compiler fails to compile with NetCDF (Added: 2021-11-18)"},{"location":"known-issues/#slurm-export-option-does-not-work-in-job-submission-script","text":"The option --export=ALL propagates all the environment variables from the login node to the compute node. If you include the option in the job submission script, it is wrongly ignored by Slurm. The current workaround is to include the option when the job submission script is launched. For instance: sbatch --export=ALL myjob.slurm","title":"Slurm  --export option does not work in job submission script"},{"location":"known-issues/#recently-resolved-issues","text":"","title":"Recently Resolved Issues"},{"location":"known-issues/#intel-mkl-libraries-fftw-gives-incorrect-results-added-2022-03-28","text":"Until recently, the Intel MKL library modules installed on ARCHER2 set environment variables to enable high-performance code paths. Unfortunately, recent investigations have revealed that these code paths resulted in incorrect results from the FFTW compenents of MKL. The modules were changed on 21 March 2022 to remove these environment variables and we have verified that the FFTW components of the MKL libraries now give the expected results.","title":"Intel MKL libraries: FFTW gives incorrect results (Added: 2022-03-28)"},{"location":"other-software/","text":"Software provided by external parties This section describes software that has been installed on ARCHER2 by external parties (i.e. not by the ARCHER2 service itself) for general use by ARCHER2 users or provides useful notes on software that is not installed centrally. Important While the ARCHER2 service desk is able to provide support for basic use of this software (e.g. access to software, writing job submission scripts) it does not generally provide detailed technical support for the software and you may be directed to seek support from other places if the service desk cannot answer the questions. Research Software CASINO CRYSTAL","title":"Overview"},{"location":"other-software/#software-provided-by-external-parties","text":"This section describes software that has been installed on ARCHER2 by external parties (i.e. not by the ARCHER2 service itself) for general use by ARCHER2 users or provides useful notes on software that is not installed centrally. Important While the ARCHER2 service desk is able to provide support for basic use of this software (e.g. access to software, writing job submission scripts) it does not generally provide detailed technical support for the software and you may be directed to seek support from other places if the service desk cannot answer the questions.","title":"Software provided by external parties"},{"location":"other-software/#research-software","text":"CASINO CRYSTAL","title":"Research Software"},{"location":"other-software/casino/","text":"CASINO Note CASINO is not available as central install/module on ARCHER2 at this time. This page provides tips on using CASINO on ARCHER2 for users who have obtained their own copy of the code. CASINO is a computer program system for performing quantum Monte Carlo (QMC) electronic structure calculations that has been developed by a group of researchers initially working in the Theory of Condensed Matter group in the Cambridge University physics department, and their collaborators, over more than 20 years. It is capable of calculating incredibly accurate solutions to the Schr\u00f6dinger equation of quantum mechanics for realistic systems built from atoms. Useful Links CASINO User Manual CASINO Licensing Compiling CASINO on ARCHER2 You should use the linuxpc-gcc-slurm-parallel.archer2 configuration that is supplied along with the CASINO source code to build on ARCHER2 and ensure that you build the \"Shm\" (System-V shared memory) version of the code. Bug The linuxpc-cray-slurm-parallel.archer2 configuration produces a binary that crashes with a segfault and should not be used. Using CASINO on ARCHER2 The performance of CASINO on ARCHER2 is critically dependent on three things: The MPI transport layer used: UCX is required for good scaling to multiple nodes The number of cores that share System-V shared memory segments: 8 or 16 cores are the best performing choices. If memory efficiency is critical, then 32 cores gives good performance. More than 32 cores sharing a memory segment gives poor performance. Ensuring that MPI processes are pinned to cores in a sequential manner so cores that are sharing shared memory segments are located as close to each other as possible. Next, we show how to make sure that the MPI transport layer is set to UCX, how to set the number of cores sharing the System-V shared memory segments and how to pin MPI processes sequentially to cores. Finally, we provide a job submission script that demonstrates all these options together. Setting the MPI transport layer to UCX In your job submission script that runs CASINO you switch to using UCX as the MPI transport layer by including the following lines before you run CASINO (i.e. before the srun command that launches the CASINO executable): module load PrgEnv-gnu module load craype-network-ucx module load cray-mpich-ucx Setting the number of cores sharing memory In your job submission script you set the number of cores sharing memory segments by setting the CASINO_NUMABLK environment variable before you run CASINO. For example, to specify that there should be shared memory segments each shared between 16 cores, you would use: export CASINO_NUMABLK=16 Tip If you do not set CASINO_NUMABLK then CASINO will use the default of all cores on a node (the equivalent of setting it to 128) which will give very poor performance so you should always set this environment variable. Setting CASINO_NUMABLK to 8 or 16 cores gives the best performance. 32 cores is acceptable if you want to maximise memory efficiency. Using 64 and 128 gives poor performance. Pinning MPI processes sequentially to cores For shared memory segments to work efficiently MPI processes must be pinned sequentially to cores on compute nodes (so that cores sharing memory are close in the node memory hierarchy). To do this, you add the following options to the srun command in your job script that runs the CASINO executable: --distribution=block:block --hint=nomultithread Example CASINO job submission script The following script will run a CASINO job using 16 nodes (2048 cores). #!/bin/bash # Request 16 nodes with 128 MPI tasks per node for 20 minutes #SBATCH --job-name=CASINO #SBATCH --nodes=16 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 #SBATCH --time=00:20:00 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Ensure we are using UCX as the MPI transport layer module load PrgEnv-gnu module load craype-network-ucx module load cray-mpich-ucx # Set CASINO to share memory across 16 core blocks export CASINO_NUMABLK=16 # Set the location of the CASINO executable - this must be on /work # Replace this with the path to your compiled CASINO binary CASINO_EXE=/work/t01/t01/auser/CASINO/bin_qmc/linuxpc-gcc-slurm-parallel.archer2/Shm/opt/casino # Launch CASINO with MPI processes pinned to cores in a sequential order srun --distribution=block:block --hint=nomultithread ${CASINO_EXE} CASINO performance on ARCHER2 We have run the benzene_dimer benchmark on ARCHER2 with the following configuration: Compiler arch: linuxpc-gcc-slurm-parallel.archer2 , \"Shm\" version Compiler: GCC 10.2.0 MPI Library: HPE Cray MPICH 8.1.4 MPI transport layer: UCX 128 MPI processes per node Timings are reported as time taken for 100 equilibration steps in DMC calculation. CASINO_NUMABLK=8 Nodes Time taken (s) Speedup 1 289.90 1.0 2 154.93 1.9 4 81.06 3.6 8 41.44 7.0 16 23.16 12.5","title":"CASINO"},{"location":"other-software/casino/#casino","text":"Note CASINO is not available as central install/module on ARCHER2 at this time. This page provides tips on using CASINO on ARCHER2 for users who have obtained their own copy of the code. CASINO is a computer program system for performing quantum Monte Carlo (QMC) electronic structure calculations that has been developed by a group of researchers initially working in the Theory of Condensed Matter group in the Cambridge University physics department, and their collaborators, over more than 20 years. It is capable of calculating incredibly accurate solutions to the Schr\u00f6dinger equation of quantum mechanics for realistic systems built from atoms.","title":"CASINO"},{"location":"other-software/casino/#useful-links","text":"CASINO User Manual CASINO Licensing","title":"Useful Links"},{"location":"other-software/casino/#compiling-casino-on-archer2","text":"You should use the linuxpc-gcc-slurm-parallel.archer2 configuration that is supplied along with the CASINO source code to build on ARCHER2 and ensure that you build the \"Shm\" (System-V shared memory) version of the code. Bug The linuxpc-cray-slurm-parallel.archer2 configuration produces a binary that crashes with a segfault and should not be used.","title":"Compiling CASINO on ARCHER2"},{"location":"other-software/casino/#using-casino-on-archer2","text":"The performance of CASINO on ARCHER2 is critically dependent on three things: The MPI transport layer used: UCX is required for good scaling to multiple nodes The number of cores that share System-V shared memory segments: 8 or 16 cores are the best performing choices. If memory efficiency is critical, then 32 cores gives good performance. More than 32 cores sharing a memory segment gives poor performance. Ensuring that MPI processes are pinned to cores in a sequential manner so cores that are sharing shared memory segments are located as close to each other as possible. Next, we show how to make sure that the MPI transport layer is set to UCX, how to set the number of cores sharing the System-V shared memory segments and how to pin MPI processes sequentially to cores. Finally, we provide a job submission script that demonstrates all these options together.","title":"Using CASINO on ARCHER2"},{"location":"other-software/casino/#setting-the-mpi-transport-layer-to-ucx","text":"In your job submission script that runs CASINO you switch to using UCX as the MPI transport layer by including the following lines before you run CASINO (i.e. before the srun command that launches the CASINO executable): module load PrgEnv-gnu module load craype-network-ucx module load cray-mpich-ucx","title":"Setting the MPI transport layer to UCX"},{"location":"other-software/casino/#setting-the-number-of-cores-sharing-memory","text":"In your job submission script you set the number of cores sharing memory segments by setting the CASINO_NUMABLK environment variable before you run CASINO. For example, to specify that there should be shared memory segments each shared between 16 cores, you would use: export CASINO_NUMABLK=16 Tip If you do not set CASINO_NUMABLK then CASINO will use the default of all cores on a node (the equivalent of setting it to 128) which will give very poor performance so you should always set this environment variable. Setting CASINO_NUMABLK to 8 or 16 cores gives the best performance. 32 cores is acceptable if you want to maximise memory efficiency. Using 64 and 128 gives poor performance.","title":"Setting the number of cores sharing memory"},{"location":"other-software/casino/#pinning-mpi-processes-sequentially-to-cores","text":"For shared memory segments to work efficiently MPI processes must be pinned sequentially to cores on compute nodes (so that cores sharing memory are close in the node memory hierarchy). To do this, you add the following options to the srun command in your job script that runs the CASINO executable: --distribution=block:block --hint=nomultithread","title":"Pinning MPI processes sequentially to cores"},{"location":"other-software/casino/#example-casino-job-submission-script","text":"The following script will run a CASINO job using 16 nodes (2048 cores). #!/bin/bash # Request 16 nodes with 128 MPI tasks per node for 20 minutes #SBATCH --job-name=CASINO #SBATCH --nodes=16 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 #SBATCH --time=00:20:00 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Ensure we are using UCX as the MPI transport layer module load PrgEnv-gnu module load craype-network-ucx module load cray-mpich-ucx # Set CASINO to share memory across 16 core blocks export CASINO_NUMABLK=16 # Set the location of the CASINO executable - this must be on /work # Replace this with the path to your compiled CASINO binary CASINO_EXE=/work/t01/t01/auser/CASINO/bin_qmc/linuxpc-gcc-slurm-parallel.archer2/Shm/opt/casino # Launch CASINO with MPI processes pinned to cores in a sequential order srun --distribution=block:block --hint=nomultithread ${CASINO_EXE}","title":"Example CASINO job submission script"},{"location":"other-software/casino/#casino-performance-on-archer2","text":"We have run the benzene_dimer benchmark on ARCHER2 with the following configuration: Compiler arch: linuxpc-gcc-slurm-parallel.archer2 , \"Shm\" version Compiler: GCC 10.2.0 MPI Library: HPE Cray MPICH 8.1.4 MPI transport layer: UCX 128 MPI processes per node Timings are reported as time taken for 100 equilibration steps in DMC calculation.","title":"CASINO performance on ARCHER2"},{"location":"other-software/casino/#casino_numablk8","text":"Nodes Time taken (s) Speedup 1 289.90 1.0 2 154.93 1.9 4 81.06 3.6 8 41.44 7.0 16 23.16 12.5","title":"CASINO_NUMABLK=8"},{"location":"other-software/crystal/","text":"CRYSTAL CRYSTAL is a general-purpose program for the study of crystalline solids. The CRYSTAL program computes the electronic structure of periodic systems within Hartree Fock, density functional or various hybrid approximations (global, range-separated and double-hybrids). The Bloch functions of the periodic systems are expanded as linear combinations of atom centred Gaussian functions. Powerful screening techniques are used to exploit real space locality. Restricted (Closed Shell) and Unrestricted (Spin-polarized) calculations can be performed with all-electron and valence-only basis sets with effective core pseudo-potentials. The current release is CRYSTAL17. Useful Links CRYSTAL home site CRYSTAL tutorials CRYSTAL licensing Using CRYSTAL on ARCHER2 CRYSTAL is only available to users who have a valid CRYSTAL license. You request access through SAFE: How to request access to package groups Please have your license details to hand. Running parallel CRYSTAL jobs The following script will run a CRYSTAL job on the ARCHER2 system using 256 cores (2 nodes). It assumes that the input file is tio2.d12 #!/bin/bash #SBATCH --nodes=2 #SBATCH --ntasks-per-node=128 #SBATCH --cpus-per-task=1 #SBATCH --time=0:20:00 # Replace [budget code] below with your project code (e.g. e05) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard module load other-software module load crystal # Change this to the name of your input file cp tio2.d12 INPUT srun --hint = nomultithread --distribution = block:block MPPcrystal Known Issues Out-of-memory errors Long-running jobs may encounter unexpected errors of the form slurmstepd: error: Detected 1 oom-kill event(s) in step 411502.0 cgroup. These are related to a memory leak in the underlying libfabric communication layer, which will be fixed in a future release. In the meantime, it should be possible to work around the problem by adding export FI_MR_CACHE_MAX_COUNT=0 to the SLURM submission script.","title":"CRYSTAL"},{"location":"other-software/crystal/#crystal","text":"CRYSTAL is a general-purpose program for the study of crystalline solids. The CRYSTAL program computes the electronic structure of periodic systems within Hartree Fock, density functional or various hybrid approximations (global, range-separated and double-hybrids). The Bloch functions of the periodic systems are expanded as linear combinations of atom centred Gaussian functions. Powerful screening techniques are used to exploit real space locality. Restricted (Closed Shell) and Unrestricted (Spin-polarized) calculations can be performed with all-electron and valence-only basis sets with effective core pseudo-potentials. The current release is CRYSTAL17.","title":"CRYSTAL"},{"location":"other-software/crystal/#useful-links","text":"CRYSTAL home site CRYSTAL tutorials CRYSTAL licensing","title":"Useful Links"},{"location":"other-software/crystal/#using-crystal-on-archer2","text":"CRYSTAL is only available to users who have a valid CRYSTAL license. You request access through SAFE: How to request access to package groups Please have your license details to hand.","title":"Using CRYSTAL on ARCHER2"},{"location":"other-software/crystal/#running-parallel-crystal-jobs","text":"The following script will run a CRYSTAL job on the ARCHER2 system using 256 cores (2 nodes). It assumes that the input file is tio2.d12 #!/bin/bash #SBATCH --nodes=2 #SBATCH --ntasks-per-node=128 #SBATCH --cpus-per-task=1 #SBATCH --time=0:20:00 # Replace [budget code] below with your project code (e.g. e05) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard module load other-software module load crystal # Change this to the name of your input file cp tio2.d12 INPUT srun --hint = nomultithread --distribution = block:block MPPcrystal","title":"Running parallel CRYSTAL jobs"},{"location":"other-software/crystal/#known-issues","text":"","title":"Known Issues"},{"location":"other-software/crystal/#out-of-memory-errors","text":"Long-running jobs may encounter unexpected errors of the form slurmstepd: error: Detected 1 oom-kill event(s) in step 411502.0 cgroup. These are related to a memory leak in the underlying libfabric communication layer, which will be fixed in a future release. In the meantime, it should be possible to work around the problem by adding export FI_MR_CACHE_MAX_COUNT=0 to the SLURM submission script.","title":"Out-of-memory errors"},{"location":"publish/","text":"ARCHER2 and publications This section provides information on how to acknowledge the use of ARCHER2 in your published work and how to register your work on ARCHER2 into the ARCHER2 publications database via SAFE. Acknowledging ARCHER2 We will shortly be publishing a description of the ARCHER2 service with a DOI that you can cite in your published work that arises from the use of ARCHER2. Until that time, please add the following words to any work you publish that arises from your use of ARCHER2: This work used the ARCHER2 UK National Supercomputing Service (https://www.archer2.ac.uk). You should also tag outputs with the keyword \"ARCHER2\" whenever possible. ARCHER2 publication database The ARCHER2 service maintains a publication database of works that have arisen from ARCHER2 and links them to project IDs that have ARCHER2 access. We ask all users of ARCHER2 to register any publications in the database - all you need is your publication's DOI. Registering your publications in SAFE has a number of advantages: For large projects in particular, it allows the project lead to collect publications from all the project members to assist with ResearchFish submissions Your publication automatically gets added to the ARCHER2 publications page Your publication is included in reports to UKRI The better the record we have of publications arising from the ARCHER2 service, the easier it is to demonstrate the impacts and benefits of the service; allowing UKRI to secure funding for future national supercomputing services. How to register a publication in the database You will need a DOI for the publication you wish to register. A DOI has the form of an set of ID strings separated by slashes. For example, 10.7488/ds/1505 , you should not include the web host address which provides a link to the DOI. Login to SAFE . Then: Go to the Menu Your details and select Publications Select the project you wish to associate the publication with from the list and click View . The next page will list currently registered publications, to add one click Add . Enter the DOI in the text field provided and click Add How to list your publications Login to SAFE . Then: Go to the Menu Your details and select Publications Select the project you wish to list the publications from using the dropdown menu and click View . The next page will list your currently registered publications. How to export your publications At the moment we support export lists of DOIs to comma-separated values (CSV) files. This does not export all the metadata, just the DOIs themselves with a maximum of 25 DOIs per line. This format is primarily useful for importing into ResearchFish (where you can paste in the comma-separated lists to import publications). We plan to add further export formats in the future. Login to SAFE . Then: Go to the Menu Your details and select Publications Select the project you wish to list the publications from using the dropdown menu and click View . The next page will list your currently registered publications. Click Export to generate a plain text comma-separated values (CSV) file that lists all DOIs. If required, you can save this file using the Save command your web browser.","title":"ARCHER2 and Publications"},{"location":"publish/#archer2-and-publications","text":"This section provides information on how to acknowledge the use of ARCHER2 in your published work and how to register your work on ARCHER2 into the ARCHER2 publications database via SAFE.","title":"ARCHER2 and publications"},{"location":"publish/#acknowledging-archer2","text":"We will shortly be publishing a description of the ARCHER2 service with a DOI that you can cite in your published work that arises from the use of ARCHER2. Until that time, please add the following words to any work you publish that arises from your use of ARCHER2: This work used the ARCHER2 UK National Supercomputing Service (https://www.archer2.ac.uk). You should also tag outputs with the keyword \"ARCHER2\" whenever possible.","title":"Acknowledging ARCHER2"},{"location":"publish/#archer2-publication-database","text":"The ARCHER2 service maintains a publication database of works that have arisen from ARCHER2 and links them to project IDs that have ARCHER2 access. We ask all users of ARCHER2 to register any publications in the database - all you need is your publication's DOI. Registering your publications in SAFE has a number of advantages: For large projects in particular, it allows the project lead to collect publications from all the project members to assist with ResearchFish submissions Your publication automatically gets added to the ARCHER2 publications page Your publication is included in reports to UKRI The better the record we have of publications arising from the ARCHER2 service, the easier it is to demonstrate the impacts and benefits of the service; allowing UKRI to secure funding for future national supercomputing services.","title":"ARCHER2 publication database"},{"location":"publish/#how-to-register-a-publication-in-the-database","text":"You will need a DOI for the publication you wish to register. A DOI has the form of an set of ID strings separated by slashes. For example, 10.7488/ds/1505 , you should not include the web host address which provides a link to the DOI. Login to SAFE . Then: Go to the Menu Your details and select Publications Select the project you wish to associate the publication with from the list and click View . The next page will list currently registered publications, to add one click Add . Enter the DOI in the text field provided and click Add","title":"How to register a publication in the database"},{"location":"publish/#how-to-list-your-publications","text":"Login to SAFE . Then: Go to the Menu Your details and select Publications Select the project you wish to list the publications from using the dropdown menu and click View . The next page will list your currently registered publications.","title":"How to list your publications"},{"location":"publish/#how-to-export-your-publications","text":"At the moment we support export lists of DOIs to comma-separated values (CSV) files. This does not export all the metadata, just the DOIs themselves with a maximum of 25 DOIs per line. This format is primarily useful for importing into ResearchFish (where you can paste in the comma-separated lists to import publications). We plan to add further export formats in the future. Login to SAFE . Then: Go to the Menu Your details and select Publications Select the project you wish to list the publications from using the dropdown menu and click View . The next page will list your currently registered publications. Click Export to generate a plain text comma-separated values (CSV) file that lists all DOIs. If required, you can save this file using the Save command your web browser.","title":"How to export your publications"},{"location":"quick-start/overview/","text":"Quickstart The ARCHER2 quickstart guides provide the minimum information for new users or users transferring from ARCHER. There are two sections available which are meant to be followed in sequence. Quickstart for users : Covers the basics of ARCHER2 useful for all users, including: applying for an account, logging in and transferring data, accessing software and running jobs. Quickstart for developers : Covers additional topics around compiling your own programs on ARCHER2, debugging and profiling. If you are not going to be compiling your own programs on ARCHER2, you do not need to follow this guide.","title":"Overview"},{"location":"quick-start/overview/#quickstart","text":"The ARCHER2 quickstart guides provide the minimum information for new users or users transferring from ARCHER. There are two sections available which are meant to be followed in sequence. Quickstart for users : Covers the basics of ARCHER2 useful for all users, including: applying for an account, logging in and transferring data, accessing software and running jobs. Quickstart for developers : Covers additional topics around compiling your own programs on ARCHER2, debugging and profiling. If you are not going to be compiling your own programs on ARCHER2, you do not need to follow this guide.","title":"Quickstart"},{"location":"quick-start/quickstart-developers/","text":"Quickstart for developers This guide aims to quickly enable developers to work on ARCHER2. It assumes that you are familiar with the material in the Quickstart for users section. Compiler wrappers When compiling code on ARCHER2, you should make use of the HPE Cray compiler wrappers. These ensure that the correct libraries and headers (for example, MPI or HPE LibSci) will be used during the compilation and linking stages. These wrappers should be accessed by providing the following compiler names. Language Wrapper name C cc C++ CC Fortran ftn This means that you should use the wrapper names whether on the command line, in build scripts, or in configure options. It could be helpful to set some or all of the following environment variables before running a build to ensure that the build tool is aware of the wrappers. export CC=cc export CXX=CC export FC=ftn export F77=ftn export F90=ftn man pages are available for each wrapper. You can also see the full set of compiler and linker options being used by passing the -craype-verbose option to the wrapper. Tip The HPE Cray compiler wrappers should be used instead of the MPI compiler wrappers such as mpicc , mpicxx and mpif90 that you may have used on other HPC systems. Programming environments On login to ARCHER2, the PrgEnv-cray compiler environment will be loaded, as will a cce module. The latter makes available the Cray compilers from the Cray Compiling Environment (CCE), while the former provides the correct wrappers and support to use them. The GNU Compiler Collection (GCC) and the AMD compiler environment (AOCC) are also available. To make use of any particular compiler environment, you load the correct PrgEnv module. After doing so the compiler wrappers ( cc , CC and ftn ) will correctly call the compilers from the new suite. The default version of the corresponding compiler suite will also be loaded, but you may swap to another available version if you wish. The following table summarises the suites and associated compiler environments. Suite name Module Programming environment collection CCE cce PrgEnv-cray GCC gcc PrgEnv-gnu AOCC aocc PrgEnv-aocc As an example, after logging in you may wish to use GCC as your compiler suite. Running module load PrgEnv-gnu will replace the default CCE (Cray) environment with the GNU environment. It will also unload the cce module and load the default version of the gcc module; at the time of writing, this is GCC 10.2.0. If you need to use a different version of GCC, for example 9.3.0, you would follow up with module swap gcc gcc/9.3.0 . At this point you may invoke the compiler wrappers and they will correctly use the HPE libraries and tools in conjunction with GCC 9.3.0. Warning The gcc/8.1.0 module is available on ARCHER2 but cannot be used as the supporting scientific and system libraries are not available. You should not use this version of GCC. When choosing the compiler environment, a big factor will likely be which compilers you have previously used for your code's development. The Cray Fortran compiler is similar to the compiler you may be familiar with from ARCHER, while the Cray C and C++ compilers provided on ARCHER2 are new versions that are now derived from Clang. The GCC suite provides gcc/g++ and gfortran. The AOCC suite provides AMD Clang/Clang++ and AMD Flang. Note The Intel compilers are not available on ARCHER2. Useful compiler options The compiler options you use will depend on both the software you are building and also on the current stage of development. The following flags should be a good starting point for reasonable performance. Compilers Optimisation flags Cray C/C++ -O2 -funroll-loops -ffast-math Cray Fortran Default options GCC -O2 -ftree-vectorize -funroll-loops -ffast-math Tip If you want to use GCC version 10 or greater to compile MPI Fortran code, you must add the -fallow-argument-mismatch option when compiling otherwise you will see compile errors associated with MPI functions. When you are happy with your code's performance you may wish to enable more aggressive optimisations; in this case you could start using the following flags. Please note, however, that these optimisations may lead to deviations from IEEE/ISO specifications. If your code relies on strict adherence then using these flags may cause incorrect output. Compilers Optimisation flags Cray C/C++ -Ofast -funroll-loops Cray Fortran -O3 -hfp3 GCC -Ofast -funroll-loops Vectorisation is enabled by the Cray Fortran compiler at -O1 and above, by Cray C and C++ at -O2 and above or when using -ftree-vectorize , and by the GCC compilers at -O3 and above or when using -ftree-vectorize . You may wish to promote default real and integer types in Fortran codes from 4 to 8 bytes. In this case, the following flags may be used. Compiler Fortran real and integer promotion flags Cray Fortran -s real64 -s integer64 gfortran -freal-4-real-8 -finteger-4-integer-8 More documentation on the compilers is available through man . The pages to read are accessed as follow. Compiler suite C C++ Fortran Cray man craycc man crayCC man crayftn GNU man gcc man g++ man gfortran Tip There are no man pages for the AOCC compilers at the moment. Linking on ARCHER2 Executables on ARCHER2 link dynamically, and the Cray Programming Environment does not currently support static linking. This is in contrast to ARCHER where the default was to build statically. If you attempt to link statically, you will see errors similar to: /usr/bin/ld: cannot find -lpmi /usr/bin/ld: cannot find -lpmi2 collect2: error: ld returned 1 exit status The compiler wrapper scripts on ARCHER link runtime libraries in using the RUNPATH by default. This means that the paths to the runtime libraries are encoded into the executable so you do not need to load the compiler environment in your job submission scripts. Using RPATHs to link The default behaviour of a dynamically linked executable will be to allow the linker to provide the libraries it needs at runtime by searching the paths in the LD_LIBRARY_PATH environment variable. This is flexible in that it allows an executable to use newly installed library versions without rebuilding, but in some cases you may prefer to bake the paths to specific libraries into the executable, keeping them constant. While the libraries are still dynamically loaded at run time, from the end user's point of view the resulting behaviour will be similar to that of a statically compiled executable in that they will not need to concern themselves with ensuring the linker will be able to find the libraries. This is achieved by providing RPATHs to the compiler as options. To set the compiler wrappers to do this, you can set the following environment variable. export CRAY_ADD_RPATH=yes You can also provide RPATHs directly to the compilers using the -Wl,-rpath=<path-to-directory> flag, where the provided path is to the directory containing the libraries which are themselves typically specified with flags of the type -l<library-name> . Debugging tools The following debugging tools are available on ARCHER2: gdb4hpc is a command-line tool working similarly to gdb that allows users to debug parallel programs. It can launch parallel programs or attach to ones already running and allows the user to step through the execution to identify the causes of any unexpected behaviour. Available via module load gdb4hpc . valgrind4hpc is a parallel memory debugging tool that aids in detection of memory leaks and errors in parallel applications. It aggregates like errors across processes and threads to simplify debugging of parallel appliciations. Available via module load valgrind4hpc . STAT , the Stack Trace Analysis Tool, generates merged stack traces for parallel applications. It also provides visualisation tools. Available via module load cray-stat . To get started debugging on ARCHER2, you might like to use gdb4hpc. You should first of all compile your code using the -g flag to enable debugging symbols. Once compiled, load the gdb4hpc module and start it: module load gdb4hpc gdb4hpc Once inside gdb4hpc, you can start your program's execution with the launch command: dbg all> launch $my_prog{128} ./prog In this example, a job called my_prog will be launched to run the executable file prog over 128 cores on a compute node. If you run squeue in another terminal you will be able to see it running. Inside gdb4hpc you may then step through the code's execution, continue to breakpoints that you set with break , print the values of variables at these points, and perform a backtrace on the stack if the program crashes. Debugging jobs will end when you exit gdb4hpc, or you can end them yourself by running, in this example, release $my_prog . For more information on debugging parallel codes, see the documentation in the Debugging section of the ARCHER2 User and Best Practice Guide. Profiling tools Profiling on ARCHER2 is provided through the Cray Performance Measurement and Analysis Tools (CrayPAT). This has a number of different components: CrayPAT the full-featured program analysis tool set. CrayPAT consists of pat_build , the utility used to instrument programs, the CrayPat run time environment, which collects the specified performance data during program execution, and pat_report , the first-level data analysis tool, used to produce text reports or export data for more sophisticated analysis. CrayPAT-lite a simplified and easy-to-use version of CrayPAT that provides basic performance analysis information automatically, with a minimum of user interaction. Reveal the next-generation integrated performance analysis and code optimization tool, which enables the user to correlate performance data captured during program execution directly to the original source, and identify opportunities for further optimization. Cray PAPI components, which are support packages for those who want to access performance counters. Cray Apprentice2 the second-level data analysis tool, used to visualize, manipulate, explore, and compare sets of program performance data in a GUI environment. The above tools are made available for use by firstly loading the perftools-base module followed by either perftools (for CrayPAT, Reveal and Apprentice2) or one of the perftools-lite modules. The simplest way to get started profiling your code is with CrayPAT-lite. For example, to sample a run of a code you would load the perftools-base and perftools-lite modules, and then compile (you will receive a message that the executable is being instrumented). Performing a batch run as usual with this executable will produce a directory such as my_prog+74653-2s which can be passed to pat_report to view the results. In this example, pat_report -O calltree+src my_prog+74653-2s will produce a report containing the call tree. You can view available report keywords to be provided to the -O option by running pat_report -O -h . The available perftools-lite modules are: perftools-lite , instrumenting a basic sampling experiment. perftools-lite-events , instrumenting a tracing experiment. perftools-lite-gpu , instrumenting OpenACC and OpenMP 4 use of GPUs. perftools-lite-hbm , instrumenting for memory bandwidth usage. perftools-lite-loops , instrumenting a loop work estimate experiment. Tip For more information on profiling parallel codes, see the documentation in the Profiling section of the ARCHER2 User and Best Practice Guide. Useful Links Links to other documentation you may find useful: ARCHER2 User and Best Practice Guide - Covers all aspects of use of the ARCHER2 service. This includes fundamentals (required by all users to use the system effectively), best practice for getting the most out of ARCHER2, and more advanced technical topics. HPE Cray Programming Environment User Guide HPE Cray Performance Measurement and Analysis Tools User Guide","title":"Quickstart for developers"},{"location":"quick-start/quickstart-developers/#quickstart-for-developers","text":"This guide aims to quickly enable developers to work on ARCHER2. It assumes that you are familiar with the material in the Quickstart for users section.","title":"Quickstart for developers"},{"location":"quick-start/quickstart-developers/#compiler-wrappers","text":"When compiling code on ARCHER2, you should make use of the HPE Cray compiler wrappers. These ensure that the correct libraries and headers (for example, MPI or HPE LibSci) will be used during the compilation and linking stages. These wrappers should be accessed by providing the following compiler names. Language Wrapper name C cc C++ CC Fortran ftn This means that you should use the wrapper names whether on the command line, in build scripts, or in configure options. It could be helpful to set some or all of the following environment variables before running a build to ensure that the build tool is aware of the wrappers. export CC=cc export CXX=CC export FC=ftn export F77=ftn export F90=ftn man pages are available for each wrapper. You can also see the full set of compiler and linker options being used by passing the -craype-verbose option to the wrapper. Tip The HPE Cray compiler wrappers should be used instead of the MPI compiler wrappers such as mpicc , mpicxx and mpif90 that you may have used on other HPC systems.","title":"Compiler wrappers"},{"location":"quick-start/quickstart-developers/#programming-environments","text":"On login to ARCHER2, the PrgEnv-cray compiler environment will be loaded, as will a cce module. The latter makes available the Cray compilers from the Cray Compiling Environment (CCE), while the former provides the correct wrappers and support to use them. The GNU Compiler Collection (GCC) and the AMD compiler environment (AOCC) are also available. To make use of any particular compiler environment, you load the correct PrgEnv module. After doing so the compiler wrappers ( cc , CC and ftn ) will correctly call the compilers from the new suite. The default version of the corresponding compiler suite will also be loaded, but you may swap to another available version if you wish. The following table summarises the suites and associated compiler environments. Suite name Module Programming environment collection CCE cce PrgEnv-cray GCC gcc PrgEnv-gnu AOCC aocc PrgEnv-aocc As an example, after logging in you may wish to use GCC as your compiler suite. Running module load PrgEnv-gnu will replace the default CCE (Cray) environment with the GNU environment. It will also unload the cce module and load the default version of the gcc module; at the time of writing, this is GCC 10.2.0. If you need to use a different version of GCC, for example 9.3.0, you would follow up with module swap gcc gcc/9.3.0 . At this point you may invoke the compiler wrappers and they will correctly use the HPE libraries and tools in conjunction with GCC 9.3.0. Warning The gcc/8.1.0 module is available on ARCHER2 but cannot be used as the supporting scientific and system libraries are not available. You should not use this version of GCC. When choosing the compiler environment, a big factor will likely be which compilers you have previously used for your code's development. The Cray Fortran compiler is similar to the compiler you may be familiar with from ARCHER, while the Cray C and C++ compilers provided on ARCHER2 are new versions that are now derived from Clang. The GCC suite provides gcc/g++ and gfortran. The AOCC suite provides AMD Clang/Clang++ and AMD Flang. Note The Intel compilers are not available on ARCHER2.","title":"Programming environments"},{"location":"quick-start/quickstart-developers/#useful-compiler-options","text":"The compiler options you use will depend on both the software you are building and also on the current stage of development. The following flags should be a good starting point for reasonable performance. Compilers Optimisation flags Cray C/C++ -O2 -funroll-loops -ffast-math Cray Fortran Default options GCC -O2 -ftree-vectorize -funroll-loops -ffast-math Tip If you want to use GCC version 10 or greater to compile MPI Fortran code, you must add the -fallow-argument-mismatch option when compiling otherwise you will see compile errors associated with MPI functions. When you are happy with your code's performance you may wish to enable more aggressive optimisations; in this case you could start using the following flags. Please note, however, that these optimisations may lead to deviations from IEEE/ISO specifications. If your code relies on strict adherence then using these flags may cause incorrect output. Compilers Optimisation flags Cray C/C++ -Ofast -funroll-loops Cray Fortran -O3 -hfp3 GCC -Ofast -funroll-loops Vectorisation is enabled by the Cray Fortran compiler at -O1 and above, by Cray C and C++ at -O2 and above or when using -ftree-vectorize , and by the GCC compilers at -O3 and above or when using -ftree-vectorize . You may wish to promote default real and integer types in Fortran codes from 4 to 8 bytes. In this case, the following flags may be used. Compiler Fortran real and integer promotion flags Cray Fortran -s real64 -s integer64 gfortran -freal-4-real-8 -finteger-4-integer-8 More documentation on the compilers is available through man . The pages to read are accessed as follow. Compiler suite C C++ Fortran Cray man craycc man crayCC man crayftn GNU man gcc man g++ man gfortran Tip There are no man pages for the AOCC compilers at the moment.","title":"Useful compiler options"},{"location":"quick-start/quickstart-developers/#linking-on-archer2","text":"Executables on ARCHER2 link dynamically, and the Cray Programming Environment does not currently support static linking. This is in contrast to ARCHER where the default was to build statically. If you attempt to link statically, you will see errors similar to: /usr/bin/ld: cannot find -lpmi /usr/bin/ld: cannot find -lpmi2 collect2: error: ld returned 1 exit status The compiler wrapper scripts on ARCHER link runtime libraries in using the RUNPATH by default. This means that the paths to the runtime libraries are encoded into the executable so you do not need to load the compiler environment in your job submission scripts.","title":"Linking on ARCHER2"},{"location":"quick-start/quickstart-developers/#using-rpaths-to-link","text":"The default behaviour of a dynamically linked executable will be to allow the linker to provide the libraries it needs at runtime by searching the paths in the LD_LIBRARY_PATH environment variable. This is flexible in that it allows an executable to use newly installed library versions without rebuilding, but in some cases you may prefer to bake the paths to specific libraries into the executable, keeping them constant. While the libraries are still dynamically loaded at run time, from the end user's point of view the resulting behaviour will be similar to that of a statically compiled executable in that they will not need to concern themselves with ensuring the linker will be able to find the libraries. This is achieved by providing RPATHs to the compiler as options. To set the compiler wrappers to do this, you can set the following environment variable. export CRAY_ADD_RPATH=yes You can also provide RPATHs directly to the compilers using the -Wl,-rpath=<path-to-directory> flag, where the provided path is to the directory containing the libraries which are themselves typically specified with flags of the type -l<library-name> .","title":"Using RPATHs to link"},{"location":"quick-start/quickstart-developers/#debugging-tools","text":"The following debugging tools are available on ARCHER2: gdb4hpc is a command-line tool working similarly to gdb that allows users to debug parallel programs. It can launch parallel programs or attach to ones already running and allows the user to step through the execution to identify the causes of any unexpected behaviour. Available via module load gdb4hpc . valgrind4hpc is a parallel memory debugging tool that aids in detection of memory leaks and errors in parallel applications. It aggregates like errors across processes and threads to simplify debugging of parallel appliciations. Available via module load valgrind4hpc . STAT , the Stack Trace Analysis Tool, generates merged stack traces for parallel applications. It also provides visualisation tools. Available via module load cray-stat . To get started debugging on ARCHER2, you might like to use gdb4hpc. You should first of all compile your code using the -g flag to enable debugging symbols. Once compiled, load the gdb4hpc module and start it: module load gdb4hpc gdb4hpc Once inside gdb4hpc, you can start your program's execution with the launch command: dbg all> launch $my_prog{128} ./prog In this example, a job called my_prog will be launched to run the executable file prog over 128 cores on a compute node. If you run squeue in another terminal you will be able to see it running. Inside gdb4hpc you may then step through the code's execution, continue to breakpoints that you set with break , print the values of variables at these points, and perform a backtrace on the stack if the program crashes. Debugging jobs will end when you exit gdb4hpc, or you can end them yourself by running, in this example, release $my_prog . For more information on debugging parallel codes, see the documentation in the Debugging section of the ARCHER2 User and Best Practice Guide.","title":"Debugging tools"},{"location":"quick-start/quickstart-developers/#profiling-tools","text":"Profiling on ARCHER2 is provided through the Cray Performance Measurement and Analysis Tools (CrayPAT). This has a number of different components: CrayPAT the full-featured program analysis tool set. CrayPAT consists of pat_build , the utility used to instrument programs, the CrayPat run time environment, which collects the specified performance data during program execution, and pat_report , the first-level data analysis tool, used to produce text reports or export data for more sophisticated analysis. CrayPAT-lite a simplified and easy-to-use version of CrayPAT that provides basic performance analysis information automatically, with a minimum of user interaction. Reveal the next-generation integrated performance analysis and code optimization tool, which enables the user to correlate performance data captured during program execution directly to the original source, and identify opportunities for further optimization. Cray PAPI components, which are support packages for those who want to access performance counters. Cray Apprentice2 the second-level data analysis tool, used to visualize, manipulate, explore, and compare sets of program performance data in a GUI environment. The above tools are made available for use by firstly loading the perftools-base module followed by either perftools (for CrayPAT, Reveal and Apprentice2) or one of the perftools-lite modules. The simplest way to get started profiling your code is with CrayPAT-lite. For example, to sample a run of a code you would load the perftools-base and perftools-lite modules, and then compile (you will receive a message that the executable is being instrumented). Performing a batch run as usual with this executable will produce a directory such as my_prog+74653-2s which can be passed to pat_report to view the results. In this example, pat_report -O calltree+src my_prog+74653-2s will produce a report containing the call tree. You can view available report keywords to be provided to the -O option by running pat_report -O -h . The available perftools-lite modules are: perftools-lite , instrumenting a basic sampling experiment. perftools-lite-events , instrumenting a tracing experiment. perftools-lite-gpu , instrumenting OpenACC and OpenMP 4 use of GPUs. perftools-lite-hbm , instrumenting for memory bandwidth usage. perftools-lite-loops , instrumenting a loop work estimate experiment. Tip For more information on profiling parallel codes, see the documentation in the Profiling section of the ARCHER2 User and Best Practice Guide.","title":"Profiling tools"},{"location":"quick-start/quickstart-developers/#useful-links","text":"Links to other documentation you may find useful: ARCHER2 User and Best Practice Guide - Covers all aspects of use of the ARCHER2 service. This includes fundamentals (required by all users to use the system effectively), best practice for getting the most out of ARCHER2, and more advanced technical topics. HPE Cray Programming Environment User Guide HPE Cray Performance Measurement and Analysis Tools User Guide","title":"Useful Links"},{"location":"quick-start/quickstart-users/","text":"Quickstart for users This guide aims to quickly enable new users to get up and running on ARCHER2. It covers the process of getting an ARCHER2 account, logging in and running your first job. Important This guide covers both the ARCHER2 4-cabinet system and the ARCHER2 full system. Please ensure you follow the instructions for the correct system. Request an account on ARCHER2 Important You need to use both a password and a passphrase-protected SSH key pair to log into ARCHER2. You get the password from SAFE, but, you will also need to setup your own SSH key pair and add the public part to your account via SAFE before you will be able to log in. We cover the authentication steps below. Obtain an account on the SAFE website The first step is to sign up for an account on the ARCHER2 SAFE website. The SAFE account is used to manage all of your login accounts, allowing you to report on your usage and quotas. To do this: Go to the SAFE New User Signup Form Fill in your personal details. You can come back later and change them if you wish Click Submit You are now registered. Your SAFE password will be emailed to the email address you provided. You can then login with that email address and password. (You can change your initial SAFE password whenever you want by selecting the Change SAFE password option from the Your details menu.) Request an ARCHER2 login account Once you have a SAFE account and an SSH key you will need to request a user account on ARCHER2 itself. To do this you will require a Project Code ; you usually obtain this from the Principle Investigator (PI) or project manager for the project you will be working on. Once you have the Project Code: Full system Log into SAFE Use the Login accounts - Request new account menu item Select the correct project from the drop down list Select the archer2 machine in the list of available machines Click Next Enter a username for the account and (optionally) an SSH public key If you do not specify an SSH key at this stage, your default key will be used (if you have one). For users who had an ARCHER account, the default key will be your ARCHER SSH key. You can always add an SSH key (or additional SSH keys) using the process described below. Click Request The PI or project manager of the project will be asked to approve your request. After your request has been approved the account will be created and when this has been done you will receive an email. You can then come back to SAFE and pick up the initial single-use password for your new account. Note ARCHER2 account passwords are also sometimes referred to as LDAP passwords by the system. Generating and adding an SSH key pair How you generate your SSH key pair depends on which operating system you use and which SSH client you use to connect to ARCHER2. We will not cover the details on generating an SSH key pair here, but detailed information on this topic is available in the ARCHER2 User and Best Practice Guide . After generating your SSH key pair, add the public part to your login account using SAFE: Log into SAFE Use the menu Login accounts and select the ARCHER2 account to be associated with the SSH key On the subsequent Login account details page, click the Add Credential button Select SSH public key as the Credential Type and click Next Either copy and paste the public part of your SSH key into the SSH Public key box or use the button to select the public key file on your computer Click Add to associate the public SSH key part with your account Once you have done this, your SSH key will be added to your ARCHER2 account. Remember, you will need to use both an SSH key and password to log into ARCHER2 so you will also need to collect your initial password before you can log into ARCHER2 for the first time. We cover this next. Note If you want to connect to ARCHER2 from more than one machine, e.g. from your home laptop as well as your work laptop, you should generate an ssh key on each machine, and add each of the public keys into SAFE. Collecting your ARCHER2 password You should now collect your ARCHER2 password: Log into SAFE Use the Login accounts menu to select your new login account Use the View Login Account Password button to view your single-use ARCHER2 password This password is generated randomly by the software. It's best to copy-and-paste it across when you log in to ARCHER2. After you login, you will immediately be prompted to begin the process of changing your password. You should now enter the initial password again, and then you will be prompted for your new, easy-to-remember password. Your new password should conform to the ARCHER2 Password Policy . Note The View Login Account Password option within SAFE will continue to display your old initial password. Your SAFE account has no knowledge of your new machine account password. Login to ARCHER2 To log into ARCHER2 you should use the address: Full system ssh [userID]@login.archer2.ac.uk The order in which you are asked for credentials depends on the system you are accessing: Full system You will first be prompted for the passphrase associated with your SSH key pair. Once you have entered this passphrase successfully, you will then be prompted for your machine account password. You need to enter both credentials correctly to be able to access ARCHER2. Tip If you previously logged into the 4-cabinet system with your account you may see an error from SSH that looks like @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ @ WARNING: POSSIBLE DNS SPOOFING DETECTED! @ @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ The ECDSA host key for login.archer2.ac.uk has changed, and the key for the corresponding IP address 193.62.216.43 has a different value. This could either mean that DNS SPOOFING is happening or the IP address for the host and its host key have changed at the same time. Offending key for IP in /Users/auser/.ssh/known_hosts:11 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ @ WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED! @ @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY! Someone could be eavesdropping on you right now (man-in-the-middle attack)! It is also possible that a host key has just been changed. The fingerprint for the ECDSA key sent by the remote host is SHA256:UGS+LA8I46LqnD58WiWNlaUFY3uD1WFr+V8RCG09fUg. Please contact your system administrator. If you see this, you should delete the offending host key from your ~/.ssh/known_hosts file (in the example above the offending line is line #11) Tip If your SSH key pair is not stored in the default location (usually ~/.ssh/id_rsa ) on your local system, you may need to specify the path to the private part of the key wih the -i option to ssh . For example, if your key is in a file called keys/id_rsa_archer2 you would use the command ssh -i keys/id_rsa_archer2 username@login.archer2.ac.uk to log in. Tip When you first log into ARCHER2, you will be prompted to change your initial password. This is a three step process: When prompted to enter your ldap password : re-enter the password you retrieved from SAFE When prompted to enter your new password: type in a new password When prompted to re-enter the new password: re-enter the new password Your password has now been changed. Hint More information on connecting to ARCHER2 is available in the Connecting to ARCHER2 section of the User Guide. File systems and manipulating data ARCHER2 has a number of different file systems and understanding the difference between them is crucial to being able to use the system. In particular, transferring and moving data often requires a bit of thought in advance to ensure that the data is secure and in a useful form. ARCHER2 file systems are: home file systems: backed up. Available on login and data analysis nodes. work file systems: not backed-up. Available on login, data analysis and compute nodes. All users have a directory on one of the home file systems and on one of the work file systems. The directories are located at: /home/[project ID]/[project ID]/[user ID] (this is also set as your home directory) /work/[project ID]/[project ID]/[user ID] Top tips for managing data on ARCHER2: Do not generate huge numbers of files (>1000) in a single directory. Poor performance relating to file transfer is often due to the number of files involved in the transfer - minimise the number of files that you have to transfer by using archiving tools to improve performance. Archive directories or large numbers of files before moving them between file systems (e.g. by using commands like tar or zip ). When using tar or rsync between file systems mounted on ARCHER2 avoid the use of compression options as these can slow performance (time saved by transferring smaller compressed files is usually less than the overhead added by having to compress files on the fly). Think about automating the merging and transfer of multiple files output by software on ARCHER2 to other resources. The Data Management Guide linked below provides examples of how to automatically verify the integrity of an archive. Hint Information on the file systems and best practice in managing you data is available in the Data management and transfer section of the User and Best Practice Guide. Accessing software Software on ARCHER2 is principally accessed through modules . These load and unload the desired applications, compilers, tools and libraries through the module command and its subcommands. Some modules will be loaded by default on login, providing a default working environment; many more will be available for use but initially unloaded, allowing you to set up the environment to suit your needs. At any stage you can check which modules have been loaded by running module list Running the following command will display all environment modules available on ARCHER2, whether loaded or unloaded module avail The search field for this command may be narrowed by providing the first few characters of the module name being queried. For example, all available versions and variants of VASP may be found by running module avail vasp You will see that different versions are available for many modules. For example, vasp/5/5.4.4.pl2 and vasp/6/6.1.0 are two available versions of VASP on the full system. Furthermore, a default version may be specified; this is used if no version is provided by the user. Important VASP is licensed software, as are other software packages on ARCHER2. You must have a valid licence to use licensed software on ARCHER2. Often you will need to request access through the SAFE. More on this below. The module load command loads a module for use. Following the above, module load vasp/5 would load the default version of VASP 5, while module load vasp/5/5.4.4.pl2 would specifically load version 5.4.4.pl2 . A loaded module may be unloaded through the identical module remove command, e.g. module unload vasp The above unloads whichever version of VASP is currently in the environment. Rather than issuing separate unload and load commands, versions of a module may be swapped as follows: module swap vasp vasp/5/5.4.4.pl2 Other helpful commands are: module help <modulename> which provides a short description of the module module show <modulename> which displays the contents of the modulefile Points to be aware of include: Some modules will conflict with others. A simple example would be the conflict arising when trying to load a different version of an already loaded module. When a conflict occurs, the loading process will fail and an error message will be displayed. Examination of the message and the module output (via module show ) should reveal the cause of the conflict and how to resolve it. The order in which modules are loaded can matter. Consider two modules which set the same variable to a different value. The final value would be that set by the module which loaded last. If you suspect that two modules may be interfering with one another, you can examine their contents with module show . More information on modules and the software environment on ARCHER2 can be found in the Software environment section of the User and Best Practice Guide. Requesting access to licensed software Some of the software installed on ARCHER2 requires a user to have a valid licence agreed with the software owners/developers to be able to use it (for example, VASP). Although you will be able to load this software on ARCHER2, you will be barred from actually using it until your licence has been verified. You request access to licensed software through the SAFE (the web administration tool you used to apply for your account and retrieve your initial password) by being added to the appropriate Package Group . To request access to licensed software: Log in to SAFE Go to the Menu Login accounts and select the login account which requires access to the software Click New Package Group Request Select the software from the list of available packages and click Select Package Group Fill in as much information as possible about your license; at the very least provide the information requested at the top of the screen such as the licence holder's name and contact details. If you are covered by the license because the licence holder is your supervisor, for example, please state this. Click Submit Your request will then be processed by the ARCHER2 Service Desk who will confirm your license with the software owners/developers before enabling your access to the software on ARCHER2. This can take several days (depending on how quickly the software owners/developers take to respond) but you will be advised once this has been done. Create a job submission script To run a program on the ARCHER2 compute nodes you need to write a job submission script that tells the system how many compute nodes you want to reserve and for how long. You also need to use the srun command to launch your parallel executable. Hint For a more details on the Slurm scheduler on ARCHER2 and writing job submission scripts see the Running jobs on ARCHER2 section of the User and Best Practice Guide. Important Parallel jobs on ARCHER2 should be run from the work file systems as the home file systems are not available on the compute nodes - you will see a chdir or file not found error if you try to access data on the home file system within a parallel job running on the compute nodes. Create a job submission script called submit.slurm in your space on the work file systems using your favourite text editor. For example, using vim : auser@ln01:~> cd /work/t01/t01/auser auser@ln01:/work/t01/t01/auser> vim submit.slurm Tip You will need to use your project code and username to get to the correct directory. i.e. replace the t01 above with your project code and replace the username auser with your ARCHER2 username. Paste the following text into your job submission script, replacing ENTER_YOUR_BUDGET_CODE_HERE with your budget code e.g. e99-ham , ENTER_PARTITION_HERE with the partition you wish to run on (e.g standard ), and ENTER_QOS_HERE with the quality of service you want (e.g. standard ). Full system #!/bin/bash --login #SBATCH --job-name=test_job #SBATCH --nodes=1 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 #SBATCH --time=0:5:0 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Load the xthi module to get access to the xthi program module load xthi # srun launches the parallel program based on the SBATCH options srun --distribution=block:block --hint=nomultithread xthi Submit your job to the queue You submit your job to the queues using the sbatch command: auser@ln01:/work/t01/t01/auser> sbatch submit.slurm Submitted batch job 23996 The value returned is your *Job ID*. Monitoring your job You use the squeue command to examine jobs in the queue. To list all the jobs you have in the queue, use: auser@ln01:/work/t01/t01/auser> squeue -u $USER squeue on its own lists all jobs in the queue from all users. Checking the output from the job The job submission script above should write the output to a file called slurm-<jobID>.out (i.e. if the Job ID was 23996, the file would be slurm-23996.out ), you can check the contents of this file with the cat command. If the job was successful you should see output that looks something like: auser@ln01:/work/t01/t01/auser> cat slurm-23996.out Node 0, hostname nid001020 Node 0, rank 0, thread 0, (affinity = 0) Node 0, rank 1, thread 0, (affinity = 1) Node 0, rank 2, thread 0, (affinity = 2) Node 0, rank 3, thread 0, (affinity = 3) Node 0, rank 4, thread 0, (affinity = 4) Node 0, rank 5, thread 0, (affinity = 5) Node 0, rank 6, thread 0, (affinity = 6) Node 0, rank 7, thread 0, (affinity = 7) Node 0, rank 8, thread 0, (affinity = 8) Node 0, rank 9, thread 0, (affinity = 9) Node 0, rank 10, thread 0, (affinity = 10) Node 0, rank 11, thread 0, (affinity = 11) Node 0, rank 12, thread 0, (affinity = 12) Node 0, rank 13, thread 0, (affinity = 13) Node 0, rank 14, thread 0, (affinity = 14) Node 0, rank 15, thread 0, (affinity = 15) Node 0, rank 16, thread 0, (affinity = 16) Node 0, rank 17, thread 0, (affinity = 17) Node 0, rank 18, thread 0, (affinity = 18) Node 0, rank 19, thread 0, (affinity = 19) Node 0, rank 20, thread 0, (affinity = 20) Node 0, rank 21, thread 0, (affinity = 21) ... output trimmed ... If something has gone wrong, you will find any error messages in the file instead of the expected output. Acknowledging ARCHER2 You should use the following phrase to acknowledge ARCHER2 for all research outputs that were generated using the ARCHER2 service: This work used the ARCHER2 UK National Supercomputing Service (https://www.archer2.ac.uk). You should also tag outputs with the keyword \"ARCHER2\" whenever possible. Useful Links If you plan to compile your own programs on ARCHER2, you may also want to look at Quickstart for developers . Other documentation you may find useful: ARCHER2 User and Best Practice Guide : Covers all aspects of use of the ARCHER2 service. This includes fundamentals (required by all users to use the system effectively), best practice for getting the most out of ARCHER2, and more advanced technical topics. HPE Cray Programming Environment User Guide","title":"Quickstart for users"},{"location":"quick-start/quickstart-users/#quickstart-for-users","text":"This guide aims to quickly enable new users to get up and running on ARCHER2. It covers the process of getting an ARCHER2 account, logging in and running your first job. Important This guide covers both the ARCHER2 4-cabinet system and the ARCHER2 full system. Please ensure you follow the instructions for the correct system.","title":"Quickstart for users"},{"location":"quick-start/quickstart-users/#request-an-account-on-archer2","text":"Important You need to use both a password and a passphrase-protected SSH key pair to log into ARCHER2. You get the password from SAFE, but, you will also need to setup your own SSH key pair and add the public part to your account via SAFE before you will be able to log in. We cover the authentication steps below.","title":"Request an account on ARCHER2"},{"location":"quick-start/quickstart-users/#obtain-an-account-on-the-safe-website","text":"The first step is to sign up for an account on the ARCHER2 SAFE website. The SAFE account is used to manage all of your login accounts, allowing you to report on your usage and quotas. To do this: Go to the SAFE New User Signup Form Fill in your personal details. You can come back later and change them if you wish Click Submit You are now registered. Your SAFE password will be emailed to the email address you provided. You can then login with that email address and password. (You can change your initial SAFE password whenever you want by selecting the Change SAFE password option from the Your details menu.)","title":"Obtain an account on the SAFE website"},{"location":"quick-start/quickstart-users/#request-an-archer2-login-account","text":"Once you have a SAFE account and an SSH key you will need to request a user account on ARCHER2 itself. To do this you will require a Project Code ; you usually obtain this from the Principle Investigator (PI) or project manager for the project you will be working on. Once you have the Project Code: Full system Log into SAFE Use the Login accounts - Request new account menu item Select the correct project from the drop down list Select the archer2 machine in the list of available machines Click Next Enter a username for the account and (optionally) an SSH public key If you do not specify an SSH key at this stage, your default key will be used (if you have one). For users who had an ARCHER account, the default key will be your ARCHER SSH key. You can always add an SSH key (or additional SSH keys) using the process described below. Click Request The PI or project manager of the project will be asked to approve your request. After your request has been approved the account will be created and when this has been done you will receive an email. You can then come back to SAFE and pick up the initial single-use password for your new account. Note ARCHER2 account passwords are also sometimes referred to as LDAP passwords by the system.","title":"Request an ARCHER2 login account"},{"location":"quick-start/quickstart-users/#generating-and-adding-an-ssh-key-pair","text":"How you generate your SSH key pair depends on which operating system you use and which SSH client you use to connect to ARCHER2. We will not cover the details on generating an SSH key pair here, but detailed information on this topic is available in the ARCHER2 User and Best Practice Guide . After generating your SSH key pair, add the public part to your login account using SAFE: Log into SAFE Use the menu Login accounts and select the ARCHER2 account to be associated with the SSH key On the subsequent Login account details page, click the Add Credential button Select SSH public key as the Credential Type and click Next Either copy and paste the public part of your SSH key into the SSH Public key box or use the button to select the public key file on your computer Click Add to associate the public SSH key part with your account Once you have done this, your SSH key will be added to your ARCHER2 account. Remember, you will need to use both an SSH key and password to log into ARCHER2 so you will also need to collect your initial password before you can log into ARCHER2 for the first time. We cover this next. Note If you want to connect to ARCHER2 from more than one machine, e.g. from your home laptop as well as your work laptop, you should generate an ssh key on each machine, and add each of the public keys into SAFE.","title":"Generating and adding an SSH key pair"},{"location":"quick-start/quickstart-users/#collecting-your-archer2-password","text":"You should now collect your ARCHER2 password: Log into SAFE Use the Login accounts menu to select your new login account Use the View Login Account Password button to view your single-use ARCHER2 password This password is generated randomly by the software. It's best to copy-and-paste it across when you log in to ARCHER2. After you login, you will immediately be prompted to begin the process of changing your password. You should now enter the initial password again, and then you will be prompted for your new, easy-to-remember password. Your new password should conform to the ARCHER2 Password Policy . Note The View Login Account Password option within SAFE will continue to display your old initial password. Your SAFE account has no knowledge of your new machine account password.","title":"Collecting your ARCHER2 password"},{"location":"quick-start/quickstart-users/#login-to-archer2","text":"To log into ARCHER2 you should use the address: Full system ssh [userID]@login.archer2.ac.uk The order in which you are asked for credentials depends on the system you are accessing: Full system You will first be prompted for the passphrase associated with your SSH key pair. Once you have entered this passphrase successfully, you will then be prompted for your machine account password. You need to enter both credentials correctly to be able to access ARCHER2. Tip If you previously logged into the 4-cabinet system with your account you may see an error from SSH that looks like @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ @ WARNING: POSSIBLE DNS SPOOFING DETECTED! @ @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ The ECDSA host key for login.archer2.ac.uk has changed, and the key for the corresponding IP address 193.62.216.43 has a different value. This could either mean that DNS SPOOFING is happening or the IP address for the host and its host key have changed at the same time. Offending key for IP in /Users/auser/.ssh/known_hosts:11 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ @ WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED! @ @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY! Someone could be eavesdropping on you right now (man-in-the-middle attack)! It is also possible that a host key has just been changed. The fingerprint for the ECDSA key sent by the remote host is SHA256:UGS+LA8I46LqnD58WiWNlaUFY3uD1WFr+V8RCG09fUg. Please contact your system administrator. If you see this, you should delete the offending host key from your ~/.ssh/known_hosts file (in the example above the offending line is line #11) Tip If your SSH key pair is not stored in the default location (usually ~/.ssh/id_rsa ) on your local system, you may need to specify the path to the private part of the key wih the -i option to ssh . For example, if your key is in a file called keys/id_rsa_archer2 you would use the command ssh -i keys/id_rsa_archer2 username@login.archer2.ac.uk to log in. Tip When you first log into ARCHER2, you will be prompted to change your initial password. This is a three step process: When prompted to enter your ldap password : re-enter the password you retrieved from SAFE When prompted to enter your new password: type in a new password When prompted to re-enter the new password: re-enter the new password Your password has now been changed. Hint More information on connecting to ARCHER2 is available in the Connecting to ARCHER2 section of the User Guide.","title":"Login to ARCHER2"},{"location":"quick-start/quickstart-users/#file-systems-and-manipulating-data","text":"ARCHER2 has a number of different file systems and understanding the difference between them is crucial to being able to use the system. In particular, transferring and moving data often requires a bit of thought in advance to ensure that the data is secure and in a useful form. ARCHER2 file systems are: home file systems: backed up. Available on login and data analysis nodes. work file systems: not backed-up. Available on login, data analysis and compute nodes. All users have a directory on one of the home file systems and on one of the work file systems. The directories are located at: /home/[project ID]/[project ID]/[user ID] (this is also set as your home directory) /work/[project ID]/[project ID]/[user ID] Top tips for managing data on ARCHER2: Do not generate huge numbers of files (>1000) in a single directory. Poor performance relating to file transfer is often due to the number of files involved in the transfer - minimise the number of files that you have to transfer by using archiving tools to improve performance. Archive directories or large numbers of files before moving them between file systems (e.g. by using commands like tar or zip ). When using tar or rsync between file systems mounted on ARCHER2 avoid the use of compression options as these can slow performance (time saved by transferring smaller compressed files is usually less than the overhead added by having to compress files on the fly). Think about automating the merging and transfer of multiple files output by software on ARCHER2 to other resources. The Data Management Guide linked below provides examples of how to automatically verify the integrity of an archive. Hint Information on the file systems and best practice in managing you data is available in the Data management and transfer section of the User and Best Practice Guide.","title":"File systems and manipulating data"},{"location":"quick-start/quickstart-users/#accessing-software","text":"Software on ARCHER2 is principally accessed through modules . These load and unload the desired applications, compilers, tools and libraries through the module command and its subcommands. Some modules will be loaded by default on login, providing a default working environment; many more will be available for use but initially unloaded, allowing you to set up the environment to suit your needs. At any stage you can check which modules have been loaded by running module list Running the following command will display all environment modules available on ARCHER2, whether loaded or unloaded module avail The search field for this command may be narrowed by providing the first few characters of the module name being queried. For example, all available versions and variants of VASP may be found by running module avail vasp You will see that different versions are available for many modules. For example, vasp/5/5.4.4.pl2 and vasp/6/6.1.0 are two available versions of VASP on the full system. Furthermore, a default version may be specified; this is used if no version is provided by the user. Important VASP is licensed software, as are other software packages on ARCHER2. You must have a valid licence to use licensed software on ARCHER2. Often you will need to request access through the SAFE. More on this below. The module load command loads a module for use. Following the above, module load vasp/5 would load the default version of VASP 5, while module load vasp/5/5.4.4.pl2 would specifically load version 5.4.4.pl2 . A loaded module may be unloaded through the identical module remove command, e.g. module unload vasp The above unloads whichever version of VASP is currently in the environment. Rather than issuing separate unload and load commands, versions of a module may be swapped as follows: module swap vasp vasp/5/5.4.4.pl2 Other helpful commands are: module help <modulename> which provides a short description of the module module show <modulename> which displays the contents of the modulefile Points to be aware of include: Some modules will conflict with others. A simple example would be the conflict arising when trying to load a different version of an already loaded module. When a conflict occurs, the loading process will fail and an error message will be displayed. Examination of the message and the module output (via module show ) should reveal the cause of the conflict and how to resolve it. The order in which modules are loaded can matter. Consider two modules which set the same variable to a different value. The final value would be that set by the module which loaded last. If you suspect that two modules may be interfering with one another, you can examine their contents with module show . More information on modules and the software environment on ARCHER2 can be found in the Software environment section of the User and Best Practice Guide.","title":"Accessing software"},{"location":"quick-start/quickstart-users/#requesting-access-to-licensed-software","text":"Some of the software installed on ARCHER2 requires a user to have a valid licence agreed with the software owners/developers to be able to use it (for example, VASP). Although you will be able to load this software on ARCHER2, you will be barred from actually using it until your licence has been verified. You request access to licensed software through the SAFE (the web administration tool you used to apply for your account and retrieve your initial password) by being added to the appropriate Package Group . To request access to licensed software: Log in to SAFE Go to the Menu Login accounts and select the login account which requires access to the software Click New Package Group Request Select the software from the list of available packages and click Select Package Group Fill in as much information as possible about your license; at the very least provide the information requested at the top of the screen such as the licence holder's name and contact details. If you are covered by the license because the licence holder is your supervisor, for example, please state this. Click Submit Your request will then be processed by the ARCHER2 Service Desk who will confirm your license with the software owners/developers before enabling your access to the software on ARCHER2. This can take several days (depending on how quickly the software owners/developers take to respond) but you will be advised once this has been done.","title":"Requesting access to licensed software"},{"location":"quick-start/quickstart-users/#create-a-job-submission-script","text":"To run a program on the ARCHER2 compute nodes you need to write a job submission script that tells the system how many compute nodes you want to reserve and for how long. You also need to use the srun command to launch your parallel executable. Hint For a more details on the Slurm scheduler on ARCHER2 and writing job submission scripts see the Running jobs on ARCHER2 section of the User and Best Practice Guide. Important Parallel jobs on ARCHER2 should be run from the work file systems as the home file systems are not available on the compute nodes - you will see a chdir or file not found error if you try to access data on the home file system within a parallel job running on the compute nodes. Create a job submission script called submit.slurm in your space on the work file systems using your favourite text editor. For example, using vim : auser@ln01:~> cd /work/t01/t01/auser auser@ln01:/work/t01/t01/auser> vim submit.slurm Tip You will need to use your project code and username to get to the correct directory. i.e. replace the t01 above with your project code and replace the username auser with your ARCHER2 username. Paste the following text into your job submission script, replacing ENTER_YOUR_BUDGET_CODE_HERE with your budget code e.g. e99-ham , ENTER_PARTITION_HERE with the partition you wish to run on (e.g standard ), and ENTER_QOS_HERE with the quality of service you want (e.g. standard ). Full system #!/bin/bash --login #SBATCH --job-name=test_job #SBATCH --nodes=1 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 #SBATCH --time=0:5:0 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Load the xthi module to get access to the xthi program module load xthi # srun launches the parallel program based on the SBATCH options srun --distribution=block:block --hint=nomultithread xthi","title":"Create a job submission script"},{"location":"quick-start/quickstart-users/#submit-your-job-to-the-queue","text":"You submit your job to the queues using the sbatch command: auser@ln01:/work/t01/t01/auser> sbatch submit.slurm Submitted batch job 23996 The value returned is your *Job ID*.","title":"Submit your job to the queue"},{"location":"quick-start/quickstart-users/#monitoring-your-job","text":"You use the squeue command to examine jobs in the queue. To list all the jobs you have in the queue, use: auser@ln01:/work/t01/t01/auser> squeue -u $USER squeue on its own lists all jobs in the queue from all users.","title":"Monitoring your job"},{"location":"quick-start/quickstart-users/#checking-the-output-from-the-job","text":"The job submission script above should write the output to a file called slurm-<jobID>.out (i.e. if the Job ID was 23996, the file would be slurm-23996.out ), you can check the contents of this file with the cat command. If the job was successful you should see output that looks something like: auser@ln01:/work/t01/t01/auser> cat slurm-23996.out Node 0, hostname nid001020 Node 0, rank 0, thread 0, (affinity = 0) Node 0, rank 1, thread 0, (affinity = 1) Node 0, rank 2, thread 0, (affinity = 2) Node 0, rank 3, thread 0, (affinity = 3) Node 0, rank 4, thread 0, (affinity = 4) Node 0, rank 5, thread 0, (affinity = 5) Node 0, rank 6, thread 0, (affinity = 6) Node 0, rank 7, thread 0, (affinity = 7) Node 0, rank 8, thread 0, (affinity = 8) Node 0, rank 9, thread 0, (affinity = 9) Node 0, rank 10, thread 0, (affinity = 10) Node 0, rank 11, thread 0, (affinity = 11) Node 0, rank 12, thread 0, (affinity = 12) Node 0, rank 13, thread 0, (affinity = 13) Node 0, rank 14, thread 0, (affinity = 14) Node 0, rank 15, thread 0, (affinity = 15) Node 0, rank 16, thread 0, (affinity = 16) Node 0, rank 17, thread 0, (affinity = 17) Node 0, rank 18, thread 0, (affinity = 18) Node 0, rank 19, thread 0, (affinity = 19) Node 0, rank 20, thread 0, (affinity = 20) Node 0, rank 21, thread 0, (affinity = 21) ... output trimmed ... If something has gone wrong, you will find any error messages in the file instead of the expected output.","title":"Checking the output from the job"},{"location":"quick-start/quickstart-users/#acknowledging-archer2","text":"You should use the following phrase to acknowledge ARCHER2 for all research outputs that were generated using the ARCHER2 service: This work used the ARCHER2 UK National Supercomputing Service (https://www.archer2.ac.uk). You should also tag outputs with the keyword \"ARCHER2\" whenever possible.","title":"Acknowledging ARCHER2"},{"location":"quick-start/quickstart-users/#useful-links","text":"If you plan to compile your own programs on ARCHER2, you may also want to look at Quickstart for developers . Other documentation you may find useful: ARCHER2 User and Best Practice Guide : Covers all aspects of use of the ARCHER2 service. This includes fundamentals (required by all users to use the system effectively), best practice for getting the most out of ARCHER2, and more advanced technical topics. HPE Cray Programming Environment User Guide","title":"Useful Links"},{"location":"research-software/","text":"Research Software Information on each of the centrally-installed Research Software packages, versions available, how to get access, example job submission scripts, good practice for getting best performance, links to associated training and webinars, links to associated technical reports (eCSE final reports, white papers), links to instruction manuals and further information. Centrally supported packages ARCHER2 provides a number of research software packages as centrally supported packages . Many of these packages are free to use, but others require a license (which you, or your research group, need to supply). The centrally supported package will usually be the current stable release, to include major releases and significant updates. We will usually not maintain older versions and versions no longer supported by the developers of the package. The following sections provide details on access to each of the centrally supported packages: CASTEP Chemshell Code_Saturne CP2K ELK FHI-aims FEniCS GROMACS LAMMPS MITgcm Met Office Unified Model NAMD Nektar++ NEMO NWChem ONETEP OpenFOAM Quantum Espresso VASP Not on the list? If the code you are interested in is not in the above list, we may still be able to help you install your own version, either individually, or as a project. Please contact the Service Desk .","title":"Overview"},{"location":"research-software/#research-software","text":"Information on each of the centrally-installed Research Software packages, versions available, how to get access, example job submission scripts, good practice for getting best performance, links to associated training and webinars, links to associated technical reports (eCSE final reports, white papers), links to instruction manuals and further information.","title":"Research Software"},{"location":"research-software/#centrally-supported-packages","text":"ARCHER2 provides a number of research software packages as centrally supported packages . Many of these packages are free to use, but others require a license (which you, or your research group, need to supply). The centrally supported package will usually be the current stable release, to include major releases and significant updates. We will usually not maintain older versions and versions no longer supported by the developers of the package. The following sections provide details on access to each of the centrally supported packages: CASTEP Chemshell Code_Saturne CP2K ELK FHI-aims FEniCS GROMACS LAMMPS MITgcm Met Office Unified Model NAMD Nektar++ NEMO NWChem ONETEP OpenFOAM Quantum Espresso VASP","title":"Centrally supported packages"},{"location":"research-software/#not-on-the-list","text":"If the code you are interested in is not in the above list, we may still be able to help you install your own version, either individually, or as a project. Please contact the Service Desk .","title":"Not on the list?"},{"location":"research-software/castep/castep/","text":"CASTEP CASTEP is a leading code for calculating the properties of materials from first principles. Using density functional theory, it can simulate a wide range of properties of materials proprieties including energetics, structure at the atomic level, vibrational properties, electronic response properties etc. In particular it has a wide range of spectroscopic features that link directly to experiment, such as infra-red and Raman spectroscopies, NMR, and core level spectra. Useful Links CASTEP User Guides CASTEP Tutorials CASTEP Licensing Using CASTEP on ARCHER2 CASTEP is only available to users who have a valid CASTEP licence. If you have a CASTEP licence and wish to have access to CASTEP on ARCHER2, please make a request via the SAFE, see: How to request access to package groups Please have your license details to hand. Note on using Relativistic J-dependent pseudopotentials These pseudopotentials cannot be generated on the fly by CASTEP and so are available in the following directory on ARCHER2: Full system /work/y07/shared/apps/core/castep/pseudopotentials Running parallel CASTEP jobs The following script will run a CASTEP job using 2 nodes (256 cores). it assumes that the input files have the file stem text_calc . Full system #!/bin/bash # Request 2 nodes with 128 MPI tasks per node for 20 minutes #SBATCH --job-name=CASTEP #SBATCH --nodes=2 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 #SBATCH --time=00:20:00 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Load the CASTEP module, avoid any unintentional OpenMP threading by # setting OMP_NUM_THREADS, and launch the code. module load castep export OMP_NUM_THREADS=1 srun --distribution=block:block --hint=nomultithread castep.mpi test_calc Using serial CASTEP tools Full system Serial CASTEP tools are available in the standard CASTEP module. Compiling CASTEP The latest instructions for building CASTEP on ARCHER2 may be found in the GitHub repository of build instructions: Build instructions for CASTEP on GitHub","title":"CASTEP"},{"location":"research-software/castep/castep/#castep","text":"CASTEP is a leading code for calculating the properties of materials from first principles. Using density functional theory, it can simulate a wide range of properties of materials proprieties including energetics, structure at the atomic level, vibrational properties, electronic response properties etc. In particular it has a wide range of spectroscopic features that link directly to experiment, such as infra-red and Raman spectroscopies, NMR, and core level spectra.","title":"CASTEP"},{"location":"research-software/castep/castep/#useful-links","text":"CASTEP User Guides CASTEP Tutorials CASTEP Licensing","title":"Useful Links"},{"location":"research-software/castep/castep/#using-castep-on-archer2","text":"CASTEP is only available to users who have a valid CASTEP licence. If you have a CASTEP licence and wish to have access to CASTEP on ARCHER2, please make a request via the SAFE, see: How to request access to package groups Please have your license details to hand.","title":"Using CASTEP on ARCHER2"},{"location":"research-software/castep/castep/#note-on-using-relativistic-j-dependent-pseudopotentials","text":"These pseudopotentials cannot be generated on the fly by CASTEP and so are available in the following directory on ARCHER2: Full system /work/y07/shared/apps/core/castep/pseudopotentials","title":"Note on using Relativistic J-dependent pseudopotentials"},{"location":"research-software/castep/castep/#running-parallel-castep-jobs","text":"The following script will run a CASTEP job using 2 nodes (256 cores). it assumes that the input files have the file stem text_calc . Full system #!/bin/bash # Request 2 nodes with 128 MPI tasks per node for 20 minutes #SBATCH --job-name=CASTEP #SBATCH --nodes=2 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 #SBATCH --time=00:20:00 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Load the CASTEP module, avoid any unintentional OpenMP threading by # setting OMP_NUM_THREADS, and launch the code. module load castep export OMP_NUM_THREADS=1 srun --distribution=block:block --hint=nomultithread castep.mpi test_calc","title":"Running parallel CASTEP jobs"},{"location":"research-software/castep/castep/#using-serial-castep-tools","text":"Full system Serial CASTEP tools are available in the standard CASTEP module.","title":"Using serial CASTEP tools"},{"location":"research-software/castep/castep/#compiling-castep","text":"The latest instructions for building CASTEP on ARCHER2 may be found in the GitHub repository of build instructions: Build instructions for CASTEP on GitHub","title":"Compiling CASTEP"},{"location":"research-software/chemshell/chemshell/","text":"ChemShell ChemShell is a script-based chemistry code focusing on hybrid QM/MM calculations with support for standard quantum chemical or force field calculations. There are two versions: an older Tcl-based version Tcl-ChemShell and a more recent python-based version Py-ChemShell. The advice from https://www.chemshell.org/licence on the difference is: We regard Py-ChemShell 19.0 as suitable for production calculations on materials systems, although you will find its feature set is more limited than Tcl-ChemShell. We do not currently recommend Py-ChemShell for calculations on biological systems, as automated import of biomolecular force fields is scheduled for a future release. Useful Links ChemShell home page https://www.chemshell.org ChemShell documentation https://www.chemshell.org/documentation ChemShell forums https://www.chemshell.org/forum Using ChemShell on ARCHER2 Warning The python-based version of ChemShell is not yet available on ARCHER2 The python-based version of ChemShell is open-source and is freely available to all users on ARCHER2. The older version of Tcl-based ChemShell requires a license. Users with a valid license should request access via the ARCHER2 SAFE. Running parallel ChemShell jobs The following script will run a pure MPI Tcl-based ChemShell job using 8 nodes (128x8 cores). Full system We are working with the ChemShell developers to make ChemShell available on the full system as soon as possible.","title":"Chemshell"},{"location":"research-software/chemshell/chemshell/#chemshell","text":"ChemShell is a script-based chemistry code focusing on hybrid QM/MM calculations with support for standard quantum chemical or force field calculations. There are two versions: an older Tcl-based version Tcl-ChemShell and a more recent python-based version Py-ChemShell. The advice from https://www.chemshell.org/licence on the difference is: We regard Py-ChemShell 19.0 as suitable for production calculations on materials systems, although you will find its feature set is more limited than Tcl-ChemShell. We do not currently recommend Py-ChemShell for calculations on biological systems, as automated import of biomolecular force fields is scheduled for a future release.","title":"ChemShell"},{"location":"research-software/chemshell/chemshell/#useful-links","text":"ChemShell home page https://www.chemshell.org ChemShell documentation https://www.chemshell.org/documentation ChemShell forums https://www.chemshell.org/forum","title":"Useful Links"},{"location":"research-software/chemshell/chemshell/#using-chemshell-on-archer2","text":"Warning The python-based version of ChemShell is not yet available on ARCHER2 The python-based version of ChemShell is open-source and is freely available to all users on ARCHER2. The older version of Tcl-based ChemShell requires a license. Users with a valid license should request access via the ARCHER2 SAFE.","title":"Using ChemShell on ARCHER2"},{"location":"research-software/chemshell/chemshell/#running-parallel-chemshell-jobs","text":"The following script will run a pure MPI Tcl-based ChemShell job using 8 nodes (128x8 cores). Full system We are working with the ChemShell developers to make ChemShell available on the full system as soon as possible.","title":"Running parallel ChemShell jobs"},{"location":"research-software/code-saturne/code-saturne/","text":"Code_Saturne Code_Saturne solves the Navier-Stokes equations for 2D, 2D-axisymmetric and 3D flows, steady or unsteady, laminar or turbulent, incompressible or weakly dilatable, isothermal or not, with scalar transport if required. Several turbulence models are available, from Reynolds-averaged models to large-eddy simulation (LES) models. In addition, a number of specific physical models are also available as \"modules\": gas, coal and heavy-fuel oil combustion, semi-transparent radiative transfer, particle-tracking with Lagrangian modeling, Joule effect, electrics arcs, weakly compressible flows, atmospheric flows, rotor/stator interaction for hydraulic machines. Useful Links Code_Saturne home page Code_Saturne user guides Code_Saturne users' forum Using Code_Saturne on ARCHER2 Code_Saturne is released under the GNU General Public Licence v2 and so is freely available to all users on ARCHER2. You can load the default GCC build of Code_Saturne for use by running the following command: module load code_saturne This will load the default code_saturne/7.0.1-gcc11 module. A build using the CCE compilers, code_saturne/7.0.1-cce12 , has also been made optionally available to users on the full ARCHER2 system as testing indicates that this may provide improved performance over the GCC build. Running parallel Code_Saturne jobs After setting up a case it should be initialized by running the following command from the case directory, where setup.xml is the input file: code_saturne run --initialize --param setup.xml This will create a directory named for the current date and time (e.g. 20201019-1636) inside the RESU directory. Inside the new directory will be a script named run_solver . You may alter this to resemble the script below, or you may wish to simply create a new one with the contents shown. If you wish to alter the existing run_solver script you will need to add all the #SBATCH options shown to set the job name, size and so on. You should also add the two module commands, and srun --distribution=block:block --hint=nomultithread as well as the --mpi option to the line executing ./cs_solver to ensure parallel execution on the compute nodes. The export LD_LIBRARY_PATH=... and cd commands are redundant and may be retained or removed. This script will run an MPI-only Code_Saturne job using the default GCC build and UCX over 4 nodes (128 x 4 = 512 cores) for a maximum of 20 minutes. Full system #!/bin/bash #SBATCH --export=none #SBATCH --job-name=CSExample #SBATCH --time=0:20:0 #SBATCH --nodes=4 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Load the GCC build of Code_Saturne 7.0.1 module load cpe/21.09 module load PrgEnv-gnu module load code_saturne # Switch to mpich-ucx implementation (see info note below) module swap craype-network-ofi craype-network-ucx module swap cray-mpich cray-mpich-ucx # Prevent threading. export OMP_NUM_THREADS=1 # Run solver. srun --distribution=block:block --hint=nomultithread ./cs_solver --mpi $@ The script can then be submitted to the batch system with sbatch . Info There is a known issue with the default MPI collectives which is causing performance issues on Code_Saturne. The suggested workaround is to switch to the mpich-ucx implementation. For this to link correctly on the full system, the extra cpe/21.09 and PrgEnv-gnu modules also have to be explicitly loaded. Compiling Code_Saturne The latest instructions for building Code_Saturne on ARCHER2 may be found in the GitHub repository of build instructions: Build instructions for Code_Saturne on GitHub","title":"Code_Saturne"},{"location":"research-software/code-saturne/code-saturne/#code_saturne","text":"Code_Saturne solves the Navier-Stokes equations for 2D, 2D-axisymmetric and 3D flows, steady or unsteady, laminar or turbulent, incompressible or weakly dilatable, isothermal or not, with scalar transport if required. Several turbulence models are available, from Reynolds-averaged models to large-eddy simulation (LES) models. In addition, a number of specific physical models are also available as \"modules\": gas, coal and heavy-fuel oil combustion, semi-transparent radiative transfer, particle-tracking with Lagrangian modeling, Joule effect, electrics arcs, weakly compressible flows, atmospheric flows, rotor/stator interaction for hydraulic machines.","title":"Code_Saturne"},{"location":"research-software/code-saturne/code-saturne/#useful-links","text":"Code_Saturne home page Code_Saturne user guides Code_Saturne users' forum","title":"Useful Links"},{"location":"research-software/code-saturne/code-saturne/#using-code_saturne-on-archer2","text":"Code_Saturne is released under the GNU General Public Licence v2 and so is freely available to all users on ARCHER2. You can load the default GCC build of Code_Saturne for use by running the following command: module load code_saturne This will load the default code_saturne/7.0.1-gcc11 module. A build using the CCE compilers, code_saturne/7.0.1-cce12 , has also been made optionally available to users on the full ARCHER2 system as testing indicates that this may provide improved performance over the GCC build.","title":"Using Code_Saturne on ARCHER2"},{"location":"research-software/code-saturne/code-saturne/#running-parallel-code_saturne-jobs","text":"After setting up a case it should be initialized by running the following command from the case directory, where setup.xml is the input file: code_saturne run --initialize --param setup.xml This will create a directory named for the current date and time (e.g. 20201019-1636) inside the RESU directory. Inside the new directory will be a script named run_solver . You may alter this to resemble the script below, or you may wish to simply create a new one with the contents shown. If you wish to alter the existing run_solver script you will need to add all the #SBATCH options shown to set the job name, size and so on. You should also add the two module commands, and srun --distribution=block:block --hint=nomultithread as well as the --mpi option to the line executing ./cs_solver to ensure parallel execution on the compute nodes. The export LD_LIBRARY_PATH=... and cd commands are redundant and may be retained or removed. This script will run an MPI-only Code_Saturne job using the default GCC build and UCX over 4 nodes (128 x 4 = 512 cores) for a maximum of 20 minutes. Full system #!/bin/bash #SBATCH --export=none #SBATCH --job-name=CSExample #SBATCH --time=0:20:0 #SBATCH --nodes=4 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Load the GCC build of Code_Saturne 7.0.1 module load cpe/21.09 module load PrgEnv-gnu module load code_saturne # Switch to mpich-ucx implementation (see info note below) module swap craype-network-ofi craype-network-ucx module swap cray-mpich cray-mpich-ucx # Prevent threading. export OMP_NUM_THREADS=1 # Run solver. srun --distribution=block:block --hint=nomultithread ./cs_solver --mpi $@ The script can then be submitted to the batch system with sbatch . Info There is a known issue with the default MPI collectives which is causing performance issues on Code_Saturne. The suggested workaround is to switch to the mpich-ucx implementation. For this to link correctly on the full system, the extra cpe/21.09 and PrgEnv-gnu modules also have to be explicitly loaded.","title":"Running parallel Code_Saturne jobs"},{"location":"research-software/code-saturne/code-saturne/#compiling-code_saturne","text":"The latest instructions for building Code_Saturne on ARCHER2 may be found in the GitHub repository of build instructions: Build instructions for Code_Saturne on GitHub","title":"Compiling Code_Saturne"},{"location":"research-software/cp2k/cp2k/","text":"CP2K CP2K is a quantum chemistry and solid state physics software package that can perform atomistic simulations of solid state, liquid, molecular, periodic, material, crystal, and biological systems. CP2K provides a general framework for different modelling methods such as DFT using the mixed Gaussian and plane waves approaches GPW and GAPW. Supported theory levels include DFTB, LDA, GGA, MP2, RPA, semi-empirical methods (AM1, PM3, PM6, RM1, MNDO), and classical force fields (AMBER, CHARMM). CP2K can do simulations of molecular dynamics, metadynamics, Monte Carlo, Ehrenfest dynamics, vibrational analysis, core level spectroscopy, energy minimisation, and transition state optimisation using NEB or dimer method. Useful links CP2K Reference Manual CP2K HOWTOs CP2K FAQs Using CP2K on ARCHER2 CP2K is available through the cp2k module. MPI only cp2k.popt and MPI/OpenMP Hybrid cp2k.psmp binaries are available. For ARCHER2, CP2K has been compiled with the following optional features: FFTW for fast Fourier transforms, libint to enable methods including Hartree-Fock exchange, libxsmm for efficient small matrix multiplications, libxc to provide a wider choice of exchange-correlation functionals, ELPA for improved performance of matrix diagonalisation, PLUMED to allow enhanced sampling methods, and SIRIUS for plane wave computations. See CP2K compile instructions for a full list of optional features. If there is an optional feature not available, and which you would like, please contact the Service Desk . Experts may also wish to compile their own versions of the code (see below for instructions). Running parallel CP2K jobs MPI only jobs To run CP2K using MPI only, load the cp2k module and use the cp2k.popt executable. For example, the following script will run a CP2K job using 4 nodes (128x4 cores): Full system #!/bin/bash # Request 4 nodes using 128 cores per node for 128 MPI tasks per node. #SBATCH --job-name=CP2K_test #SBATCH --nodes=4 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 #SBATCH --time=00:20:00 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Load the relevent CP2K module module load cp2k export OMP_NUM_THREADS=1 srun --hint=nomultithread --distribution=block:block cp2k.popt -i MYINPUT.inp MPI/OpenMP hybrid jobs To run CP2K using MPI and OpenMP, load the cp2k module and use the cp2k.psmp executable. Full system #!/bin/bash # Request 4 nodes with 16 MPI tasks per node each using 8 threads; # note this means 128 MPI tasks in total. # Remember to replace [budget code] below with your account code, # e.g. '--account=t01'. #SBATCH --job-name=CP2K_test #SBATCH --nodes=4 #SBATCH --tasks-per-node=16 #SBATCH --cpus-per-task=8 #SBATCH --time=00:20:00 #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Load the relevant CP2K module module load cp2k # Ensure OMP_NUM_THREADS is consistent with cpus-per-task above export OMP_NUM_THREADS=8 export OMP_PLACES=cores srun --hint=nomultithread --distribution=block:block cp2k.psmp -i MYINPUT.inp Compiling CP2K The latest instructions for building CP2K on ARCHER2 may be found in the GitHub repository of build instructions: Build instructions for CP2K on GitHub","title":"CP2K"},{"location":"research-software/cp2k/cp2k/#cp2k","text":"CP2K is a quantum chemistry and solid state physics software package that can perform atomistic simulations of solid state, liquid, molecular, periodic, material, crystal, and biological systems. CP2K provides a general framework for different modelling methods such as DFT using the mixed Gaussian and plane waves approaches GPW and GAPW. Supported theory levels include DFTB, LDA, GGA, MP2, RPA, semi-empirical methods (AM1, PM3, PM6, RM1, MNDO), and classical force fields (AMBER, CHARMM). CP2K can do simulations of molecular dynamics, metadynamics, Monte Carlo, Ehrenfest dynamics, vibrational analysis, core level spectroscopy, energy minimisation, and transition state optimisation using NEB or dimer method.","title":"CP2K"},{"location":"research-software/cp2k/cp2k/#useful-links","text":"CP2K Reference Manual CP2K HOWTOs CP2K FAQs","title":"Useful links"},{"location":"research-software/cp2k/cp2k/#using-cp2k-on-archer2","text":"CP2K is available through the cp2k module. MPI only cp2k.popt and MPI/OpenMP Hybrid cp2k.psmp binaries are available. For ARCHER2, CP2K has been compiled with the following optional features: FFTW for fast Fourier transforms, libint to enable methods including Hartree-Fock exchange, libxsmm for efficient small matrix multiplications, libxc to provide a wider choice of exchange-correlation functionals, ELPA for improved performance of matrix diagonalisation, PLUMED to allow enhanced sampling methods, and SIRIUS for plane wave computations. See CP2K compile instructions for a full list of optional features. If there is an optional feature not available, and which you would like, please contact the Service Desk . Experts may also wish to compile their own versions of the code (see below for instructions).","title":"Using CP2K on ARCHER2"},{"location":"research-software/cp2k/cp2k/#running-parallel-cp2k-jobs","text":"","title":"Running parallel CP2K jobs"},{"location":"research-software/cp2k/cp2k/#mpi-only-jobs","text":"To run CP2K using MPI only, load the cp2k module and use the cp2k.popt executable. For example, the following script will run a CP2K job using 4 nodes (128x4 cores): Full system #!/bin/bash # Request 4 nodes using 128 cores per node for 128 MPI tasks per node. #SBATCH --job-name=CP2K_test #SBATCH --nodes=4 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 #SBATCH --time=00:20:00 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Load the relevent CP2K module module load cp2k export OMP_NUM_THREADS=1 srun --hint=nomultithread --distribution=block:block cp2k.popt -i MYINPUT.inp","title":"MPI only jobs"},{"location":"research-software/cp2k/cp2k/#mpiopenmp-hybrid-jobs","text":"To run CP2K using MPI and OpenMP, load the cp2k module and use the cp2k.psmp executable. Full system #!/bin/bash # Request 4 nodes with 16 MPI tasks per node each using 8 threads; # note this means 128 MPI tasks in total. # Remember to replace [budget code] below with your account code, # e.g. '--account=t01'. #SBATCH --job-name=CP2K_test #SBATCH --nodes=4 #SBATCH --tasks-per-node=16 #SBATCH --cpus-per-task=8 #SBATCH --time=00:20:00 #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Load the relevant CP2K module module load cp2k # Ensure OMP_NUM_THREADS is consistent with cpus-per-task above export OMP_NUM_THREADS=8 export OMP_PLACES=cores srun --hint=nomultithread --distribution=block:block cp2k.psmp -i MYINPUT.inp","title":"MPI/OpenMP hybrid jobs"},{"location":"research-software/cp2k/cp2k/#compiling-cp2k","text":"The latest instructions for building CP2K on ARCHER2 may be found in the GitHub repository of build instructions: Build instructions for CP2K on GitHub","title":"Compiling CP2K"},{"location":"research-software/elk/elk/","text":"ELK ELK is an all-electron full-potential linearised augmented-plane wave (FP-LAPW) code with many advanced features. It was written originally at Karl-Franzens-Universitt Graz as a milestone of the EXCITING EU Research and Training Network. Useful Links ELK home page http://elk.sourceforge.net ELK documentation http://elk.sourceforge.net/#documentation Using ELK on ARCHER2 ELK is freely available to all users on ARCHER2. Running parallel ELK jobs Example MPI ELK job The following script will run an ELK job on 4 nodes (512 cores). Full system #!/bin/bash # Request 512 MPI tasks (4 nodes at 128 tasks per node) with a #SBATCH --job-name=elk_job #SBATCH --nodes=4 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 #SBATCH --time=00:20:00 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Load the relevant elk module module load elk srun --distribution=block:block --hint=nomultithread elk Example mixed MPI/OpenMP ELK job The following script will run an ELK job on 4 nodes, using 8 OpenMP threads and 16 MPI tasks per node. Full system #!/bin/bash # Request 4 nodes (using 8 threads and 16 MPI tasks per node) with a # maximum wall clock time limit of 20 minutes. # Replace [budget code] with your account code. #SBATCH --job-name=elk_job #SBATCH --nodes=4 #SBATCH --tasks-per-node=16 #SBATCH --cpus-per-task=8 #SBATCH --time=00:20:00 #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Ensure OMP_NUM_THREADS is consistent with cpus-per-task above export OMP_NUM_THREADS=8 # Load the relevant elk module module load elk srun --distribution=block:block --hint=nomultithread elk Compiling ELK The latest instructions for building ELK on ARCHER2 may be found in the GitHub repository of build instructions: Build instructions for ELK on GitHub","title":"ELK"},{"location":"research-software/elk/elk/#elk","text":"ELK is an all-electron full-potential linearised augmented-plane wave (FP-LAPW) code with many advanced features. It was written originally at Karl-Franzens-Universitt Graz as a milestone of the EXCITING EU Research and Training Network.","title":"ELK"},{"location":"research-software/elk/elk/#useful-links","text":"ELK home page http://elk.sourceforge.net ELK documentation http://elk.sourceforge.net/#documentation","title":"Useful Links"},{"location":"research-software/elk/elk/#using-elk-on-archer2","text":"ELK is freely available to all users on ARCHER2.","title":"Using ELK on ARCHER2"},{"location":"research-software/elk/elk/#running-parallel-elk-jobs","text":"","title":"Running parallel ELK jobs"},{"location":"research-software/elk/elk/#example-mpi-elk-job","text":"The following script will run an ELK job on 4 nodes (512 cores). Full system #!/bin/bash # Request 512 MPI tasks (4 nodes at 128 tasks per node) with a #SBATCH --job-name=elk_job #SBATCH --nodes=4 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 #SBATCH --time=00:20:00 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Load the relevant elk module module load elk srun --distribution=block:block --hint=nomultithread elk","title":"Example MPI ELK job"},{"location":"research-software/elk/elk/#example-mixed-mpiopenmp-elk-job","text":"The following script will run an ELK job on 4 nodes, using 8 OpenMP threads and 16 MPI tasks per node. Full system #!/bin/bash # Request 4 nodes (using 8 threads and 16 MPI tasks per node) with a # maximum wall clock time limit of 20 minutes. # Replace [budget code] with your account code. #SBATCH --job-name=elk_job #SBATCH --nodes=4 #SBATCH --tasks-per-node=16 #SBATCH --cpus-per-task=8 #SBATCH --time=00:20:00 #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Ensure OMP_NUM_THREADS is consistent with cpus-per-task above export OMP_NUM_THREADS=8 # Load the relevant elk module module load elk srun --distribution=block:block --hint=nomultithread elk","title":"Example mixed MPI/OpenMP ELK job"},{"location":"research-software/elk/elk/#compiling-elk","text":"The latest instructions for building ELK on ARCHER2 may be found in the GitHub repository of build instructions: Build instructions for ELK on GitHub","title":"Compiling ELK"},{"location":"research-software/fenics/fenics/","text":"FEniCS FEniCS is an open-source (LGPLv3) computing platform for solving partial differential equations (PDEs). FEniCS enables users to translate scientific models into efficient finite element code. With the high-level Python and C++ interfaces to FEniCS, it is easy to get started, but FEniCS also offers powerful capabilities for more experienced programmers. Useful Links FEniCS home page https://fenicsproject.org FEniCS documentation https://fenicsproject.org/documentation/ Using FEniCS on ARCHER2 Warning FEniCS is not yet available on ARCHER2","title":"FEniCS"},{"location":"research-software/fenics/fenics/#fenics","text":"FEniCS is an open-source (LGPLv3) computing platform for solving partial differential equations (PDEs). FEniCS enables users to translate scientific models into efficient finite element code. With the high-level Python and C++ interfaces to FEniCS, it is easy to get started, but FEniCS also offers powerful capabilities for more experienced programmers.","title":"FEniCS"},{"location":"research-software/fenics/fenics/#useful-links","text":"FEniCS home page https://fenicsproject.org FEniCS documentation https://fenicsproject.org/documentation/","title":"Useful Links"},{"location":"research-software/fenics/fenics/#using-fenics-on-archer2","text":"Warning FEniCS is not yet available on ARCHER2","title":"Using FEniCS on ARCHER2"},{"location":"research-software/fhi-aims/fhi-aims/","text":"FHI-aims FHI-aims is an all-electron electronic structure code based on numeric atom-centered orbitals. It enables first-principles simulations with very high numerical accuracy for production calculations, with excellent scalability up to very large system sizes (thousands of atoms) and up to very large, massively parallel supercomputers (ten thousand CPU cores). Useful Links FHI-aims website FHI-aims Tutorials Using FHI-aims on ARCHER2 FHI-aims is only available to users who have a valid FHI-aims licence. If you have a FHI-aims licence and wish to have access to FHI-aims on ARCHER2, please make a request via the SAFE, see: How to request access to package groups Please have your license details to hand. Running parallel FHI-aims jobs The following script will run a FHI-aims job using 8 nodes (1024 cores). The script assumes that the input have the default names control.in and geometry.in . Full system #!/bin/bash # Request 2 nodes with 128 MPI tasks per node for 20 minutes #SBATCH --job-name=FHI-aims #SBATCH --nodes=8 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 #SBATCH --time=00:20:00 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Load the FHI-aims module, avoid any unintentional OpenMP threading by # setting OMP_NUM_THREADS, and launch the code. module load fhiaims export OMP_NUM_THREADS=1 srun --distribution=block:block --hint=nomultithread aims.mpi.x Compiling FHI-aims The latest instructions for building FHI-aims on ARCHER2 may be found in the GitHub repository of build instructions: Build instructions for FHI-aims on GitHub","title":"FHI-aims"},{"location":"research-software/fhi-aims/fhi-aims/#fhi-aims","text":"FHI-aims is an all-electron electronic structure code based on numeric atom-centered orbitals. It enables first-principles simulations with very high numerical accuracy for production calculations, with excellent scalability up to very large system sizes (thousands of atoms) and up to very large, massively parallel supercomputers (ten thousand CPU cores).","title":"FHI-aims"},{"location":"research-software/fhi-aims/fhi-aims/#useful-links","text":"FHI-aims website FHI-aims Tutorials","title":"Useful Links"},{"location":"research-software/fhi-aims/fhi-aims/#using-fhi-aims-on-archer2","text":"FHI-aims is only available to users who have a valid FHI-aims licence. If you have a FHI-aims licence and wish to have access to FHI-aims on ARCHER2, please make a request via the SAFE, see: How to request access to package groups Please have your license details to hand.","title":"Using FHI-aims on ARCHER2"},{"location":"research-software/fhi-aims/fhi-aims/#running-parallel-fhi-aims-jobs","text":"The following script will run a FHI-aims job using 8 nodes (1024 cores). The script assumes that the input have the default names control.in and geometry.in . Full system #!/bin/bash # Request 2 nodes with 128 MPI tasks per node for 20 minutes #SBATCH --job-name=FHI-aims #SBATCH --nodes=8 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 #SBATCH --time=00:20:00 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Load the FHI-aims module, avoid any unintentional OpenMP threading by # setting OMP_NUM_THREADS, and launch the code. module load fhiaims export OMP_NUM_THREADS=1 srun --distribution=block:block --hint=nomultithread aims.mpi.x","title":"Running parallel FHI-aims jobs"},{"location":"research-software/fhi-aims/fhi-aims/#compiling-fhi-aims","text":"The latest instructions for building FHI-aims on ARCHER2 may be found in the GitHub repository of build instructions: Build instructions for FHI-aims on GitHub","title":"Compiling FHI-aims"},{"location":"research-software/gromacs/gromacs/","text":"GROMACS GROMACS is a versatile package to perform molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. It is primarily designed for biochemical molecules like proteins, lipids and nucleic acids that have a lot of complicated bonded interactions, but since GROMACS is extremely fast at calculating the nonbonded interactions (that usually dominate simulations) many groups are also using it for research on non-biological systems, e.g. polymers. Useful Links GROMACS User Guides GROMACS Tutorials Using GROMACS on ARCHER2 GROMACS is Open Source software and is freely available to all users. Three versions are available: Parallel MPI/OpenMP, single precision: gmx_mpi Parallel MPI/OpenMP, double precision: gmx_mpi_d Serial, single precision: gmx Running parallel GROMACS jobs Running MPI only jobs The following script will run a GROMACS MD job using 4 nodes (128x4 cores) with pure MPI. Full system #!/bin/bash #SBATCH --job-name=mdrun_test #SBATCH --nodes=4 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 #SBATCH --time=00:20:00 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Setup the environment module load gromacs export OMP_NUM_THREADS=1 srun --distribution=block:block --hint=nomultithread gmx_mpi mdrun -s test_calc.tpr Running hybrid MPI/OpenMP jobs The following script will run a GROMACS MD job using 4 nodes (128x4 cores) with 6 MPI processes per node (24 MPI processes in total) and 6 OpenMP threads per MPI process. Full system #!/bin/bash #SBATCH --job-name=mdrun_test #SBATCH --nodes=4 #SBATCH --tasks-per-node=16 #SBATCH --cpus-per-task=8 #SBATCH --time=00:20:00 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Setup the environment module load gromacs export OMP_NUM_THREADS=8 srun --distribution=block:block --hint=nomultithread gmx_mpi mdrun -s test_calc.tpr Compiling Gromacs The latest instructions for building GROMACS on ARCHER2 may be found in the GitHub repository of build instructions: Build instructions for GROMACS on GitHub","title":"GROMACS"},{"location":"research-software/gromacs/gromacs/#gromacs","text":"GROMACS is a versatile package to perform molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. It is primarily designed for biochemical molecules like proteins, lipids and nucleic acids that have a lot of complicated bonded interactions, but since GROMACS is extremely fast at calculating the nonbonded interactions (that usually dominate simulations) many groups are also using it for research on non-biological systems, e.g. polymers.","title":"GROMACS"},{"location":"research-software/gromacs/gromacs/#useful-links","text":"GROMACS User Guides GROMACS Tutorials","title":"Useful Links"},{"location":"research-software/gromacs/gromacs/#using-gromacs-on-archer2","text":"GROMACS is Open Source software and is freely available to all users. Three versions are available: Parallel MPI/OpenMP, single precision: gmx_mpi Parallel MPI/OpenMP, double precision: gmx_mpi_d Serial, single precision: gmx","title":"Using GROMACS on ARCHER2"},{"location":"research-software/gromacs/gromacs/#running-parallel-gromacs-jobs","text":"","title":"Running parallel GROMACS jobs"},{"location":"research-software/gromacs/gromacs/#running-mpi-only-jobs","text":"The following script will run a GROMACS MD job using 4 nodes (128x4 cores) with pure MPI. Full system #!/bin/bash #SBATCH --job-name=mdrun_test #SBATCH --nodes=4 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 #SBATCH --time=00:20:00 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Setup the environment module load gromacs export OMP_NUM_THREADS=1 srun --distribution=block:block --hint=nomultithread gmx_mpi mdrun -s test_calc.tpr","title":"Running MPI only jobs"},{"location":"research-software/gromacs/gromacs/#running-hybrid-mpiopenmp-jobs","text":"The following script will run a GROMACS MD job using 4 nodes (128x4 cores) with 6 MPI processes per node (24 MPI processes in total) and 6 OpenMP threads per MPI process. Full system #!/bin/bash #SBATCH --job-name=mdrun_test #SBATCH --nodes=4 #SBATCH --tasks-per-node=16 #SBATCH --cpus-per-task=8 #SBATCH --time=00:20:00 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Setup the environment module load gromacs export OMP_NUM_THREADS=8 srun --distribution=block:block --hint=nomultithread gmx_mpi mdrun -s test_calc.tpr","title":"Running hybrid MPI/OpenMP jobs"},{"location":"research-software/gromacs/gromacs/#compiling-gromacs","text":"The latest instructions for building GROMACS on ARCHER2 may be found in the GitHub repository of build instructions: Build instructions for GROMACS on GitHub","title":"Compiling Gromacs"},{"location":"research-software/lammps/lammps/","text":"LAMMPS LAMMPS (Large-scale Atomic/Molecular Massively Parallel Simulator) is a classical molecular dynamics code. LAMMPS has potentials for solid-state materials (metals, semiconductors) and soft matter (biomolecules, polymers), and coarse-grained or mesoscopic systems. It can be used to model atoms or, more generically, as a parallel particle simulator at the atomic, mesoscopic, or continuum scale. Useful Links LAMMPS Documentation LAMMPS Mailing list details Using LAMMPS on ARCHER2 LAMMPS is freely available to all ARCHER2 users. The centrally installed version of LAMMPS is compiled with all the standard packages included: ASPHERE , BODY , CLASS2 , COLLOID , COMPRESS , CORESHELL , DIPOLE , GRANULAR , KSPACE , MANYBODY , MC , MISC , MOLECULE , OPT , PERI , QEQ , REPLICA , RIGID , SHOCK , SNAP , SRD . We do not install any USER packages. If you are interested in a USER package, we would encourage you to try to compile your own version and we can help out if necessary (see below). Running parallel LAMMPS jobs LAMMPS can exploit multiple nodes on ARCHER2 and will generally be run in exclusive mode using more than one node. For example, the following script will run a LAMMPS MD job using 4 nodes (128x4 cores) with MPI only. Full system #!/bin/bash #SBATCH --job-name=lammps_test #SBATCH --nodes=4 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 #SBATCH --time=00:20:00 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard module load lammps srun --distribution=block:block --hint=nomultithread lmp -i in.test -l out.test Compiling LAMMPS The large range of optional packages available for LAMMPS, and opportunity for extensibility, may mean that it is convenient for users to compile their own copy. In practice, LAMMPS is relatively easy to compile, so we encourage users to have a go. Compilation instructions for LAMMPS on ARCHER2 can be found on GitHub: Build instructions for LAMMPS on GitHub","title":"LAMMPS"},{"location":"research-software/lammps/lammps/#lammps","text":"LAMMPS (Large-scale Atomic/Molecular Massively Parallel Simulator) is a classical molecular dynamics code. LAMMPS has potentials for solid-state materials (metals, semiconductors) and soft matter (biomolecules, polymers), and coarse-grained or mesoscopic systems. It can be used to model atoms or, more generically, as a parallel particle simulator at the atomic, mesoscopic, or continuum scale.","title":"LAMMPS"},{"location":"research-software/lammps/lammps/#useful-links","text":"LAMMPS Documentation LAMMPS Mailing list details","title":"Useful Links"},{"location":"research-software/lammps/lammps/#using-lammps-on-archer2","text":"LAMMPS is freely available to all ARCHER2 users. The centrally installed version of LAMMPS is compiled with all the standard packages included: ASPHERE , BODY , CLASS2 , COLLOID , COMPRESS , CORESHELL , DIPOLE , GRANULAR , KSPACE , MANYBODY , MC , MISC , MOLECULE , OPT , PERI , QEQ , REPLICA , RIGID , SHOCK , SNAP , SRD . We do not install any USER packages. If you are interested in a USER package, we would encourage you to try to compile your own version and we can help out if necessary (see below).","title":"Using LAMMPS on ARCHER2"},{"location":"research-software/lammps/lammps/#running-parallel-lammps-jobs","text":"LAMMPS can exploit multiple nodes on ARCHER2 and will generally be run in exclusive mode using more than one node. For example, the following script will run a LAMMPS MD job using 4 nodes (128x4 cores) with MPI only. Full system #!/bin/bash #SBATCH --job-name=lammps_test #SBATCH --nodes=4 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 #SBATCH --time=00:20:00 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard module load lammps srun --distribution=block:block --hint=nomultithread lmp -i in.test -l out.test","title":"Running parallel LAMMPS jobs"},{"location":"research-software/lammps/lammps/#compiling-lammps","text":"The large range of optional packages available for LAMMPS, and opportunity for extensibility, may mean that it is convenient for users to compile their own copy. In practice, LAMMPS is relatively easy to compile, so we encourage users to have a go. Compilation instructions for LAMMPS on ARCHER2 can be found on GitHub: Build instructions for LAMMPS on GitHub","title":"Compiling LAMMPS"},{"location":"research-software/mitgcm/mitgcm/","text":"MITgcm The Massachusetts Institute of Technology General Circulation Model (MITgcm) is a numerical model designed for study of the atmosphere, ocean, and climate. MITgcm's flexible non-hydrostatic formulation enables it to simulate fluid phenomena over a wide range of scales; its adjoint capabilities enable it to be applied to sensitivity questions and to parameter and state estimation problems. By employing fluid equation isomorphisms, a single dynamical kernel can be used to simulate flow of both the atmosphere and ocean. Useful Links MITgcm home page MITgcm documentation Building MITgcm on ARCHER2 MITgcm is not available via a module on ARCHER2 as users will build their own executables specific to the problem they are working on. However, we do provide an optfile which will allow genmake2 to create Makefiles which will work on ARCHER2. Note The processes to build MITgcm on the ARCHER2 4-cabinet system and full system are slightly different. Please make sure you use the commands for the correct system below. You can obtain the MITgcm source code from the developers by cloning from the GitHub repository with the command git clone https://github.com/MITgcm/MITgcm.git You should then copy the ARCHER2 optfile into the MITgcm directories. You may use the files at the locations below for the 4-cabinet and full systems. Full system cp /work/n02/shared/mjmn02/ECCOv4/cases/cce/cce1/scripts/dev_linux_amd64_cray_archer2 MITgcm/tools/build_options/ You should also set the following environment variables. MITGCM_ROOTDIR is used to locate the source code and should point to the top MITgcm directory. Optionally, adding the MITgcm tools directory to your PATH environment variable makes it easier to use tools such as genmake2 , and the MITGCM_OPT environment variable makes it easier to refer to pass the optfile to genmake2 . Full system export MITGCM_ROOTDIR=/path/to/MITgcm export PATH=$MITGCM_ROOTDIR/tools:$PATH export MITGCM_OPT=$MITGCM_ROOTDIR/tools/build_options/dev_linux_amd64_cray_archer2 When using genmake2 to create the Makefile, you will need to specify the optfile to use. Other commonly used options might be to use extra source code with the -mods option, and to enable MPI with -mpi . You might then run a command that resembles the following: genmake2 -mods /path/to/additional/source -mpi -optfile $MITGCM_OPT You can read about the full set of options available to genmake2 by running genmake2 -help Finally, you may then build your executable by running make depend make Running MITgcm on ARCHER2 Once you have built your executable you can write a script like the following which will allow it to run on the ARCHER2 compute nodes. This example would run a pure MPI MITgcm simulation over 2 nodes of 128 cores each for up to one hour. Full system #!/bin/bash # Slurm job options (job-name, compute nodes, job time) #SBATCH --job-name=MITgcm-simulation #SBATCH --time=1:0:0 #SBATCH --nodes=2 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Set the number of threads to 1 # This prevents any threaded system libraries from automatically # using threading. export OMP_NUM_THREADS=1 # Launch the parallel job # Using 256 MPI processes and 128 MPI processes per node # srun picks up the distribution from the sbatch options srun --distribution=block:block --hint=nomultithread ./mitgcmuv Reproducing the ECCO version 4 (release 4) state estimate on ARCHER2 The ECCO version 4 state estimate (ECCOv4-r4) is an observationally-constrained numerical solution produced by the ECCO group at JPL. If you would like to reproduce the state estimate on ARCHER2 in order to create customised runs and experiments, follow the instructions below. They have been slightly modified from the JPL instructions for ARCHER2. For more information, see the ECCOv4-r4 website https://ecco-group.org/products-ECCO-V4r4.htm Get the ECCOv4-r4 source code First, navigate to your directory on the /work filesystem in order to get access to the compute nodes. Next, create a working directory, perhaps MYECCO, and navigate into this working directory: mkdir MYECCO cd MYECCO In order to reproduce ECCOv4-r4, we need a specific checkpoint of the MITgcm source code. git clone https://github.com/MITgcm/MITgcm.git -b checkpoint66g Next, get the ECCOv4-r4 specific code from GitHub: cd MITgcm mkdir -p ECCOV4/release4 cd ECCOV4/release4 git clone https://github.com/ECCO-GROUP/ECCO-v4-Configurations.git mv ECCO-v4-Configurations/ECCOv4\\ Release\\ 4/code . rm -rf ECCO-v4-Configurations Get the ECCOv4-r4 forcing files The surface forcing and other input files that are too large to be stored on GitHub are available via NASA data servers. In total, these files are about 200 GB in size. You must register for an Earthdata account and connect to a WebDAV server in order to access these files. For more detailed instructions, read the help page https://ecco.jpl.nasa.gov/drive/help . First, apply for an Earthdata account: https://urs.earthdata.nasa.gov/users/new Next, acquire your WebDAV credentials: https://ecco.jpl.nasa.gov/drive (second box from the top) Now, you can use wget to download the required forcing and input files: wget -r --no-parent --user YOURUSERNAME --ask-password https://ecco.jpl.nasa.gov/drive/files/Version4/Release4/input_forcing wget -r --no-parent --user YOURUSERNAME --ask-password https://ecco.jpl.nasa.gov/drive/files/Version4/Release4/input_init wget -r --no-parent --user YOURUSERNAME --ask-password https://ecco.jpl.nasa.gov/drive/files/Version4/Release4/input_ecco After using wget , you will notice that the input* directories are, by default, several levels deep in the directory structure. Use the mv command to move the input* directories to the directory where you executed the wget command. Specifically, mv ecco.jpl.nasa.gov/drive/files/Version4/Release4/input_forcing/ . mv ecco.jpl.nasa.gov/drive/files/Version4/Release4/input_init/ . mv ecco.jpl.nasa.gov/drive/files/Version4/Release4/input_ecco/ . rm -rf ecco.jpl.nasa.gov Compiling and running ECCOv4-r4 The steps for building the ECCOv4-r4 instance of MITgcm are very similar to those for other build cases. First, wou will need to create a build directory: cd MITgcm/ECCOV4/release4 mkdir build cd build If you haven't already, copy the ARCHER2 optfile into the MITgcm directories: cp /work/n02/shared/mjmn02/ECCOv4/cases/cce/cce1/scripts/dev_linux_amd64_cray_archer2 MITgcm/tools/build_options/ Load the NetCDF modules: module load cray-hdf5 module load cray-netcdf If you haven't already, set your environment variables: export MITGCM_ROOTDIR=../../../../MITgcm export PATH=$MITGCM_ROOTDIR/tools:$PATH export MITGCM_OPT=$MITGCM_ROOTDIR/tools/build_options/dev_linux_amd64_cray_archer2 Next, compile the executable: genmake2 -mods ../code -mpi -optfile $MITGCM_OPT make depend make Once you have compiled the model, you will have the mitgcmuv executable for ECCOv4-r4. Create run directory and link files In order to run the model, you need to create a run directory and link/copy the appropriate files. First, navigate to your directory on the work filesystem. From the MITgcm/ECCOV4/release4 directory: mkdir run cd run # link the data files ln -s ../input_init/NAMELIST/* . ln -s ../input_init/error_weight/ctrl_weight/* . ln -s ../input_init/error_weight/data_error/* . ln -s ../input_init/* . ln -s ../input_init/tools/* . ln -s ../input_ecco/*/* . ln -s ../input_forcing/eccov4r4* . python mkdir_subdir_diags.py # manually copy the mitgcmuv executable cp -p ../build/mitgcmuv . For a short test run, edit the nTimeSteps variable in the file data . Comment out the default value and uncomment the line reading nTimeSteps=8 . This is a useful test to make sure that the model can at least start up. To run on ARCHER2, submit a batch script to the Slurm scheduler. Here is an example submission script: #!/bin/bash # Slurm job options (job-name, compute nodes, job time) #SBATCH --job-name=ECCOv4r4-test #SBATCH --time=1:0:0 #SBATCH --nodes=8 #SBATCH --tasks-per-node=12 #SBATCH --cpus-per-task=1 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Set the number of threads to 1 # This prevents any threaded system libraries from automatically # using threading. export OMP_NUM_THREADS=1 # Launch the parallel job # Using 256 MPI processes and 128 MPI processes per node # srun picks up the distribution from the sbatch options srun --distribution=block:block --hint=nomultithread ./mitgcmuv This configuration uses 96 MPI processes at 12 MPI processes per node. Once the run has finished, in order to check that the run has successfully completed, check the end of one of the standard output files. tail STDOUT.0000 It should read PROGRAM MAIN: Execution ended Normally The files named STDOUT.* contain diagnostic information that you can use to check your results. As a first pass, check the printed statistics for any clear signs of trouble (e.g. NaN values, extremely large values). ECCOv4-r4 in adjoint mode If you have access to the commercial TAF software produced by http://FastOpt.de , then you can compile and run the ECCOv4-r4 instance of MITgcm in adjoint mode. This mode is useful for comprehensive sensitivity studies and for constructing state estimates. From the MITgcm/ECCOV4/release4 directory, create a new code directory and a new build directory: mkdir code_ad cd code_ad ln -s ../code/* . cd .. mkdir build_ad cd build_ad In this instance, the code_ad and code directories are identical, although this does not have to be the case. Make sure that you have the staf script in your path or in the build_ad directory itself. To make sure that you have the most up-to-date script, run: ./staf -get staf To test your connection to the FastOpt servers, try: ./staf -test You should receive the following message: Your access to the TAF server is enabled. The compilation commands are similar to those used to build the forward case. # load relevant modules module load cray-netcdf-hdf5parallel module load cray-hdf5-parallel # compile adjoint model ../../../MITgcm/tools/genmake2 -ieee -mpi -mods=../code_ad -of=(PATH_TO_OPTFILE) make depend make adtaf make adall The source code will be packaged and forwarded to the FastOpt servers, where it will undergo source-to-source translation via the TAF algorithmic differentiation software. If the compilation is successful, you will have an executable named mitgcmuv_ad . This will run the ECCOv4-r4 configuration of MITgcm in adjoint mode. As before, create a run directory and copy in the relevant files. The procedure is the same as for the forward model, with the following modifications: cd .. mkdir run_ad cd run_ad # manually copy the mitgcmuv executable cp -p ../build_ad/mitgcmuv_ad . To run the model, change the name of the executable in the Slurm submission script; everything else should be the same as in the forward case. As above, at the end of the run you should have a set of STDOUT.* files that you can examine for any obvious problems.","title":"MITgcm"},{"location":"research-software/mitgcm/mitgcm/#mitgcm","text":"The Massachusetts Institute of Technology General Circulation Model (MITgcm) is a numerical model designed for study of the atmosphere, ocean, and climate. MITgcm's flexible non-hydrostatic formulation enables it to simulate fluid phenomena over a wide range of scales; its adjoint capabilities enable it to be applied to sensitivity questions and to parameter and state estimation problems. By employing fluid equation isomorphisms, a single dynamical kernel can be used to simulate flow of both the atmosphere and ocean.","title":"MITgcm"},{"location":"research-software/mitgcm/mitgcm/#useful-links","text":"MITgcm home page MITgcm documentation","title":"Useful Links"},{"location":"research-software/mitgcm/mitgcm/#building-mitgcm-on-archer2","text":"MITgcm is not available via a module on ARCHER2 as users will build their own executables specific to the problem they are working on. However, we do provide an optfile which will allow genmake2 to create Makefiles which will work on ARCHER2. Note The processes to build MITgcm on the ARCHER2 4-cabinet system and full system are slightly different. Please make sure you use the commands for the correct system below. You can obtain the MITgcm source code from the developers by cloning from the GitHub repository with the command git clone https://github.com/MITgcm/MITgcm.git You should then copy the ARCHER2 optfile into the MITgcm directories. You may use the files at the locations below for the 4-cabinet and full systems. Full system cp /work/n02/shared/mjmn02/ECCOv4/cases/cce/cce1/scripts/dev_linux_amd64_cray_archer2 MITgcm/tools/build_options/ You should also set the following environment variables. MITGCM_ROOTDIR is used to locate the source code and should point to the top MITgcm directory. Optionally, adding the MITgcm tools directory to your PATH environment variable makes it easier to use tools such as genmake2 , and the MITGCM_OPT environment variable makes it easier to refer to pass the optfile to genmake2 . Full system export MITGCM_ROOTDIR=/path/to/MITgcm export PATH=$MITGCM_ROOTDIR/tools:$PATH export MITGCM_OPT=$MITGCM_ROOTDIR/tools/build_options/dev_linux_amd64_cray_archer2 When using genmake2 to create the Makefile, you will need to specify the optfile to use. Other commonly used options might be to use extra source code with the -mods option, and to enable MPI with -mpi . You might then run a command that resembles the following: genmake2 -mods /path/to/additional/source -mpi -optfile $MITGCM_OPT You can read about the full set of options available to genmake2 by running genmake2 -help Finally, you may then build your executable by running make depend make","title":"Building MITgcm on ARCHER2"},{"location":"research-software/mitgcm/mitgcm/#running-mitgcm-on-archer2","text":"Once you have built your executable you can write a script like the following which will allow it to run on the ARCHER2 compute nodes. This example would run a pure MPI MITgcm simulation over 2 nodes of 128 cores each for up to one hour. Full system #!/bin/bash # Slurm job options (job-name, compute nodes, job time) #SBATCH --job-name=MITgcm-simulation #SBATCH --time=1:0:0 #SBATCH --nodes=2 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Set the number of threads to 1 # This prevents any threaded system libraries from automatically # using threading. export OMP_NUM_THREADS=1 # Launch the parallel job # Using 256 MPI processes and 128 MPI processes per node # srun picks up the distribution from the sbatch options srun --distribution=block:block --hint=nomultithread ./mitgcmuv","title":"Running MITgcm on ARCHER2"},{"location":"research-software/mitgcm/mitgcm/#reproducing-the-ecco-version-4-release-4-state-estimate-on-archer2","text":"The ECCO version 4 state estimate (ECCOv4-r4) is an observationally-constrained numerical solution produced by the ECCO group at JPL. If you would like to reproduce the state estimate on ARCHER2 in order to create customised runs and experiments, follow the instructions below. They have been slightly modified from the JPL instructions for ARCHER2. For more information, see the ECCOv4-r4 website https://ecco-group.org/products-ECCO-V4r4.htm","title":"Reproducing the ECCO version 4 (release 4) state estimate on ARCHER2"},{"location":"research-software/mitgcm/mitgcm/#get-the-eccov4-r4-source-code","text":"First, navigate to your directory on the /work filesystem in order to get access to the compute nodes. Next, create a working directory, perhaps MYECCO, and navigate into this working directory: mkdir MYECCO cd MYECCO In order to reproduce ECCOv4-r4, we need a specific checkpoint of the MITgcm source code. git clone https://github.com/MITgcm/MITgcm.git -b checkpoint66g Next, get the ECCOv4-r4 specific code from GitHub: cd MITgcm mkdir -p ECCOV4/release4 cd ECCOV4/release4 git clone https://github.com/ECCO-GROUP/ECCO-v4-Configurations.git mv ECCO-v4-Configurations/ECCOv4\\ Release\\ 4/code . rm -rf ECCO-v4-Configurations","title":"Get the ECCOv4-r4 source code"},{"location":"research-software/mitgcm/mitgcm/#get-the-eccov4-r4-forcing-files","text":"The surface forcing and other input files that are too large to be stored on GitHub are available via NASA data servers. In total, these files are about 200 GB in size. You must register for an Earthdata account and connect to a WebDAV server in order to access these files. For more detailed instructions, read the help page https://ecco.jpl.nasa.gov/drive/help . First, apply for an Earthdata account: https://urs.earthdata.nasa.gov/users/new Next, acquire your WebDAV credentials: https://ecco.jpl.nasa.gov/drive (second box from the top) Now, you can use wget to download the required forcing and input files: wget -r --no-parent --user YOURUSERNAME --ask-password https://ecco.jpl.nasa.gov/drive/files/Version4/Release4/input_forcing wget -r --no-parent --user YOURUSERNAME --ask-password https://ecco.jpl.nasa.gov/drive/files/Version4/Release4/input_init wget -r --no-parent --user YOURUSERNAME --ask-password https://ecco.jpl.nasa.gov/drive/files/Version4/Release4/input_ecco After using wget , you will notice that the input* directories are, by default, several levels deep in the directory structure. Use the mv command to move the input* directories to the directory where you executed the wget command. Specifically, mv ecco.jpl.nasa.gov/drive/files/Version4/Release4/input_forcing/ . mv ecco.jpl.nasa.gov/drive/files/Version4/Release4/input_init/ . mv ecco.jpl.nasa.gov/drive/files/Version4/Release4/input_ecco/ . rm -rf ecco.jpl.nasa.gov","title":"Get the ECCOv4-r4 forcing files"},{"location":"research-software/mitgcm/mitgcm/#compiling-and-running-eccov4-r4","text":"The steps for building the ECCOv4-r4 instance of MITgcm are very similar to those for other build cases. First, wou will need to create a build directory: cd MITgcm/ECCOV4/release4 mkdir build cd build If you haven't already, copy the ARCHER2 optfile into the MITgcm directories: cp /work/n02/shared/mjmn02/ECCOv4/cases/cce/cce1/scripts/dev_linux_amd64_cray_archer2 MITgcm/tools/build_options/ Load the NetCDF modules: module load cray-hdf5 module load cray-netcdf If you haven't already, set your environment variables: export MITGCM_ROOTDIR=../../../../MITgcm export PATH=$MITGCM_ROOTDIR/tools:$PATH export MITGCM_OPT=$MITGCM_ROOTDIR/tools/build_options/dev_linux_amd64_cray_archer2 Next, compile the executable: genmake2 -mods ../code -mpi -optfile $MITGCM_OPT make depend make Once you have compiled the model, you will have the mitgcmuv executable for ECCOv4-r4.","title":"Compiling and running ECCOv4-r4"},{"location":"research-software/mitgcm/mitgcm/#create-run-directory-and-link-files","text":"In order to run the model, you need to create a run directory and link/copy the appropriate files. First, navigate to your directory on the work filesystem. From the MITgcm/ECCOV4/release4 directory: mkdir run cd run # link the data files ln -s ../input_init/NAMELIST/* . ln -s ../input_init/error_weight/ctrl_weight/* . ln -s ../input_init/error_weight/data_error/* . ln -s ../input_init/* . ln -s ../input_init/tools/* . ln -s ../input_ecco/*/* . ln -s ../input_forcing/eccov4r4* . python mkdir_subdir_diags.py # manually copy the mitgcmuv executable cp -p ../build/mitgcmuv . For a short test run, edit the nTimeSteps variable in the file data . Comment out the default value and uncomment the line reading nTimeSteps=8 . This is a useful test to make sure that the model can at least start up. To run on ARCHER2, submit a batch script to the Slurm scheduler. Here is an example submission script: #!/bin/bash # Slurm job options (job-name, compute nodes, job time) #SBATCH --job-name=ECCOv4r4-test #SBATCH --time=1:0:0 #SBATCH --nodes=8 #SBATCH --tasks-per-node=12 #SBATCH --cpus-per-task=1 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Set the number of threads to 1 # This prevents any threaded system libraries from automatically # using threading. export OMP_NUM_THREADS=1 # Launch the parallel job # Using 256 MPI processes and 128 MPI processes per node # srun picks up the distribution from the sbatch options srun --distribution=block:block --hint=nomultithread ./mitgcmuv This configuration uses 96 MPI processes at 12 MPI processes per node. Once the run has finished, in order to check that the run has successfully completed, check the end of one of the standard output files. tail STDOUT.0000 It should read PROGRAM MAIN: Execution ended Normally The files named STDOUT.* contain diagnostic information that you can use to check your results. As a first pass, check the printed statistics for any clear signs of trouble (e.g. NaN values, extremely large values).","title":"Create run directory and link files"},{"location":"research-software/mitgcm/mitgcm/#eccov4-r4-in-adjoint-mode","text":"If you have access to the commercial TAF software produced by http://FastOpt.de , then you can compile and run the ECCOv4-r4 instance of MITgcm in adjoint mode. This mode is useful for comprehensive sensitivity studies and for constructing state estimates. From the MITgcm/ECCOV4/release4 directory, create a new code directory and a new build directory: mkdir code_ad cd code_ad ln -s ../code/* . cd .. mkdir build_ad cd build_ad In this instance, the code_ad and code directories are identical, although this does not have to be the case. Make sure that you have the staf script in your path or in the build_ad directory itself. To make sure that you have the most up-to-date script, run: ./staf -get staf To test your connection to the FastOpt servers, try: ./staf -test You should receive the following message: Your access to the TAF server is enabled. The compilation commands are similar to those used to build the forward case. # load relevant modules module load cray-netcdf-hdf5parallel module load cray-hdf5-parallel # compile adjoint model ../../../MITgcm/tools/genmake2 -ieee -mpi -mods=../code_ad -of=(PATH_TO_OPTFILE) make depend make adtaf make adall The source code will be packaged and forwarded to the FastOpt servers, where it will undergo source-to-source translation via the TAF algorithmic differentiation software. If the compilation is successful, you will have an executable named mitgcmuv_ad . This will run the ECCOv4-r4 configuration of MITgcm in adjoint mode. As before, create a run directory and copy in the relevant files. The procedure is the same as for the forward model, with the following modifications: cd .. mkdir run_ad cd run_ad # manually copy the mitgcmuv executable cp -p ../build_ad/mitgcmuv_ad . To run the model, change the name of the executable in the Slurm submission script; everything else should be the same as in the forward case. As above, at the end of the run you should have a set of STDOUT.* files that you can examine for any obvious problems.","title":"ECCOv4-r4 in adjoint mode"},{"location":"research-software/mo-unified-model/mo-unified-model/","text":"Met Office Unified Model The Met Office Unified Model (\"the UM\") is a numerical model of the atmosphere used for both weather and climate applications. It is often coupled to the NEMO ocean model using the OASIS coupling framework to provide a full Earth system model. Useful Links Met Office Unified Model home page Using the UM Information on using the UM is provided by the NCAS Computational Modelling Service (CMS) .","title":"Met Office Unified Model"},{"location":"research-software/mo-unified-model/mo-unified-model/#met-office-unified-model","text":"The Met Office Unified Model (\"the UM\") is a numerical model of the atmosphere used for both weather and climate applications. It is often coupled to the NEMO ocean model using the OASIS coupling framework to provide a full Earth system model.","title":"Met Office Unified Model"},{"location":"research-software/mo-unified-model/mo-unified-model/#useful-links","text":"Met Office Unified Model home page","title":"Useful Links"},{"location":"research-software/mo-unified-model/mo-unified-model/#using-the-um","text":"Information on using the UM is provided by the NCAS Computational Modelling Service (CMS) .","title":"Using the UM"},{"location":"research-software/namd/namd/","text":"NAMD NAMD is an award-winning parallel molecular dynamics code designed for high-performance simulation of large biomolecular systems. Based on Charm++ parallel objects, NAMD scales to hundreds of cores for typical simulations and beyond 500,000 cores for the largest simulations. NAMD uses the popular molecular graphics program VMD for simulation setup and trajectory analysis, but is also file-compatible with AMBER, CHARMM, and X-PLOR. Useful Links NAMD User Guide NAMD Tutorials Using NAMD on ARCHER2 Note Our optimisation advice is based on the ARCHER2 4-cabinet preview system with the same node architecture as the current ARCHER2 service but a total of 1,024 compute nodes. NAMD is freely available to all ARCHER2 users. ------------------- /work/y07/shared/archer2-lmod/apps/core -------------------- namd/2.14-nosmp namd/2.14 (D) ARCHER2 has two versions of NAMD available: no-SMP ( namd/2.14-nosmp ) or SMP ( namd/2.14 ). The SMP (Shared Memory Parallelism) build of NAMD introduces threaded parallelism to address memory limitations. The no-SMP build will typically provide the best performance but most users will require SMP in order to cope with high memory requirements. Running MPI only NAMD jobs Using no-SMP NAMD will run jobs with only MPI processes and will not introduce additional threaded parallelism. This is the simplest approach to running NAMD jobs and is likely to give the best performance unless simulations are limited by high memory requirements. The following script will run a pure MPI NAMD MD job using 4 nodes (i.e. 128x4 = 512 MPI parallel processes). Full system #!/bin/bash # Request four nodes to run a job of 512 MPI tasks with 128 MPI # tasks per node, here for maximum time 20 minutes. #SBATCH --job-name=namd-nosmp #SBATCH --nodes=4 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 #SBATCH --time=00:20:00 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard module load namd/2.14-nosmp srun --distribution=block:block --hint=nomultithread namd2 input.namd Running SMP NAMD jobs If your jobs runs out of memory, then using the SMP version of NAMD will reduce the memory requirements. This involves launching a combination of MPI processes for communication and worker threads which perform computation. The following script will run a SMP NAMD MD job using 4 nodes with 8 MPI communication processes per node and 16 worker threads per communication process (i.e. a fully-occupied node with all 512 cores populated with processes). Full system #!/bin/bash #SBATCH --job-name=namd-smp #SBATCH --tasks-per-node=8 #SBATCH --cpus-per-task=16 #SBATCH --nodes=4 #SBATCH --time=00:20:00 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Load the relevant modules module load namd # Set procs per node (PPN) & OMP_NUM_THREADS export PPN=$(($SLURM_CPUS_PER_TASK-1)) export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK export OMP_PLACES=cores # Record PPN in the output file echo \"Number of worker threads PPN = $PPN\" # Run NAMD srun --distribution=block:block --hint=nomultithread namd2 +setcpuaffinity +ppn $PPN input.namd How do I choose an optimal choice of MPI processes and worker threads for my simulations? The optimal choice for the numbers of MPI processes and worker threads per node depends on the data set and the number of compute nodes. Before running large production jobs, it is worth experimenting with these parameters to find the optimal configuration for your simulation. We recommend that users match the ARCHER2 NUMA architecture, with 8 NUMA regions per node and 16 cores per region, to find the optimal balance of thread and process parallelism. For example, the above submission script specifies 8 MPI communication processes per node and 16 worker threads per communication process which places 1 MPI process per NUMA region on each node. Note To ensure fully occupied nodes with the SMP build of NAMD and match the NUMA region, the optimal values of ( tasks-per-node , cpus-per-task ) are likely to be (32,4), (16,8) or (8,16). How do I choose a value for the +ppn flag? The number of workers per communication process is specified by the +ppn argument to NAMD, which is set here to equal cpus-per-task - 1, to leave a CPU-core free for the associated MPI process. We recommend that users reserve a thread per process to improve the scalability. Reserving this thread on a many-cores-per-node architecture like ARCHER2 will reduce the communication between threads and improve the scalability. Compiling NAMD The latest instructions for building NAMD on ARCHER2 may be found in the GitHub repository of build instructions. ARCHER2 Full System","title":"NAMD"},{"location":"research-software/namd/namd/#namd","text":"NAMD is an award-winning parallel molecular dynamics code designed for high-performance simulation of large biomolecular systems. Based on Charm++ parallel objects, NAMD scales to hundreds of cores for typical simulations and beyond 500,000 cores for the largest simulations. NAMD uses the popular molecular graphics program VMD for simulation setup and trajectory analysis, but is also file-compatible with AMBER, CHARMM, and X-PLOR.","title":"NAMD"},{"location":"research-software/namd/namd/#useful-links","text":"NAMD User Guide NAMD Tutorials","title":"Useful Links"},{"location":"research-software/namd/namd/#using-namd-on-archer2","text":"Note Our optimisation advice is based on the ARCHER2 4-cabinet preview system with the same node architecture as the current ARCHER2 service but a total of 1,024 compute nodes. NAMD is freely available to all ARCHER2 users. ------------------- /work/y07/shared/archer2-lmod/apps/core -------------------- namd/2.14-nosmp namd/2.14 (D) ARCHER2 has two versions of NAMD available: no-SMP ( namd/2.14-nosmp ) or SMP ( namd/2.14 ). The SMP (Shared Memory Parallelism) build of NAMD introduces threaded parallelism to address memory limitations. The no-SMP build will typically provide the best performance but most users will require SMP in order to cope with high memory requirements.","title":"Using NAMD on ARCHER2"},{"location":"research-software/namd/namd/#running-mpi-only-namd-jobs","text":"Using no-SMP NAMD will run jobs with only MPI processes and will not introduce additional threaded parallelism. This is the simplest approach to running NAMD jobs and is likely to give the best performance unless simulations are limited by high memory requirements. The following script will run a pure MPI NAMD MD job using 4 nodes (i.e. 128x4 = 512 MPI parallel processes). Full system #!/bin/bash # Request four nodes to run a job of 512 MPI tasks with 128 MPI # tasks per node, here for maximum time 20 minutes. #SBATCH --job-name=namd-nosmp #SBATCH --nodes=4 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 #SBATCH --time=00:20:00 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard module load namd/2.14-nosmp srun --distribution=block:block --hint=nomultithread namd2 input.namd","title":"Running MPI only NAMD jobs"},{"location":"research-software/namd/namd/#running-smp-namd-jobs","text":"If your jobs runs out of memory, then using the SMP version of NAMD will reduce the memory requirements. This involves launching a combination of MPI processes for communication and worker threads which perform computation. The following script will run a SMP NAMD MD job using 4 nodes with 8 MPI communication processes per node and 16 worker threads per communication process (i.e. a fully-occupied node with all 512 cores populated with processes). Full system #!/bin/bash #SBATCH --job-name=namd-smp #SBATCH --tasks-per-node=8 #SBATCH --cpus-per-task=16 #SBATCH --nodes=4 #SBATCH --time=00:20:00 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Load the relevant modules module load namd # Set procs per node (PPN) & OMP_NUM_THREADS export PPN=$(($SLURM_CPUS_PER_TASK-1)) export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK export OMP_PLACES=cores # Record PPN in the output file echo \"Number of worker threads PPN = $PPN\" # Run NAMD srun --distribution=block:block --hint=nomultithread namd2 +setcpuaffinity +ppn $PPN input.namd How do I choose an optimal choice of MPI processes and worker threads for my simulations? The optimal choice for the numbers of MPI processes and worker threads per node depends on the data set and the number of compute nodes. Before running large production jobs, it is worth experimenting with these parameters to find the optimal configuration for your simulation. We recommend that users match the ARCHER2 NUMA architecture, with 8 NUMA regions per node and 16 cores per region, to find the optimal balance of thread and process parallelism. For example, the above submission script specifies 8 MPI communication processes per node and 16 worker threads per communication process which places 1 MPI process per NUMA region on each node. Note To ensure fully occupied nodes with the SMP build of NAMD and match the NUMA region, the optimal values of ( tasks-per-node , cpus-per-task ) are likely to be (32,4), (16,8) or (8,16). How do I choose a value for the +ppn flag? The number of workers per communication process is specified by the +ppn argument to NAMD, which is set here to equal cpus-per-task - 1, to leave a CPU-core free for the associated MPI process. We recommend that users reserve a thread per process to improve the scalability. Reserving this thread on a many-cores-per-node architecture like ARCHER2 will reduce the communication between threads and improve the scalability.","title":"Running SMP NAMD jobs"},{"location":"research-software/namd/namd/#compiling-namd","text":"The latest instructions for building NAMD on ARCHER2 may be found in the GitHub repository of build instructions. ARCHER2 Full System","title":"Compiling NAMD"},{"location":"research-software/nektarplusplus/nektarplusplus/","text":"Nektar++ Nektar++ is a tensor product based finite element package designed to allow one to construct efficient classical low polynomial order h -type solvers (where h is the size of the finite element) as well as higher p -order piecewise polynomial order solvers. The Nektar++ framework comes with a number of solvers and also allows one to construct a variety of new solvers. Users can therefore use Nektar++ just to run simulations, or to extend and/or develop new functionality. Useful Links Nektar++ home page Nektar++ tutorials Nektar GitLab repository Using Nektar++ on ARCHER2 Nektar++ is released under an MIT license and is available to all users on the ARCHER2 full system. Where can I get help? Specific issues with Nektar++ itself might be submitted to the issue tracker at the Nektar++ gitlab repository (see link above). More general questions might also be directed to the Nektar-users mailing list . Issues specific to the use or behaviour of Nektar++ on ARCHER2 should be sent to the Service Desk . Running parallel Nektar++ jobs Below is the submission script for running the Taylor-Green Vortex, one of the Nektar++ tutorials, see https://doc.nektar.info/tutorials/latest/incns/taylor-green-vortex/incns-taylor-green-vortex.html#incns-taylor-green-vortexch4.html . You first need to download the archive linked on the tutorial page. cd /path/to/work/dir wget https://doc.nektar.info/tutorials/latest/incns/taylor-green-vortex/incns-taylor-green-vortex.tar.gz tar -xvzf incns-taylor-green-vortex.tar.gz Full system #!/bin/bash #SBATCH --job-name=nektar #SBATCH --nodes=1 #SBATCH --tasks-per-node=32 #SBATCH --cpus-per-task=1 #SBATCH --time=02:00:00 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard module load nektar export OMP_NUM_THREADS = 1 NEK_INPUT_PATH = /path/to/work/dir/incns-taylor-green-vortex/completed/solver64 srun --distribution = block:cyclic --hint = nomultithread \\ ${ NEK_DIR } /bin/IncNavierStokesSolver \\ ${ NEK_INPUT_PATH } /TGV64_mesh.xml \\ ${ NEK_INPUT_PATH } /TGV64_conditions.xml Compiling Nektar++ The latest instructions for building Nektar++ on ARCHER2 may be found in the GitHub repository of build instructions. ARCHER2 Full System","title":"Nektar++"},{"location":"research-software/nektarplusplus/nektarplusplus/#nektar","text":"Nektar++ is a tensor product based finite element package designed to allow one to construct efficient classical low polynomial order h -type solvers (where h is the size of the finite element) as well as higher p -order piecewise polynomial order solvers. The Nektar++ framework comes with a number of solvers and also allows one to construct a variety of new solvers. Users can therefore use Nektar++ just to run simulations, or to extend and/or develop new functionality.","title":"Nektar++"},{"location":"research-software/nektarplusplus/nektarplusplus/#useful-links","text":"Nektar++ home page Nektar++ tutorials Nektar GitLab repository","title":"Useful Links"},{"location":"research-software/nektarplusplus/nektarplusplus/#using-nektar-on-archer2","text":"Nektar++ is released under an MIT license and is available to all users on the ARCHER2 full system.","title":"Using Nektar++ on ARCHER2"},{"location":"research-software/nektarplusplus/nektarplusplus/#where-can-i-get-help","text":"Specific issues with Nektar++ itself might be submitted to the issue tracker at the Nektar++ gitlab repository (see link above). More general questions might also be directed to the Nektar-users mailing list . Issues specific to the use or behaviour of Nektar++ on ARCHER2 should be sent to the Service Desk .","title":"Where can I get help?"},{"location":"research-software/nektarplusplus/nektarplusplus/#running-parallel-nektar-jobs","text":"Below is the submission script for running the Taylor-Green Vortex, one of the Nektar++ tutorials, see https://doc.nektar.info/tutorials/latest/incns/taylor-green-vortex/incns-taylor-green-vortex.html#incns-taylor-green-vortexch4.html . You first need to download the archive linked on the tutorial page. cd /path/to/work/dir wget https://doc.nektar.info/tutorials/latest/incns/taylor-green-vortex/incns-taylor-green-vortex.tar.gz tar -xvzf incns-taylor-green-vortex.tar.gz Full system #!/bin/bash #SBATCH --job-name=nektar #SBATCH --nodes=1 #SBATCH --tasks-per-node=32 #SBATCH --cpus-per-task=1 #SBATCH --time=02:00:00 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard module load nektar export OMP_NUM_THREADS = 1 NEK_INPUT_PATH = /path/to/work/dir/incns-taylor-green-vortex/completed/solver64 srun --distribution = block:cyclic --hint = nomultithread \\ ${ NEK_DIR } /bin/IncNavierStokesSolver \\ ${ NEK_INPUT_PATH } /TGV64_mesh.xml \\ ${ NEK_INPUT_PATH } /TGV64_conditions.xml","title":"Running parallel Nektar++ jobs"},{"location":"research-software/nektarplusplus/nektarplusplus/#compiling-nektar","text":"The latest instructions for building Nektar++ on ARCHER2 may be found in the GitHub repository of build instructions. ARCHER2 Full System","title":"Compiling Nektar++"},{"location":"research-software/nemo/nemo/","text":"NEMO NEMO (Nucleus for European Modelling of the Ocean) is a state-of-the-art framework for research activities and forecasting services in ocean and climate sciences, developed in a sustainable way by a European consortium. Useful Links NEMO home page NEMO documentation NEMO users' area , which includes information on obtaining and downloading the latest source code releases. NEMO is released under a CeCILL license and is freely available to all users on ARCHER2. Compiling NEMO A central install of NEMO is not appropriate for most users of ARCHER2 since many configurations will want to add bespoke code changes. The latest instructions for building NEMO on ARCHER2 are found in the Github repository of build instructions: Build instructions for NEMO on GitHub Using NEMO on ARCHER2 Typical NEMO production runs perform significant I/O management to handle the very large volumes of data associated with ocean modelling. To address this, NEMO ocean clients are interfaced with XIOS I/O servers. XIOS is a library which manages NetCDF outputs for climate models. NEMO uses XIOS to simplify the I/O management and introduce dedicated processors to manage large volumes of data. Users can choose to run NEMO in attached or detached mode: - In attached mode each processor acts as an ocean client and I/O-server process. - In detached mode ocean clients and external XIOS I/O-server processors are separately defined. Running NEMO in attached mode can be done with a simple submission script specifying both the NEMO and XIOS executable to srun . However, typical production runs of NEMO will perform significant I/O management and will be unable to run in attached mode. Detached mode introduces external XIOS I/O-servers to help manage the large volumes of data. This requires users to specify the placement of clients and servers on different cores throughout the node using the \u2013cpu-bind=map_cpu:<cpu map> srun option to define a CPU map or mask. It is tedious to construct these maps by hand. Instead, Andrew Coward provides a tool to aid users in the construction submission scripts: /work/n01/shared/nemo/mkslurm_hetjob /work/n01/shared/nemo/mkslurm_hetjob_gnu Usage of the script: usage: mkslurm_hetjob [-h] [-S S] [-s S] [-m M] [-C C] [-g G] [-N N] [-t T] [-a A] [-j J] [-v] Python version of mkslurm_alt by Andrew Coward using HetJob. Server placement and spacing remains as mkslurm but clients are always tightly packed with a gap left every \"NC_GAP\" cores where NC_GAP can be given by the -g argument. values of 4, 8 or 16 are recommended. optional arguments: -h, --help show this help message and exit -S S num_servers (default: 4) -s S server_spacing (default: 8) -m M max_servers_per_node (default: 2) -C C num_clients (default: 28) -g G client_gap_interval (default: 4) -N N ncores_per_node (default: 128) -t T time_limit (default: 00:10:00) -a A account (default: n01) -j J job_name (default: nemo_test) -v show human readable hetjobs (default: False) Note We recommend that you retain your own copy of this script as it is not directly provided by the ARCHER2 CSE team and subject to change. Once obtained, you can set your own defaults for options in the script. For example, to run with 4 XIOS I/O-servers (a maximum of 2 per node), each with sole occupancy of a 16-core NUMA region and 96 ocean cores, spaced with a idle core in between each, use: ./mkslurm_hetjob -S 4 -s 16 -m 2 -C 96 -g 2 > myscript.slurm INFO:root:Running mkslurm_hetjob -S 4 -s 16 -m 2 -C 96 -g 2 -N 128 -t 00:10:00 -a n01 -j nemo_test -v False INFO:root:nodes needed= 2 (256) INFO:root:cores to be used= 100 (256) This has reported that 2 nodes are needed with 100 active cores spread over 256 cores. This will also have produced a submission script \"myscript.slurm\": #!/bin/bash #SBATCH --job-name=nemo_test #SBATCH --time=00:10:00 #SBATCH --account=n01 #SBATCH --partition=standard #SBATCH --qos=standard #SBATCH --nodes=2 #SBATCH --ntasks-per-core=1 # Created by: mkslurm_hetjob -S 4 -s 16 -m 2 -C 96 -g 2 -N 128 -t 00:10:00 -a n01 -j nemo_test -v False module swap craype-network-ofi craype-network-ucx module swap cray-mpich cray-mpich-ucx module load cray-hdf5-parallel/1.12.0.7 module load cray-netcdf-hdf5parallel/4.7.4.7 export OMP_NUM_THREADS=1 cat > myscript_wrapper.sh << EOFB #!/bin/ksh # set -A map ./xios_server.exe ./nemo exec_map=( 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ) # exec \\${map[\\${exec_map[\\$SLURM_PROCID]}]} ## EOFB chmod u+x ./myscript_wrapper.sh srun --mem-bind=local \\ --ntasks=100 --ntasks-per-node=50 --cpu-bind=v,mask_cpu:0x1,0x10000,0x100000000,0x400000000,0x1000000000,0x4000000000,0x10000000000,0x40000000000,0x100000000000,0x400000000000,0x1000000000000,0x4000000000000,0x10000000000000,0x40000000000000,0x100000000000000,0x400000000000000,0x1000000000000000,0x4000000000000000,0x10000000000000000,0x40000000000000000,0x100000000000000000,0x400000000000000000,0x1000000000000000000,0x4000000000000000000,0x10000000000000000000,0x40000000000000000000,0x100000000000000000000,0x400000000000000000000,0x1000000000000000000000,0x4000000000000000000000,0x10000000000000000000000,0x40000000000000000000000,0x100000000000000000000000,0x400000000000000000000000,0x1000000000000000000000000,0x4000000000000000000000000,0x10000000000000000000000000,0x40000000000000000000000000,0x100000000000000000000000000,0x400000000000000000000000000,0x1000000000000000000000000000,0x4000000000000000000000000000,0x10000000000000000000000000000,0x40000000000000000000000000000,0x100000000000000000000000000000,0x400000000000000000000000000000,0x1000000000000000000000000000000,0x4000000000000000000000000000000,0x10000000000000000000000000000000,0x40000000000000000000000000000000 ./myscript_wrapper.sh Submitting this script in a directory with the nemo and xios_server.exe executables will run the desired MPMD job. The exec_map array shows the position of each executable in the rank list (0 = xios_server.exe, 1 = nemo). For larger core counts the cpu_map can be limited to a single node map which will be cycled through as many times as necessary. How to optimise the performance of NEMO Note Our optimisation advice is based on the ARCHER2 4-cabinet preview system with the same node architecture as the current ARCHER2 service but a total of 1,024 compute nodes. During these investigations we used NEMO-4.0.6 and XIOS-2.5. Through testing with idealised test cases to optimise the computational performance (i.e. without the demanding I/O management that is typical of NEMO production runs), we have found that drastically under-populating the nodes does not affect the performance of the computation. This indicates that users can reserve large portions of the nodes without a performance detriment. Users can run larger simulations by reserving up to 75% of the node can be reserved for I/O management (i.e. XIOS I/O-servers). XIOS I/O-servers can be more lightly packed than ocean clients and should be evenly distributed amongst the nodes i.e. not concentrated on a specific node. We found that placing 1 XIOS I/O-server per node with 4, 8, and 16 dedicated cores did not affect the performance. However, the performance was affected when allocating dedicated I/O-server cores outside of a 16-core NUMA region. Thus, users should confine XIOS I/O-servers to NUMA regions to improve performance and benefit from the memory hierarchy. A performance investigation Note These results were collated during early user testing of the ARCHER2 service by Andrew Coward and is subject to change. This table shows some preliminary results of a repeated 60 day simulation of the ORCA2_ICE_PISCES, SETTE configuration using various core counts and packing strategies: Note These results used the mkslurm script, now hosted in /work/n01/shared/nemo/old_scripts/mkslurm It is clear from the previous results that fully populating an ARCHER2 node is unlikely to provide the optimal performance for any codes with moderate memory bandwidth requirements. The explored regular packing strategy does not allow experimentation with less wasteful packing strategies than half-population though. There may be a case, for example, for just leaving every 1 in 4 cores idle, or every 1 in 8, or even fewer idle cores per node. The mkslurm_alt script (/work/n01/shared/nemo/old_scripts/mkslurm_alt) provided a method of generating cpu-bind maps for exploring these strategies. The script assumed no change in the packing strategy for the servers but the core spacing argument (-c) for the ocean cores is replaced by a -g option representing the frequency of a gap in the, otherwise tightly-packed, ocean cores. Preliminary tests have been conducted with the ORCA2_ICE_PISCES SETTE test case. This is a relatively small test case that will fit onto a single node. It is also small enough to perform well in attached mode. First some baseline tests in attached mode. Previous tests used 4 I/O servers each occupying a single NUMA. For this size model, 2 servers occupying half a NUMA each will suffice. That leaves 112 cores with which to try different packing strategies. Is it possible to match or better this elapsed time on a single node including external I/O servers? -Yes! -but not with an obvious gap frequency: And activating land suppression can reduce times further: The optimal two-node solution is also shown (this is quicker but the one node solution is cheaper). This leads us to the current iteration of the mkslurm script - mkslurm_hetjob. Note a tightly-packed placement with no gaps amongst the ocean processes can be generated using a client gap interval greater than the number of clients. This script has been used to explore the different placement strategies with a larger configuration based on eORCA025. In all cases, 8 XIOS servers were used, each with sole occupancy of a 16-core NUMA and a maximum of 2 servers per node. The rest of the initial 4 nodes (and any subsequent ocean core-only nodes) were filled with ocean cores at various packing densities (from tightly packed to half-populated). A summary of the results are shown below. The limit of scalability for this problem size lies around 1500 cores. One interesting aspect is that the cost, in terms of node hours, remains fairly flat up to a thousand processes and the choice of gap placement makes much less difference as the individual domains shrink. It looks as if, so long as you avoid inappropriately high numbers of processors, choosing the wrong placement won't waste your allocation but may waste your time.","title":"NEMO"},{"location":"research-software/nemo/nemo/#nemo","text":"NEMO (Nucleus for European Modelling of the Ocean) is a state-of-the-art framework for research activities and forecasting services in ocean and climate sciences, developed in a sustainable way by a European consortium.","title":"NEMO"},{"location":"research-software/nemo/nemo/#useful-links","text":"NEMO home page NEMO documentation NEMO users' area , which includes information on obtaining and downloading the latest source code releases. NEMO is released under a CeCILL license and is freely available to all users on ARCHER2.","title":"Useful Links"},{"location":"research-software/nemo/nemo/#compiling-nemo","text":"A central install of NEMO is not appropriate for most users of ARCHER2 since many configurations will want to add bespoke code changes. The latest instructions for building NEMO on ARCHER2 are found in the Github repository of build instructions: Build instructions for NEMO on GitHub","title":"Compiling NEMO"},{"location":"research-software/nemo/nemo/#using-nemo-on-archer2","text":"Typical NEMO production runs perform significant I/O management to handle the very large volumes of data associated with ocean modelling. To address this, NEMO ocean clients are interfaced with XIOS I/O servers. XIOS is a library which manages NetCDF outputs for climate models. NEMO uses XIOS to simplify the I/O management and introduce dedicated processors to manage large volumes of data. Users can choose to run NEMO in attached or detached mode: - In attached mode each processor acts as an ocean client and I/O-server process. - In detached mode ocean clients and external XIOS I/O-server processors are separately defined. Running NEMO in attached mode can be done with a simple submission script specifying both the NEMO and XIOS executable to srun . However, typical production runs of NEMO will perform significant I/O management and will be unable to run in attached mode. Detached mode introduces external XIOS I/O-servers to help manage the large volumes of data. This requires users to specify the placement of clients and servers on different cores throughout the node using the \u2013cpu-bind=map_cpu:<cpu map> srun option to define a CPU map or mask. It is tedious to construct these maps by hand. Instead, Andrew Coward provides a tool to aid users in the construction submission scripts: /work/n01/shared/nemo/mkslurm_hetjob /work/n01/shared/nemo/mkslurm_hetjob_gnu Usage of the script: usage: mkslurm_hetjob [-h] [-S S] [-s S] [-m M] [-C C] [-g G] [-N N] [-t T] [-a A] [-j J] [-v] Python version of mkslurm_alt by Andrew Coward using HetJob. Server placement and spacing remains as mkslurm but clients are always tightly packed with a gap left every \"NC_GAP\" cores where NC_GAP can be given by the -g argument. values of 4, 8 or 16 are recommended. optional arguments: -h, --help show this help message and exit -S S num_servers (default: 4) -s S server_spacing (default: 8) -m M max_servers_per_node (default: 2) -C C num_clients (default: 28) -g G client_gap_interval (default: 4) -N N ncores_per_node (default: 128) -t T time_limit (default: 00:10:00) -a A account (default: n01) -j J job_name (default: nemo_test) -v show human readable hetjobs (default: False) Note We recommend that you retain your own copy of this script as it is not directly provided by the ARCHER2 CSE team and subject to change. Once obtained, you can set your own defaults for options in the script. For example, to run with 4 XIOS I/O-servers (a maximum of 2 per node), each with sole occupancy of a 16-core NUMA region and 96 ocean cores, spaced with a idle core in between each, use: ./mkslurm_hetjob -S 4 -s 16 -m 2 -C 96 -g 2 > myscript.slurm INFO:root:Running mkslurm_hetjob -S 4 -s 16 -m 2 -C 96 -g 2 -N 128 -t 00:10:00 -a n01 -j nemo_test -v False INFO:root:nodes needed= 2 (256) INFO:root:cores to be used= 100 (256) This has reported that 2 nodes are needed with 100 active cores spread over 256 cores. This will also have produced a submission script \"myscript.slurm\": #!/bin/bash #SBATCH --job-name=nemo_test #SBATCH --time=00:10:00 #SBATCH --account=n01 #SBATCH --partition=standard #SBATCH --qos=standard #SBATCH --nodes=2 #SBATCH --ntasks-per-core=1 # Created by: mkslurm_hetjob -S 4 -s 16 -m 2 -C 96 -g 2 -N 128 -t 00:10:00 -a n01 -j nemo_test -v False module swap craype-network-ofi craype-network-ucx module swap cray-mpich cray-mpich-ucx module load cray-hdf5-parallel/1.12.0.7 module load cray-netcdf-hdf5parallel/4.7.4.7 export OMP_NUM_THREADS=1 cat > myscript_wrapper.sh << EOFB #!/bin/ksh # set -A map ./xios_server.exe ./nemo exec_map=( 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ) # exec \\${map[\\${exec_map[\\$SLURM_PROCID]}]} ## EOFB chmod u+x ./myscript_wrapper.sh srun --mem-bind=local \\ --ntasks=100 --ntasks-per-node=50 --cpu-bind=v,mask_cpu:0x1,0x10000,0x100000000,0x400000000,0x1000000000,0x4000000000,0x10000000000,0x40000000000,0x100000000000,0x400000000000,0x1000000000000,0x4000000000000,0x10000000000000,0x40000000000000,0x100000000000000,0x400000000000000,0x1000000000000000,0x4000000000000000,0x10000000000000000,0x40000000000000000,0x100000000000000000,0x400000000000000000,0x1000000000000000000,0x4000000000000000000,0x10000000000000000000,0x40000000000000000000,0x100000000000000000000,0x400000000000000000000,0x1000000000000000000000,0x4000000000000000000000,0x10000000000000000000000,0x40000000000000000000000,0x100000000000000000000000,0x400000000000000000000000,0x1000000000000000000000000,0x4000000000000000000000000,0x10000000000000000000000000,0x40000000000000000000000000,0x100000000000000000000000000,0x400000000000000000000000000,0x1000000000000000000000000000,0x4000000000000000000000000000,0x10000000000000000000000000000,0x40000000000000000000000000000,0x100000000000000000000000000000,0x400000000000000000000000000000,0x1000000000000000000000000000000,0x4000000000000000000000000000000,0x10000000000000000000000000000000,0x40000000000000000000000000000000 ./myscript_wrapper.sh Submitting this script in a directory with the nemo and xios_server.exe executables will run the desired MPMD job. The exec_map array shows the position of each executable in the rank list (0 = xios_server.exe, 1 = nemo). For larger core counts the cpu_map can be limited to a single node map which will be cycled through as many times as necessary.","title":"Using NEMO on ARCHER2"},{"location":"research-software/nemo/nemo/#how-to-optimise-the-performance-of-nemo","text":"Note Our optimisation advice is based on the ARCHER2 4-cabinet preview system with the same node architecture as the current ARCHER2 service but a total of 1,024 compute nodes. During these investigations we used NEMO-4.0.6 and XIOS-2.5. Through testing with idealised test cases to optimise the computational performance (i.e. without the demanding I/O management that is typical of NEMO production runs), we have found that drastically under-populating the nodes does not affect the performance of the computation. This indicates that users can reserve large portions of the nodes without a performance detriment. Users can run larger simulations by reserving up to 75% of the node can be reserved for I/O management (i.e. XIOS I/O-servers). XIOS I/O-servers can be more lightly packed than ocean clients and should be evenly distributed amongst the nodes i.e. not concentrated on a specific node. We found that placing 1 XIOS I/O-server per node with 4, 8, and 16 dedicated cores did not affect the performance. However, the performance was affected when allocating dedicated I/O-server cores outside of a 16-core NUMA region. Thus, users should confine XIOS I/O-servers to NUMA regions to improve performance and benefit from the memory hierarchy.","title":"How to optimise the performance of NEMO"},{"location":"research-software/nemo/nemo/#a-performance-investigation","text":"Note These results were collated during early user testing of the ARCHER2 service by Andrew Coward and is subject to change. This table shows some preliminary results of a repeated 60 day simulation of the ORCA2_ICE_PISCES, SETTE configuration using various core counts and packing strategies: Note These results used the mkslurm script, now hosted in /work/n01/shared/nemo/old_scripts/mkslurm It is clear from the previous results that fully populating an ARCHER2 node is unlikely to provide the optimal performance for any codes with moderate memory bandwidth requirements. The explored regular packing strategy does not allow experimentation with less wasteful packing strategies than half-population though. There may be a case, for example, for just leaving every 1 in 4 cores idle, or every 1 in 8, or even fewer idle cores per node. The mkslurm_alt script (/work/n01/shared/nemo/old_scripts/mkslurm_alt) provided a method of generating cpu-bind maps for exploring these strategies. The script assumed no change in the packing strategy for the servers but the core spacing argument (-c) for the ocean cores is replaced by a -g option representing the frequency of a gap in the, otherwise tightly-packed, ocean cores. Preliminary tests have been conducted with the ORCA2_ICE_PISCES SETTE test case. This is a relatively small test case that will fit onto a single node. It is also small enough to perform well in attached mode. First some baseline tests in attached mode. Previous tests used 4 I/O servers each occupying a single NUMA. For this size model, 2 servers occupying half a NUMA each will suffice. That leaves 112 cores with which to try different packing strategies. Is it possible to match or better this elapsed time on a single node including external I/O servers? -Yes! -but not with an obvious gap frequency: And activating land suppression can reduce times further: The optimal two-node solution is also shown (this is quicker but the one node solution is cheaper). This leads us to the current iteration of the mkslurm script - mkslurm_hetjob. Note a tightly-packed placement with no gaps amongst the ocean processes can be generated using a client gap interval greater than the number of clients. This script has been used to explore the different placement strategies with a larger configuration based on eORCA025. In all cases, 8 XIOS servers were used, each with sole occupancy of a 16-core NUMA and a maximum of 2 servers per node. The rest of the initial 4 nodes (and any subsequent ocean core-only nodes) were filled with ocean cores at various packing densities (from tightly packed to half-populated). A summary of the results are shown below. The limit of scalability for this problem size lies around 1500 cores. One interesting aspect is that the cost, in terms of node hours, remains fairly flat up to a thousand processes and the choice of gap placement makes much less difference as the individual domains shrink. It looks as if, so long as you avoid inappropriately high numbers of processors, choosing the wrong placement won't waste your allocation but may waste your time.","title":"A performance investigation"},{"location":"research-software/nwchem/nwchem/","text":"NWChem NWChem aims to provide its users with computational chemistry tools that are scalable both in their ability to treat large scientific computational chemistry problems efficiently, and in their use of available parallel computing resources from high-performance parallel supercomputers to conventional workstation clusters. The NWChem software can handle: biomolecules, nanostructures, and solid-state system; from quantum to classical, and all combinations; Gaussian basis functions or plane-waves; scaling from one to thousands of processors; properties and relativity. Useful Links NWChem home page NWChem documentation NWChem forum Using NWChem on ARCHER2 NWChem is released under an Educational Community License (ECL 2.0) and is freely available to all users on ARCHER2. Where can I get help? If you have problems accessing or running NWChem on ARCHER2, please contact the Service Desk. General questions on the use of NWChem might also be directed to the [NWChem forum][1]. More experienced users with detailed technical issues on NWChem should consider submitting them to the NWChem GitHub issue tracker . Running NWChem jobs The following script will run a NWChem job using 2 nodes (256 cores) in the standard partition. It assumes that the input file is called test\\_calc.nw . Full system #!/bin/bash # Request 2 nodes with 128 MPI tasks per node for 20 minutes #SBATCH --job-name=NWChem_test #SBATCH --nodes=2 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 #SBATCH --time=00:20:00 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Load the NWChem module, avoid any unintentional OpenMP threading by # setting OMP_NUM_THREADS, and launch the code. module load nwchem export OMP_NUM_THREADS=1 srun --distribution=block:block --hint=nomultithread nwchem test_calc Compiling NWChem The latest instructions for building NWChem on ARCHER2 may be found in the GitHub repository of build instructions: Build instructions for NWChem on GitHub","title":"NWChem"},{"location":"research-software/nwchem/nwchem/#nwchem","text":"NWChem aims to provide its users with computational chemistry tools that are scalable both in their ability to treat large scientific computational chemistry problems efficiently, and in their use of available parallel computing resources from high-performance parallel supercomputers to conventional workstation clusters. The NWChem software can handle: biomolecules, nanostructures, and solid-state system; from quantum to classical, and all combinations; Gaussian basis functions or plane-waves; scaling from one to thousands of processors; properties and relativity.","title":"NWChem"},{"location":"research-software/nwchem/nwchem/#useful-links","text":"NWChem home page NWChem documentation NWChem forum","title":"Useful Links"},{"location":"research-software/nwchem/nwchem/#using-nwchem-on-archer2","text":"NWChem is released under an Educational Community License (ECL 2.0) and is freely available to all users on ARCHER2.","title":"Using NWChem on ARCHER2"},{"location":"research-software/nwchem/nwchem/#where-can-i-get-help","text":"If you have problems accessing or running NWChem on ARCHER2, please contact the Service Desk. General questions on the use of NWChem might also be directed to the [NWChem forum][1]. More experienced users with detailed technical issues on NWChem should consider submitting them to the NWChem GitHub issue tracker .","title":"Where can I get help?"},{"location":"research-software/nwchem/nwchem/#running-nwchem-jobs","text":"The following script will run a NWChem job using 2 nodes (256 cores) in the standard partition. It assumes that the input file is called test\\_calc.nw . Full system #!/bin/bash # Request 2 nodes with 128 MPI tasks per node for 20 minutes #SBATCH --job-name=NWChem_test #SBATCH --nodes=2 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 #SBATCH --time=00:20:00 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Load the NWChem module, avoid any unintentional OpenMP threading by # setting OMP_NUM_THREADS, and launch the code. module load nwchem export OMP_NUM_THREADS=1 srun --distribution=block:block --hint=nomultithread nwchem test_calc","title":"Running NWChem jobs"},{"location":"research-software/nwchem/nwchem/#compiling-nwchem","text":"The latest instructions for building NWChem on ARCHER2 may be found in the GitHub repository of build instructions: Build instructions for NWChem on GitHub","title":"Compiling NWChem"},{"location":"research-software/onetep/onetep/","text":"ONETEP ONETEP (Order-N Electronic Total Energy Package) is a linear-scaling code for quantum-mechanical calculations based on density-functional theory. Useful Links ONETEP home page ONETEP tutorials ONETEP documentation Using ONETEP on ARCHER2 ONETEP is only available to users who have a valid ONETEP licence. If you have a ONETEP licence and wish to have access to ONETEP on ARCHER2, please make a request via the SAFE, see: How to request access to package groups Please have your license details to hand. Running parallel ONETEP jobs The following script will run a ONETEP job using 2 nodes (256 cores). It assumes that the input file is called text_calc.dat . Full system #!/bin/bash # Request 2 nodes with 128 MPI tasks per node for 20 minutes # Replace [budget code] below with your account code, # e.g. '--account=t01' #SBATCH --job-name=ONETEP #SBATCH --nodes=2 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 #SBATCH --time=00:20:00 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Load the ONETEP module module load onetep # Make sure that the stack settings are correct export OMP_STACKSIZE=64M export OMP_NUM_THREADS=1 # Launch the executable srun --distribution=block:block --hint=nomultithread onetep.archer2 test_calc > test_calc.out Hints and Tips See the information in the ONETEP documentation , in particular the information on stack sizes. Compiling ONETEP The latest instructions for building ONETEP on ARCHER2 may be found in the GitHub repository of build instructions: Build instructions for ONETEP on GitHub","title":"ONETEP"},{"location":"research-software/onetep/onetep/#onetep","text":"ONETEP (Order-N Electronic Total Energy Package) is a linear-scaling code for quantum-mechanical calculations based on density-functional theory.","title":"ONETEP"},{"location":"research-software/onetep/onetep/#useful-links","text":"ONETEP home page ONETEP tutorials ONETEP documentation","title":"Useful Links"},{"location":"research-software/onetep/onetep/#using-onetep-on-archer2","text":"ONETEP is only available to users who have a valid ONETEP licence. If you have a ONETEP licence and wish to have access to ONETEP on ARCHER2, please make a request via the SAFE, see: How to request access to package groups Please have your license details to hand.","title":"Using ONETEP on ARCHER2"},{"location":"research-software/onetep/onetep/#running-parallel-onetep-jobs","text":"The following script will run a ONETEP job using 2 nodes (256 cores). It assumes that the input file is called text_calc.dat . Full system #!/bin/bash # Request 2 nodes with 128 MPI tasks per node for 20 minutes # Replace [budget code] below with your account code, # e.g. '--account=t01' #SBATCH --job-name=ONETEP #SBATCH --nodes=2 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 #SBATCH --time=00:20:00 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Load the ONETEP module module load onetep # Make sure that the stack settings are correct export OMP_STACKSIZE=64M export OMP_NUM_THREADS=1 # Launch the executable srun --distribution=block:block --hint=nomultithread onetep.archer2 test_calc > test_calc.out","title":"Running parallel ONETEP jobs"},{"location":"research-software/onetep/onetep/#hints-and-tips","text":"See the information in the ONETEP documentation , in particular the information on stack sizes.","title":"Hints and Tips"},{"location":"research-software/onetep/onetep/#compiling-onetep","text":"The latest instructions for building ONETEP on ARCHER2 may be found in the GitHub repository of build instructions: Build instructions for ONETEP on GitHub","title":"Compiling ONETEP"},{"location":"research-software/openfoam/openfoam/","text":"OpenFOAM OpenFOAM is an open-source toolbox for computational fluid dynamics. OpenFOAM consists of generic tools to simulate complex physics for a variety of fields of interest, from fluid flows involving chemical reactions, turbulence and heat transfer, to solid dynamics, electromagnetism and the pricing of financial options. The core technology of OpenFOAM is a flexible set of modules written in C++. These are used to build solvers and utilities to perform pre-processing and post-processing tasks ranging from simple data manipulation to visualisation and mesh processing. There are a number of different flavours of the OpenFOAM package with slightly different histories, and slightly different features. The two most common are distributed by openfoam.org and openfoam.com. Useful Links OpenFOAM website (.org) OpenFOAM documentation OpenFOAM website (.com) OpenFOAM documentation Using OpenFOAM on ARCHER2 OpenFOAM is released under a GPL v3 license and is freely available to all users on ARCHER2. Full system auser@ln01> module avail openfoam --------------- /work/y07/shared/archer2-lmod/apps/core ----------------- openfoam/com/v2106 openfoam/org/v9.20210903 (D) openfoam/org/v8.20200901 Versions from openfoam.org are typically v8.0 etc and there is typically one release per year (in June; with a patch release in September). Versions from openfoam.com are e.g., v2006 (to be read as 2020 June) and there are typically two releases a year (one in June, and one in December). To use OpenFOAM on ARCHER2 you should first load the OpenFOAM module, e.g. Full system user@ln01:> module load PrgEnv-gnu user@ln01:> module load openfoam/com/v2006 The module defines only the base installation directory via the environment variable FOAM_INSTALL_DIR . After loading the module you need to source the etc/bashrc file provided by OpenFOAM, e.g. user@ln01:> source ${FOAM_INSTALL_DIR}/etc/bashrc You should then be able to use OpenFOAM. The above commands will also need to be added to any job/batch submission scripts you want to use to run OpenFOAM. Note that all the centrally installed versions of OpenFOAM are compiled under PrgEnv-gnu . Note there are no default module versions specificied. It is recommended to use a fully qualified module name (with the exact version, as in the example above). Running parallel OpenFOAM jobs While it is possible to run limited OpenFOAM pre-processing and post-processing activities on the front end, we request all significant work is submitted to the queue system. Please remember that the front end is a shared resource. A typical SLURM job submission script for OpenFOAM is given here. This would request 4 nodes to run with 128 MPI tasks per node (a total of 512 MPI tasks). Each MPI task is allocated one core ( --cpus-per-task=1 ). Full system #!/bin/bash #SBATCH --nodes=4 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 #SBATCH --distribution=block:block #SBATCH --hint=nomultithread #SBATCH --time=00:10:00 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Load the appropriate module and source the OpenFOAM bashrc file module load openfoam/org/v8.20210901 source ${FOAM_INSTALL_DIR}/etc/bashrc # Run OpenFOAM work, e.g., srun interFoam -parallel Compiling OpenFOAM If you want to compile your own version of OpenFOAM, instructions are available for ARCHER2 at: Build instructions for OpenFOAM on GitHub Module version history The following centrally installed versions are available. Full system Module openfoam/com/v2106 installed October 2021 (Cray PE 21.04) Version v2106 (June 2021). See OpenFOAM.com website Module openfoam/org/v9.20200903 installed October 2021 (Cray PE 21.09) Version 9 patch release 3rd September 2021. See OpenFOAM.org website Module openfoam/org/v8.20200901 installed October 2021 (Cray PE 21.09) Version 8 patch release 1st September 2020. See OpenFOAM.org website","title":"OpenFOAM"},{"location":"research-software/openfoam/openfoam/#openfoam","text":"OpenFOAM is an open-source toolbox for computational fluid dynamics. OpenFOAM consists of generic tools to simulate complex physics for a variety of fields of interest, from fluid flows involving chemical reactions, turbulence and heat transfer, to solid dynamics, electromagnetism and the pricing of financial options. The core technology of OpenFOAM is a flexible set of modules written in C++. These are used to build solvers and utilities to perform pre-processing and post-processing tasks ranging from simple data manipulation to visualisation and mesh processing. There are a number of different flavours of the OpenFOAM package with slightly different histories, and slightly different features. The two most common are distributed by openfoam.org and openfoam.com.","title":"OpenFOAM"},{"location":"research-software/openfoam/openfoam/#useful-links","text":"OpenFOAM website (.org) OpenFOAM documentation OpenFOAM website (.com) OpenFOAM documentation","title":"Useful Links"},{"location":"research-software/openfoam/openfoam/#using-openfoam-on-archer2","text":"OpenFOAM is released under a GPL v3 license and is freely available to all users on ARCHER2. Full system auser@ln01> module avail openfoam --------------- /work/y07/shared/archer2-lmod/apps/core ----------------- openfoam/com/v2106 openfoam/org/v9.20210903 (D) openfoam/org/v8.20200901 Versions from openfoam.org are typically v8.0 etc and there is typically one release per year (in June; with a patch release in September). Versions from openfoam.com are e.g., v2006 (to be read as 2020 June) and there are typically two releases a year (one in June, and one in December). To use OpenFOAM on ARCHER2 you should first load the OpenFOAM module, e.g. Full system user@ln01:> module load PrgEnv-gnu user@ln01:> module load openfoam/com/v2006 The module defines only the base installation directory via the environment variable FOAM_INSTALL_DIR . After loading the module you need to source the etc/bashrc file provided by OpenFOAM, e.g. user@ln01:> source ${FOAM_INSTALL_DIR}/etc/bashrc You should then be able to use OpenFOAM. The above commands will also need to be added to any job/batch submission scripts you want to use to run OpenFOAM. Note that all the centrally installed versions of OpenFOAM are compiled under PrgEnv-gnu . Note there are no default module versions specificied. It is recommended to use a fully qualified module name (with the exact version, as in the example above).","title":"Using OpenFOAM on ARCHER2"},{"location":"research-software/openfoam/openfoam/#running-parallel-openfoam-jobs","text":"While it is possible to run limited OpenFOAM pre-processing and post-processing activities on the front end, we request all significant work is submitted to the queue system. Please remember that the front end is a shared resource. A typical SLURM job submission script for OpenFOAM is given here. This would request 4 nodes to run with 128 MPI tasks per node (a total of 512 MPI tasks). Each MPI task is allocated one core ( --cpus-per-task=1 ). Full system #!/bin/bash #SBATCH --nodes=4 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 #SBATCH --distribution=block:block #SBATCH --hint=nomultithread #SBATCH --time=00:10:00 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Load the appropriate module and source the OpenFOAM bashrc file module load openfoam/org/v8.20210901 source ${FOAM_INSTALL_DIR}/etc/bashrc # Run OpenFOAM work, e.g., srun interFoam -parallel","title":"Running parallel OpenFOAM jobs"},{"location":"research-software/openfoam/openfoam/#compiling-openfoam","text":"If you want to compile your own version of OpenFOAM, instructions are available for ARCHER2 at: Build instructions for OpenFOAM on GitHub","title":"Compiling OpenFOAM"},{"location":"research-software/openfoam/openfoam/#module-version-history","text":"The following centrally installed versions are available. Full system Module openfoam/com/v2106 installed October 2021 (Cray PE 21.04) Version v2106 (June 2021). See OpenFOAM.com website Module openfoam/org/v9.20200903 installed October 2021 (Cray PE 21.09) Version 9 patch release 3rd September 2021. See OpenFOAM.org website Module openfoam/org/v8.20200901 installed October 2021 (Cray PE 21.09) Version 8 patch release 1st September 2020. See OpenFOAM.org website","title":"Module version history"},{"location":"research-software/qe/qe/","text":"Quantum Espresso Quantum Espresso (QE) is an integrated suite of open-source computer codes for electronic-structure calculations and materials modeling at the nanoscale. It is based on density-functional theory, plane waves, and pseudopotentials. Useful Links Quantum Espresso home page Quantum Espresso User Guides Quantum Espresso Tutorials Using QE on ARCHER2 QE is released under a GPL v2 license and is freely available to all ARCHER2 users. Running parallel QE jobs For example, the following script will run a QE pw.x job using 4 nodes (128x4 cores). Full system #!/bin/bash # Request 4 nodes to run a 512 MPI task job with 128 MPI tasks per node. # The maximum walltime limit is set to be 20 minutes. #SBATCH --job-name=qe_test #SBATCH --nodes=4 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 #SBATCH --time=00:20:00 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Load the relevant Quantum Espresso module module load quantum_espresso srun pw.x < test_calc.in Hints and tips The QE module is set to load up the default QE-provided pseudo-potentials. If you wish to use non-default pseudo-potentials, you will need to change the ESPRESSO_PSEUDO variable to point to the directory you wish. This can be done by adding the following line after the module is loaded export ESPRESSO_PSEUDO /path/to/pseudo_potentials Compiling QE The latest instructions for building QE on ARCHER2 can be found in the GitHub repository of build instructions: Build instructions for Quantum Espresso","title":"Quantum Espresso"},{"location":"research-software/qe/qe/#quantum-espresso","text":"Quantum Espresso (QE) is an integrated suite of open-source computer codes for electronic-structure calculations and materials modeling at the nanoscale. It is based on density-functional theory, plane waves, and pseudopotentials.","title":"Quantum Espresso"},{"location":"research-software/qe/qe/#useful-links","text":"Quantum Espresso home page Quantum Espresso User Guides Quantum Espresso Tutorials","title":"Useful Links"},{"location":"research-software/qe/qe/#using-qe-on-archer2","text":"QE is released under a GPL v2 license and is freely available to all ARCHER2 users.","title":"Using QE on ARCHER2"},{"location":"research-software/qe/qe/#running-parallel-qe-jobs","text":"For example, the following script will run a QE pw.x job using 4 nodes (128x4 cores). Full system #!/bin/bash # Request 4 nodes to run a 512 MPI task job with 128 MPI tasks per node. # The maximum walltime limit is set to be 20 minutes. #SBATCH --job-name=qe_test #SBATCH --nodes=4 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 #SBATCH --time=00:20:00 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Load the relevant Quantum Espresso module module load quantum_espresso srun pw.x < test_calc.in","title":"Running parallel QE jobs"},{"location":"research-software/qe/qe/#hints-and-tips","text":"The QE module is set to load up the default QE-provided pseudo-potentials. If you wish to use non-default pseudo-potentials, you will need to change the ESPRESSO_PSEUDO variable to point to the directory you wish. This can be done by adding the following line after the module is loaded export ESPRESSO_PSEUDO /path/to/pseudo_potentials","title":"Hints and tips"},{"location":"research-software/qe/qe/#compiling-qe","text":"The latest instructions for building QE on ARCHER2 can be found in the GitHub repository of build instructions: Build instructions for Quantum Espresso","title":"Compiling QE"},{"location":"research-software/vasp/vasp/","text":"VASP The Vienna Ab initio Simulation Package (VASP) is a computer program for atomic scale materials modelling, e.g. electronic structure calculations and quantum-mechanical molecular dynamics, from first principles. VASP computes an approximate solution to the many-body Schr\u00f6dinger equation, either within density functional theory (DFT), solving the Kohn-Sham equations, or within the Hartree-Fock (HF) approximation, solving the Roothaan equations. Hybrid functionals that mix the Hartree-Fock approach with density functional theory are implemented as well. Furthermore, Green's functions methods (GW quasiparticles, and ACFDT-RPA) and many-body perturbation theory (2nd-order M\u00f8ller-Plesset) are available in VASP. In VASP, central quantities, like the one-electron orbitals, the electronic charge density, and the local potential are expressed in plane wave basis sets. The interactions between the electrons and ions are described using norm-conserving or ultrasoft pseudopotentials, or the projector-augmented-wave method. To determine the electronic ground state, VASP makes use of efficient iterative matrix diagonalisation techniques, like the residual minimisation method with direct inversion of the iterative subspace (RMM-DIIS) or blocked Davidson algorithms. These are coupled to highly efficient Broyden and Pulay density mixing schemes to speed up the self-consistency cycle. Useful Links VASP Manual VASP wiki VASP FAQs Using VASP on ARCHER2 VASP is only available to users who have a valid VASP licence. If you have a VASP 5 or 6 licence and wish to have access to VASP on ARCHER2, please make a request via the SAFE, see: How to request access to package groups Please have your license details to hand. Note Both VASP 5 and VASP 6 are available on ARCHER2. You generally need a different licence for each of these versions. Running parallel VASP jobs To access VASP you should load the appropriate vasp module in your job submission scripts. VASP 5 To load the default version of VASP 5, you would use: module load vasp/5 Once loaded, the executables are called: vasp_std - Multiple k-point version vasp_gam - GAMMA-point only version vasp_ncl - Non-collinear version Once the module has been loaded, you can access the LDA and PBE pseudopotentials for VASP on ARCHER2 at: $VASP_PSPOT_DIR VASP Transition State Tools (VTST) As well as the standard VASP 5 modules, we provide versions of VASP 5 with the VASP Transition State Tools (VTST) from the University of Texas added. The VTST version adds various functionality to VASP and provides additional scripts to use with VASP. Additional functionality includes: Climbing Image NEB: method for finding reaction pathways between two stable states. Dimer: method for finding reaction pathways when only one state is known. Lanczos: provides an alternative way to find the lowest mode and find saddle points. Optimisers: provides an alternative way to find the lowest mode and find saddle points. Dynamical Matrix: uses finite difference to find normal modes and reaction prefactors. Full details of these methods and the provided scripts can be found on the VTST website . On ARCHER2, the VTST version of VASP 5 can be accessed by loading the modules with VTST in the module name, for example: Full system module load vasp/5/5.4.4.pl2-vtst Example VASP 5 job submission script The following script will run a VASP job using 2 nodes (128x2, 256 total cores). Full system #!/bin/bash # Request 16 nodes (2048 MPI tasks at 128 tasks per node) for 20 minutes. #SBATCH --job-name=VASP_test #SBATCH --nodes=16 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 #SBATCH --time=00:20:00 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Load the VASP module module load vasp/5 # Avoid any unintentional OpenMP threading by setting OMP_NUM_THREADS export OMP_NUM_THREADS=1 # Launch the code. srun --distribution=block:block --hint=nomultithread vasp_std VASP 6 To load the default version of VASP 6, you would use: module load vasp/6 Once loaded, the executables are called: vasp_std - Multiple k-point version vasp_gam - GAMMA-point only version vasp_ncl - Non-collinear version Once the module has been loaded, you can access the LDA and PBE pseudopotentials for VASP on ARCHER2 at: $VASP_PSPOT_DIR The following script will run a VASP job using 2 nodes (128x2, 256 total cores) using only MPI ranks and no OpenMP threading. Tip VASP 6 can make use of OpenMP threads in addition to running with pure MPI. We will add notes on performance and use of threading in VASP as information becomes available. Full system #!/bin/bash # Request 16 nodes (2048 MPI tasks at 128 tasks per node) for 20 minutes. #SBATCH --job-name=VASP_test #SBATCH --nodes=16 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 #SBATCH --time=00:20:00 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Load the VASP module module load vasp/6 # Avoid any unintentional OpenMP threading by setting OMP_NUM_THREADS export OMP_NUM_THREADS=1 # Launch the code. srun --distribution=block:block --hint=nomultithread vasp_std Compiling VASP on ARCHER2 If you wish to compile your own version of VASP on ARCHER2 (either VASP 5 or VASP 6) you can find information on how we compiled the central versions in the build instructions GitHub repository. See: Build instructions for VASP on GitHub Tips for using VASP on ARCHER2 Switching MPI transport protocol from UCX to OpenFabrics The VASP modules are setup to use the UCX MPI transport protocol as testing has shown that this passes all the regression tests and gives the best performance on ARCHER2. However, there may be cases where using UCX can give errors that can be fixed by switching to the OpenFabrics MPI transport protocol. If you see errors in VASP calculations that implicate UCX (they will typically be MPI errors with the string ucx or UCX in the output) then you can try using OpenFabrics instead by loading additional modules after you have loaded the VASP modules. For example, for VASP 6, you would use: module load vasp/6 module load craype-network-ofi module load cray-mpich Performance tips The performance of VASP depends on the version of VASP used, the performance of MPI collective operations, the choice of VASP parallelisation parameters ( NCORE / NPAR and KPAR ) and how many MPI processes per node are used. VASP version For the benchmarks studied, VASP 6.3.0 usually gives better performance than the older VASP 5.4.4.pl2. The exception is for when using higher node counts (above 8 nodes) for the CdTe benchmark (which uses vasp_ncl ). Users should generally use VASP 6.3.0 but it may be worth evaluating the performance fo VASP 5.4.4.pl2 for your case if you are running larger calculations, particularly when using the non-collinear version of VASP. MPI collective performance: To ensure that the MPI collective operations give the best performance, you should ensure that consecutive MPI rank IDs are pinned to consecutive cores on a node to maximise shared memory optimisations in NUMA regions. In practice, the recommended options to the srun command: --hint=nomultithread and --distribution=block:block should always be specified when running VASP on ARCHER2. You should also make sure that you use the UCX transport layer for MPI rather than the default OpenFabrics transport layer. The VASP modules are setup to enable this but if you are using your own compiled version of VASP you should add the following lines to your job submission script before you run VASP (assuming that you compiled VASP using GCC): module load PrgEnv-gnu module load craype-network-ucx module load cray-mpich-ucx export UCX_IB_REG_METHODS=direct KPAR: You should always use the maximum value of KPAR that is possible for your calculation within the memory limits of what is possible. NCORE/NPAR: We have found that the optimal values of NCORE (and hence NPAR ) depend on both the type of calculation you are performing (e.g. pure DFT, hybrid functional, \u0393-point, non-collinear) and the number of nodes/cores you are using for your calculation. In practice, this means that you should experiment with different values to find the best choice for your calculation. There is information below on the best choices for the benchmarks we have run on ARCHER2 that may serve as a useful starting point. The performance difference from choosing different values can vary by up to 100% so it is worth spending time investigating this. MPI processes per node We found that it is sometimes beneficial to performance to use less MPI processes per node than the total number of cores per node in some cases for the benchmarks used. We found that for the large TiO2 \u0393-point calculation it was best to use just 64 MPI processes per node (leaving half of the cores idle). For the CdTe non-collinear, multiple k-point benchmark, best performance was achieved when all 128 cores on the node had an MPI process (128 MPI processes per node). OpenMP threads The use of OpenMP threads did not improve performance or scaling for either of the benchmarks used. This was true even for the TiO2 benchmark case where we used only 64 MPI processes per node, the performance was better with 64 idle cores on a node rather than using the spare core for OpenMP threads. This seems to be because when OpenMP threading is used, NCORE is fixed at a value of 1, which gives poor performance. VASP performance data on ARCHER2 VASP performance data on ARCHER2 is currently available for two different benchmark systems: TiO_2 Supercell, pure DFT functional, \u0393-point, 1080 atoms CdTe Supercell, hybrid DFT functional. 8 k-points, 65 atoms TiO_2 Supercell, pure DFT functional, Gamma-point, 1080 atoms Basic information: Uses vasp_gam NELM = 10 Full TiO2 performance data Performance summary for best choices of MPI processes per node and NCORE at different node counts. Performance reported as timing for LOOP+ in seconds. Performance summary: Best performance from VASP 6.3.0 Best performance from 64 MPI processes per node - leaves 64 cores idle on each node Best performance with NCORE = 64 Scales well to 16 nodes Using OpenMP threads results in worse performance Full system, VASP 6.3.0 vasp/6/6.3.0 module GCC 11.2.0 AOCL 3.1 for BLAS/LAPACK/ScaLAPACK and FFTW UCX for MPI transport layer Nodes MPI processes per node Total MPI processes NCORE 6.3.0 (full system) 1 64 128 64 3295 2 64 256 64 1548 4 64 512 64 814 8 64 512 64 416 16 64 1024 64 221 32 64 2048 64 131 64 64 4096 64 82 Full system, 5.4.4.pl2 vasp/5/5.4.4.pl2 module GCC 11.2.0 HPE Cray LibSci 21.09 for BLAS/LAPACK/ScaLAPACK and FFTW 3.3.8.11 UCX for MPI transport layer Nodes MPI processes per node Total MPI processes NCORE 5.4.4.pl2 (full system) 1 64 64 64 3428 2 64 128 64 1615 4 64 256 64 823 8 64 512 64 429 16 64 1024 64 231 32 64 2048 64 135 64 64 4096 64 79 CdTe Supercell, hybrid DFT functional. 8 k-points, 65 atoms Basic information: Uses vasp_ncl NELM = 6 CdTe performance data Performance summary: VASP version: Up to 8 nodes: best performance from VASP 6.3.0 16 nodes or more: best performance from VASP 5.4.4.pl2 Cores per node: Best performance usually from 128 MPI processes per node - all cores occupied At 64 nodes, best performance from 64 MPI processes per node - 64 core idle NCORE : Up to 8 nodes: best performance with NCORE = 4 (VASP 6.3.0) 16 nodes or more: best performance with NCORE = 16 (VASP 5.4.4.pl2) KPAR = 2 is maximum that can be used on standard memory nodes Scales well to 64 nodes Using OpenMP threads results in worse performance Full system, VASP 6.3.0 vasp/6/6.3.0 module GCC 11.2.0 AOCL 3.1 for BLAS/LAPACK/ScaLAPACK and FFTW UCX for MPI transport layer Nodes MPI processes per node Total MPI processes NCORE KPAR 5.4.4.pl2 (4-cab system) 1 128 128 4 2 19000 2 128 256 4 2 10021 4 128 512 4 2 5560 8 128 1024 4 2 3176 16 128 2048 8 2 2413 32 64 2048 16 2 1340 64 64 4096 16 2 908 Full system, 5.4.4.pl2 vasp/5/5.4.4.pl2 module GCC 11.2.0 HPE Cray LibSci 21.09 for BLAS/LAPACK/ScaLAPACK and FFTW 3.3.8 UCX for MPI transport layer Nodes MPI processes per node Total MPI processes NCORE KPAR 5.4.4.pl2 (4-cab system) 1 128 128 4 2 23417 2 128 256 4 2 12338 4 128 512 4 2 6751 8 128 1024 4 2 3676 16 64 1024 16 2 2136 32 64 2048 16 2 1266 64 64 4096 16 2 806","title":"VASP"},{"location":"research-software/vasp/vasp/#vasp","text":"The Vienna Ab initio Simulation Package (VASP) is a computer program for atomic scale materials modelling, e.g. electronic structure calculations and quantum-mechanical molecular dynamics, from first principles. VASP computes an approximate solution to the many-body Schr\u00f6dinger equation, either within density functional theory (DFT), solving the Kohn-Sham equations, or within the Hartree-Fock (HF) approximation, solving the Roothaan equations. Hybrid functionals that mix the Hartree-Fock approach with density functional theory are implemented as well. Furthermore, Green's functions methods (GW quasiparticles, and ACFDT-RPA) and many-body perturbation theory (2nd-order M\u00f8ller-Plesset) are available in VASP. In VASP, central quantities, like the one-electron orbitals, the electronic charge density, and the local potential are expressed in plane wave basis sets. The interactions between the electrons and ions are described using norm-conserving or ultrasoft pseudopotentials, or the projector-augmented-wave method. To determine the electronic ground state, VASP makes use of efficient iterative matrix diagonalisation techniques, like the residual minimisation method with direct inversion of the iterative subspace (RMM-DIIS) or blocked Davidson algorithms. These are coupled to highly efficient Broyden and Pulay density mixing schemes to speed up the self-consistency cycle.","title":"VASP"},{"location":"research-software/vasp/vasp/#useful-links","text":"VASP Manual VASP wiki VASP FAQs","title":"Useful Links"},{"location":"research-software/vasp/vasp/#using-vasp-on-archer2","text":"VASP is only available to users who have a valid VASP licence. If you have a VASP 5 or 6 licence and wish to have access to VASP on ARCHER2, please make a request via the SAFE, see: How to request access to package groups Please have your license details to hand. Note Both VASP 5 and VASP 6 are available on ARCHER2. You generally need a different licence for each of these versions.","title":"Using VASP on ARCHER2"},{"location":"research-software/vasp/vasp/#running-parallel-vasp-jobs","text":"To access VASP you should load the appropriate vasp module in your job submission scripts.","title":"Running parallel VASP jobs"},{"location":"research-software/vasp/vasp/#vasp-5","text":"To load the default version of VASP 5, you would use: module load vasp/5 Once loaded, the executables are called: vasp_std - Multiple k-point version vasp_gam - GAMMA-point only version vasp_ncl - Non-collinear version Once the module has been loaded, you can access the LDA and PBE pseudopotentials for VASP on ARCHER2 at: $VASP_PSPOT_DIR","title":"VASP 5"},{"location":"research-software/vasp/vasp/#vasp-transition-state-tools-vtst","text":"As well as the standard VASP 5 modules, we provide versions of VASP 5 with the VASP Transition State Tools (VTST) from the University of Texas added. The VTST version adds various functionality to VASP and provides additional scripts to use with VASP. Additional functionality includes: Climbing Image NEB: method for finding reaction pathways between two stable states. Dimer: method for finding reaction pathways when only one state is known. Lanczos: provides an alternative way to find the lowest mode and find saddle points. Optimisers: provides an alternative way to find the lowest mode and find saddle points. Dynamical Matrix: uses finite difference to find normal modes and reaction prefactors. Full details of these methods and the provided scripts can be found on the VTST website . On ARCHER2, the VTST version of VASP 5 can be accessed by loading the modules with VTST in the module name, for example: Full system module load vasp/5/5.4.4.pl2-vtst","title":"VASP Transition State Tools (VTST)"},{"location":"research-software/vasp/vasp/#example-vasp-5-job-submission-script","text":"The following script will run a VASP job using 2 nodes (128x2, 256 total cores). Full system #!/bin/bash # Request 16 nodes (2048 MPI tasks at 128 tasks per node) for 20 minutes. #SBATCH --job-name=VASP_test #SBATCH --nodes=16 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 #SBATCH --time=00:20:00 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Load the VASP module module load vasp/5 # Avoid any unintentional OpenMP threading by setting OMP_NUM_THREADS export OMP_NUM_THREADS=1 # Launch the code. srun --distribution=block:block --hint=nomultithread vasp_std","title":"Example VASP 5 job submission script"},{"location":"research-software/vasp/vasp/#vasp-6","text":"To load the default version of VASP 6, you would use: module load vasp/6 Once loaded, the executables are called: vasp_std - Multiple k-point version vasp_gam - GAMMA-point only version vasp_ncl - Non-collinear version Once the module has been loaded, you can access the LDA and PBE pseudopotentials for VASP on ARCHER2 at: $VASP_PSPOT_DIR The following script will run a VASP job using 2 nodes (128x2, 256 total cores) using only MPI ranks and no OpenMP threading. Tip VASP 6 can make use of OpenMP threads in addition to running with pure MPI. We will add notes on performance and use of threading in VASP as information becomes available. Full system #!/bin/bash # Request 16 nodes (2048 MPI tasks at 128 tasks per node) for 20 minutes. #SBATCH --job-name=VASP_test #SBATCH --nodes=16 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 #SBATCH --time=00:20:00 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Load the VASP module module load vasp/6 # Avoid any unintentional OpenMP threading by setting OMP_NUM_THREADS export OMP_NUM_THREADS=1 # Launch the code. srun --distribution=block:block --hint=nomultithread vasp_std","title":"VASP 6"},{"location":"research-software/vasp/vasp/#compiling-vasp-on-archer2","text":"If you wish to compile your own version of VASP on ARCHER2 (either VASP 5 or VASP 6) you can find information on how we compiled the central versions in the build instructions GitHub repository. See: Build instructions for VASP on GitHub","title":"Compiling VASP on ARCHER2"},{"location":"research-software/vasp/vasp/#tips-for-using-vasp-on-archer2","text":"","title":"Tips for using VASP on ARCHER2"},{"location":"research-software/vasp/vasp/#switching-mpi-transport-protocol-from-ucx-to-openfabrics","text":"The VASP modules are setup to use the UCX MPI transport protocol as testing has shown that this passes all the regression tests and gives the best performance on ARCHER2. However, there may be cases where using UCX can give errors that can be fixed by switching to the OpenFabrics MPI transport protocol. If you see errors in VASP calculations that implicate UCX (they will typically be MPI errors with the string ucx or UCX in the output) then you can try using OpenFabrics instead by loading additional modules after you have loaded the VASP modules. For example, for VASP 6, you would use: module load vasp/6 module load craype-network-ofi module load cray-mpich","title":"Switching MPI transport protocol from UCX to OpenFabrics"},{"location":"research-software/vasp/vasp/#performance-tips","text":"The performance of VASP depends on the version of VASP used, the performance of MPI collective operations, the choice of VASP parallelisation parameters ( NCORE / NPAR and KPAR ) and how many MPI processes per node are used. VASP version For the benchmarks studied, VASP 6.3.0 usually gives better performance than the older VASP 5.4.4.pl2. The exception is for when using higher node counts (above 8 nodes) for the CdTe benchmark (which uses vasp_ncl ). Users should generally use VASP 6.3.0 but it may be worth evaluating the performance fo VASP 5.4.4.pl2 for your case if you are running larger calculations, particularly when using the non-collinear version of VASP. MPI collective performance: To ensure that the MPI collective operations give the best performance, you should ensure that consecutive MPI rank IDs are pinned to consecutive cores on a node to maximise shared memory optimisations in NUMA regions. In practice, the recommended options to the srun command: --hint=nomultithread and --distribution=block:block should always be specified when running VASP on ARCHER2. You should also make sure that you use the UCX transport layer for MPI rather than the default OpenFabrics transport layer. The VASP modules are setup to enable this but if you are using your own compiled version of VASP you should add the following lines to your job submission script before you run VASP (assuming that you compiled VASP using GCC): module load PrgEnv-gnu module load craype-network-ucx module load cray-mpich-ucx export UCX_IB_REG_METHODS=direct KPAR: You should always use the maximum value of KPAR that is possible for your calculation within the memory limits of what is possible. NCORE/NPAR: We have found that the optimal values of NCORE (and hence NPAR ) depend on both the type of calculation you are performing (e.g. pure DFT, hybrid functional, \u0393-point, non-collinear) and the number of nodes/cores you are using for your calculation. In practice, this means that you should experiment with different values to find the best choice for your calculation. There is information below on the best choices for the benchmarks we have run on ARCHER2 that may serve as a useful starting point. The performance difference from choosing different values can vary by up to 100% so it is worth spending time investigating this. MPI processes per node We found that it is sometimes beneficial to performance to use less MPI processes per node than the total number of cores per node in some cases for the benchmarks used. We found that for the large TiO2 \u0393-point calculation it was best to use just 64 MPI processes per node (leaving half of the cores idle). For the CdTe non-collinear, multiple k-point benchmark, best performance was achieved when all 128 cores on the node had an MPI process (128 MPI processes per node). OpenMP threads The use of OpenMP threads did not improve performance or scaling for either of the benchmarks used. This was true even for the TiO2 benchmark case where we used only 64 MPI processes per node, the performance was better with 64 idle cores on a node rather than using the spare core for OpenMP threads. This seems to be because when OpenMP threading is used, NCORE is fixed at a value of 1, which gives poor performance.","title":"Performance tips"},{"location":"research-software/vasp/vasp/#vasp-performance-data-on-archer2","text":"VASP performance data on ARCHER2 is currently available for two different benchmark systems: TiO_2 Supercell, pure DFT functional, \u0393-point, 1080 atoms CdTe Supercell, hybrid DFT functional. 8 k-points, 65 atoms","title":"VASP performance data on ARCHER2"},{"location":"research-software/vasp/vasp/#tio_2-supercell-pure-dft-functional-gamma-point-1080-atoms","text":"Basic information: Uses vasp_gam NELM = 10 Full TiO2 performance data Performance summary for best choices of MPI processes per node and NCORE at different node counts. Performance reported as timing for LOOP+ in seconds. Performance summary: Best performance from VASP 6.3.0 Best performance from 64 MPI processes per node - leaves 64 cores idle on each node Best performance with NCORE = 64 Scales well to 16 nodes Using OpenMP threads results in worse performance","title":"TiO_2 Supercell, pure DFT functional, Gamma-point, 1080 atoms"},{"location":"research-software/vasp/vasp/#full-system-vasp-630","text":"vasp/6/6.3.0 module GCC 11.2.0 AOCL 3.1 for BLAS/LAPACK/ScaLAPACK and FFTW UCX for MPI transport layer Nodes MPI processes per node Total MPI processes NCORE 6.3.0 (full system) 1 64 128 64 3295 2 64 256 64 1548 4 64 512 64 814 8 64 512 64 416 16 64 1024 64 221 32 64 2048 64 131 64 64 4096 64 82","title":"Full system, VASP 6.3.0"},{"location":"research-software/vasp/vasp/#full-system-544pl2","text":"vasp/5/5.4.4.pl2 module GCC 11.2.0 HPE Cray LibSci 21.09 for BLAS/LAPACK/ScaLAPACK and FFTW 3.3.8.11 UCX for MPI transport layer Nodes MPI processes per node Total MPI processes NCORE 5.4.4.pl2 (full system) 1 64 64 64 3428 2 64 128 64 1615 4 64 256 64 823 8 64 512 64 429 16 64 1024 64 231 32 64 2048 64 135 64 64 4096 64 79","title":"Full system, 5.4.4.pl2"},{"location":"research-software/vasp/vasp/#cdte-supercell-hybrid-dft-functional-8-k-points-65-atoms","text":"Basic information: Uses vasp_ncl NELM = 6 CdTe performance data Performance summary: VASP version: Up to 8 nodes: best performance from VASP 6.3.0 16 nodes or more: best performance from VASP 5.4.4.pl2 Cores per node: Best performance usually from 128 MPI processes per node - all cores occupied At 64 nodes, best performance from 64 MPI processes per node - 64 core idle NCORE : Up to 8 nodes: best performance with NCORE = 4 (VASP 6.3.0) 16 nodes or more: best performance with NCORE = 16 (VASP 5.4.4.pl2) KPAR = 2 is maximum that can be used on standard memory nodes Scales well to 64 nodes Using OpenMP threads results in worse performance","title":"CdTe Supercell, hybrid DFT functional. 8 k-points, 65 atoms"},{"location":"research-software/vasp/vasp/#full-system-vasp-630_1","text":"vasp/6/6.3.0 module GCC 11.2.0 AOCL 3.1 for BLAS/LAPACK/ScaLAPACK and FFTW UCX for MPI transport layer Nodes MPI processes per node Total MPI processes NCORE KPAR 5.4.4.pl2 (4-cab system) 1 128 128 4 2 19000 2 128 256 4 2 10021 4 128 512 4 2 5560 8 128 1024 4 2 3176 16 128 2048 8 2 2413 32 64 2048 16 2 1340 64 64 4096 16 2 908","title":"Full system, VASP 6.3.0"},{"location":"research-software/vasp/vasp/#full-system-544pl2_1","text":"vasp/5/5.4.4.pl2 module GCC 11.2.0 HPE Cray LibSci 21.09 for BLAS/LAPACK/ScaLAPACK and FFTW 3.3.8 UCX for MPI transport layer Nodes MPI processes per node Total MPI processes NCORE KPAR 5.4.4.pl2 (4-cab system) 1 128 128 4 2 23417 2 128 256 4 2 12338 4 128 512 4 2 6751 8 128 1024 4 2 3676 16 64 1024 16 2 2136 32 64 2048 16 2 1266 64 64 4096 16 2 806","title":"Full system, 5.4.4.pl2"},{"location":"software-libraries/","text":"Software Libraries This section provides information on centrally-installed software libraries and library-based packages. These provide significant functionality that is of interest to both users and developers of applications. Libraries are made available via the module system, and fall into a number of distinct groups. Libraries via modules cray-* The following libraries are available as modules prefixed by cray- and may be of direct interest to developers and users. The modules are provided by HPE Cray to be optimised for performance on the ARCHER2 hardware, and should be used where possible. The relevant modules are: cray-fftw ...details for module load cray-fttw... FFTW (Fastest Fourier Transform in the West) is a standard package for discrete Fourier transforms. See the FFTW home page cray-hdf5 and cray-hdf5-parallel ...details for hdf5... Hierarchical Data Format (HDF5) is a high-performance and portable data format and data model. These modules provide serial and parallel variants of HDF5. See the HDF5 home page cray-libsci ...details for cray-libsci... BLAS, LAPACK, BLACS, and SCALAPACK provide basic linear algebra functionality such as vector-vector, matrix-vector, and matrix-matrix multiplication. Module cray-libsci is loaded by default in all programming environments. cray-netcdf ...details for cray-netcdf... Serial version of Network Common Data Form (NetCDF), a widely used and portable data format. See the NETCDF website cray-netcdf-hdf5parallel A serial NetCDF built against parallel HDF5. Load module cray-hdf5-parallel first. cray-parallel-netcdf ...deatils for Parallel NetCDF... A parallel NetCDF implementation (sometimes referred to as \"Pnetcdf\"). Integration with compiler environment All libraries provided by modules prefixed cray- integrate with the compiler environment, and so appropriate compiler and link stage options are injected when using the standard compiler wrappers cc , CC and ftn . Libraries supported by ARCHER2 CSE team The following libraries will also made available by the ARCHER2 CSE team: ADIOS ...details for ADIOS on ARCHER2... ADIOS (Adaptable I/O System) provides library services for parallel I/O. AOCL ...details for AOCL on ARCHER2... AOCL (AMD Optimizing CPU Libraries) provides a set of numerical libraries optimised for AMD \"Zen\"-based processors. ARPACK-NG ...details for ARPACK-NG on ARCHER2... ARPACK-NG (Arnodli Package) computes eigenvalues and eigenvectors of large sparse matrics. Boost ...details for Boost on ARCHER2... Boost is a portable C++ library providing reference implementations of many common containers, operations and algorithms. Eigen ...details for Eigen on ARCHER2... Eigen is a C++ template library for linear algebra: matrices, vectors, numerical solvers, and related algorithms. GLM ...details for GLM on ARCHER2... GLM (GL Math library) is a C++ header-only library for performing operations commonly encountered in graphics applications. Hypre ...details for HYPRE on ARCHER2... HYPRE provides pre-conditioners and solvers for sparse linear algebra problems. Metis and Parmetis ...details for Metis and Parmetis... METIS is a set of (serial) routines for partitioning graphs and meshes, and computing reduced-fill orderings of sparse matrices. It is commonly used e.g., to compute decompositions for finite element problems. Parmetis is the distributed memory counterpart. Mumps ...details for MUMPS on ARCHER2... MUMPS provides parallel direct solution of large sparse matrix problems. PETSc ...details for PETSc on ARCHER2... PETSc is a general package with functionality related to the solution of a wide range of problems described by partial differential equations. Scotch ...details for Scotch and PT-Scotch on ARCHER2... Scotch (and its parallel partner PT-Scotch) is a graph partitioning library. SLEPc ...details for SLEPc on ARCHER2... SLEPc is a package for large eigenvalue problems based on PETSc. SUNDIALS ...details pending... Sundials is a suite of libraries which address problems including ordinary differential equation integration, initial value problems, and non-linear algebraic equations. See SuperLU and SuperLU_DIST ...details for SuperLU on ARCHER2... SuperLU provides solutions to large non-symmetric sparse systems. SuperLU_DIST is the distributed memory version. Trilinos ...details for Trilinos on ARCHER2... Trilinos is a large collection of packages for the solution of complex scientific and engineering problems. Integration with compiler environment Again, all the libraries listed above are supported by all programming environments via the module system. Additional compile and link time flags should not be required. Building your own library versions For the libraries listed in this section, a set of build and installation scripts are available at the ARCHER2 Github repository . Follow the instructions to build the relevant package (note this is the cse-develop branch of the repository). See also individual libraries pages in the list above for further details. The scripts available from this repository should work in all three programming environments.","title":"Overview"},{"location":"software-libraries/#software-libraries","text":"This section provides information on centrally-installed software libraries and library-based packages. These provide significant functionality that is of interest to both users and developers of applications. Libraries are made available via the module system, and fall into a number of distinct groups.","title":"Software Libraries"},{"location":"software-libraries/#libraries-via-modules-cray-","text":"The following libraries are available as modules prefixed by cray- and may be of direct interest to developers and users. The modules are provided by HPE Cray to be optimised for performance on the ARCHER2 hardware, and should be used where possible. The relevant modules are: cray-fftw ...details for module load cray-fttw... FFTW (Fastest Fourier Transform in the West) is a standard package for discrete Fourier transforms. See the FFTW home page cray-hdf5 and cray-hdf5-parallel ...details for hdf5... Hierarchical Data Format (HDF5) is a high-performance and portable data format and data model. These modules provide serial and parallel variants of HDF5. See the HDF5 home page cray-libsci ...details for cray-libsci... BLAS, LAPACK, BLACS, and SCALAPACK provide basic linear algebra functionality such as vector-vector, matrix-vector, and matrix-matrix multiplication. Module cray-libsci is loaded by default in all programming environments. cray-netcdf ...details for cray-netcdf... Serial version of Network Common Data Form (NetCDF), a widely used and portable data format. See the NETCDF website cray-netcdf-hdf5parallel A serial NetCDF built against parallel HDF5. Load module cray-hdf5-parallel first. cray-parallel-netcdf ...deatils for Parallel NetCDF... A parallel NetCDF implementation (sometimes referred to as \"Pnetcdf\").","title":"Libraries via modules cray-*"},{"location":"software-libraries/#integration-with-compiler-environment","text":"All libraries provided by modules prefixed cray- integrate with the compiler environment, and so appropriate compiler and link stage options are injected when using the standard compiler wrappers cc , CC and ftn .","title":"Integration with compiler environment"},{"location":"software-libraries/#libraries-supported-by-archer2-cse-team","text":"The following libraries will also made available by the ARCHER2 CSE team: ADIOS ...details for ADIOS on ARCHER2... ADIOS (Adaptable I/O System) provides library services for parallel I/O. AOCL ...details for AOCL on ARCHER2... AOCL (AMD Optimizing CPU Libraries) provides a set of numerical libraries optimised for AMD \"Zen\"-based processors. ARPACK-NG ...details for ARPACK-NG on ARCHER2... ARPACK-NG (Arnodli Package) computes eigenvalues and eigenvectors of large sparse matrics. Boost ...details for Boost on ARCHER2... Boost is a portable C++ library providing reference implementations of many common containers, operations and algorithms. Eigen ...details for Eigen on ARCHER2... Eigen is a C++ template library for linear algebra: matrices, vectors, numerical solvers, and related algorithms. GLM ...details for GLM on ARCHER2... GLM (GL Math library) is a C++ header-only library for performing operations commonly encountered in graphics applications. Hypre ...details for HYPRE on ARCHER2... HYPRE provides pre-conditioners and solvers for sparse linear algebra problems. Metis and Parmetis ...details for Metis and Parmetis... METIS is a set of (serial) routines for partitioning graphs and meshes, and computing reduced-fill orderings of sparse matrices. It is commonly used e.g., to compute decompositions for finite element problems. Parmetis is the distributed memory counterpart. Mumps ...details for MUMPS on ARCHER2... MUMPS provides parallel direct solution of large sparse matrix problems. PETSc ...details for PETSc on ARCHER2... PETSc is a general package with functionality related to the solution of a wide range of problems described by partial differential equations. Scotch ...details for Scotch and PT-Scotch on ARCHER2... Scotch (and its parallel partner PT-Scotch) is a graph partitioning library. SLEPc ...details for SLEPc on ARCHER2... SLEPc is a package for large eigenvalue problems based on PETSc. SUNDIALS ...details pending... Sundials is a suite of libraries which address problems including ordinary differential equation integration, initial value problems, and non-linear algebraic equations. See SuperLU and SuperLU_DIST ...details for SuperLU on ARCHER2... SuperLU provides solutions to large non-symmetric sparse systems. SuperLU_DIST is the distributed memory version. Trilinos ...details for Trilinos on ARCHER2... Trilinos is a large collection of packages for the solution of complex scientific and engineering problems.","title":"Libraries supported by ARCHER2 CSE team"},{"location":"software-libraries/#integration-with-compiler-environment_1","text":"Again, all the libraries listed above are supported by all programming environments via the module system. Additional compile and link time flags should not be required.","title":"Integration with compiler environment"},{"location":"software-libraries/#building-your-own-library-versions","text":"For the libraries listed in this section, a set of build and installation scripts are available at the ARCHER2 Github repository . Follow the instructions to build the relevant package (note this is the cse-develop branch of the repository). See also individual libraries pages in the list above for further details. The scripts available from this repository should work in all three programming environments.","title":"Building your own library versions"},{"location":"software-libraries/adios/","text":"ADIOS The Adaptable I/O System (ADIOS) is developed at Oak Ridge National Laboratory and is freely available under a BSD license. Compiling and linking against ADIOS module load adios Adios is available in both serial and parallel versions. Configuration details for ADIOS are obtained via the utility adios_config which is available in the PATH once the adios module is loaded. For example, to recover the compiler options required to provide serial C include files, issue: $ adios_config -s -c Use adios_config --help for a summary of options. To compile and link applciation, such statements can be embedded in a Makefile via, e.g., ADIOS_INC := $(shell adios_config -s -c) ADIOS_CLIB := $(shell adios_config -s -l) See the ADIOS user manual for further details and examples. The adios module defines the environment variable ADIOS_DIR which will be appropriate for the current programming environment when the adios module is loaded. Version history Full system 4-cabinet system Module adios/1.13.1 installed October 2021 (PE 21.04) Module adios/1.13.1 installed January 2021 Compile your own version The Archer2 github repository provides a script which can be used to build ADIOS as for the currently supported version, e.g.,: $ git clone https://github.com/ARCHER2-HPC/pe-scripts.git $ cd pe-scripts $ git checkout modules-2021-10 $ module load cray-hdf5-parallel $ ./sh/adios.sh --prefix=/path/to/install/location where the --prefix option determines the install location. See the Archer2 github repository for further details and options. Resources The ADIOS home page ADIOS user manual (v1.10 pdf version) ADIOS 1.x github repository","title":"ADIOS"},{"location":"software-libraries/adios/#adios","text":"The Adaptable I/O System (ADIOS) is developed at Oak Ridge National Laboratory and is freely available under a BSD license.","title":"ADIOS"},{"location":"software-libraries/adios/#compiling-and-linking-against-adios","text":"module load adios Adios is available in both serial and parallel versions. Configuration details for ADIOS are obtained via the utility adios_config which is available in the PATH once the adios module is loaded. For example, to recover the compiler options required to provide serial C include files, issue: $ adios_config -s -c Use adios_config --help for a summary of options. To compile and link applciation, such statements can be embedded in a Makefile via, e.g., ADIOS_INC := $(shell adios_config -s -c) ADIOS_CLIB := $(shell adios_config -s -l) See the ADIOS user manual for further details and examples. The adios module defines the environment variable ADIOS_DIR which will be appropriate for the current programming environment when the adios module is loaded.","title":"Compiling and linking against ADIOS"},{"location":"software-libraries/adios/#version-history","text":"Full system 4-cabinet system Module adios/1.13.1 installed October 2021 (PE 21.04) Module adios/1.13.1 installed January 2021","title":"Version history"},{"location":"software-libraries/adios/#compile-your-own-version","text":"The Archer2 github repository provides a script which can be used to build ADIOS as for the currently supported version, e.g.,: $ git clone https://github.com/ARCHER2-HPC/pe-scripts.git $ cd pe-scripts $ git checkout modules-2021-10 $ module load cray-hdf5-parallel $ ./sh/adios.sh --prefix=/path/to/install/location where the --prefix option determines the install location. See the Archer2 github repository for further details and options.","title":"Compile your own version"},{"location":"software-libraries/adios/#resources","text":"The ADIOS home page ADIOS user manual (v1.10 pdf version) ADIOS 1.x github repository","title":"Resources"},{"location":"software-libraries/aocl/","text":"AMD Optimizing CPU Libraries (AOCL) AMD Optimizing CPU Libraries (AOCL) are a set of numerical libraries optimized for AMD \u201cZen\u201d-based processors, including EPYC, Ryzen Threadripper PRO, and Ryzen. AOCL is comprised of the following eight libraries: - BLIS (BLAS Library) - libFLAME (LAPACK) - AMD-FFTW - LibM (AMD Core Math Library) - ScaLAPACK - AMD Random Number Generator (RNG) - AMD Secure RNG - AOCL-Sparse Important The cray-libsci module is loaded by default for all users and this module also contains definitions of BLAS, LAPACK and ScaLAPACK routines that conflict with those in AOCL. The aocl module automatically unloads cray-libsci . Compiling with AOCL Tip AOCL is currently unavailable for the Cray programming environments ( PrgEnv-cray ). This is untested and currently unsupported on ARCHER2. GNU Programming Environment module load PrgEnv-gnu module load aocl AOCC Programming Environment AOCL is only available with aocc/3.0.0. module load PrgEnv-aocc module swap aocc/2.2.0.1 aocc/3.0.0 module load aocl Resources For more information on AOCL, please see: https://developer.amd.com/amd-aocl/#documentation Version history.. Module aocl/3.1 installed April 2022","title":"AOCL"},{"location":"software-libraries/aocl/#amd-optimizing-cpu-libraries-aocl","text":"AMD Optimizing CPU Libraries (AOCL) are a set of numerical libraries optimized for AMD \u201cZen\u201d-based processors, including EPYC, Ryzen Threadripper PRO, and Ryzen. AOCL is comprised of the following eight libraries: - BLIS (BLAS Library) - libFLAME (LAPACK) - AMD-FFTW - LibM (AMD Core Math Library) - ScaLAPACK - AMD Random Number Generator (RNG) - AMD Secure RNG - AOCL-Sparse Important The cray-libsci module is loaded by default for all users and this module also contains definitions of BLAS, LAPACK and ScaLAPACK routines that conflict with those in AOCL. The aocl module automatically unloads cray-libsci .","title":"AMD Optimizing CPU Libraries (AOCL)"},{"location":"software-libraries/aocl/#compiling-with-aocl","text":"Tip AOCL is currently unavailable for the Cray programming environments ( PrgEnv-cray ). This is untested and currently unsupported on ARCHER2.","title":"Compiling with AOCL"},{"location":"software-libraries/aocl/#gnu-programming-environment","text":"module load PrgEnv-gnu module load aocl","title":"GNU Programming Environment"},{"location":"software-libraries/aocl/#aocc-programming-environment","text":"AOCL is only available with aocc/3.0.0. module load PrgEnv-aocc module swap aocc/2.2.0.1 aocc/3.0.0 module load aocl","title":"AOCC Programming Environment"},{"location":"software-libraries/aocl/#resources","text":"For more information on AOCL, please see: https://developer.amd.com/amd-aocl/#documentation","title":"Resources"},{"location":"software-libraries/aocl/#version-history","text":"Module aocl/3.1 installed April 2022","title":"Version history.."},{"location":"software-libraries/arpack/","text":"ARPACK-NG The Arnoldi Package (ARPACK) was designed to compute eigenvalues and eigenvectors of large sparse matrices. Originally from Rice University, an open source version (ARPACK-NG) is available under a BSD license and is made available here. Compiling and linking with ARPACK module load arpack-ng To compile an application against the ARPACK-NG libraries, load the arpack-ng module and use the compiler wrappers cc , CC , and ftn in the usual way. The arpack-ng module defines ARPACK_NG_DIR which locates the root of the installation for the current programming environment. Version history Full system 4-cabinet system Module arpack-ng/3.8.0 installed October 2021 (PE 21.04) Not available Compiling your own version The current supported version of MUMPS on Archer2 can be compiled using a script available from the Archer githug repository. $ git clone https://github.com/ARCHER2-HPC/pe-scripts.git $ cd pe-scripts $ git checkout modules-2021-10 $ ./sh/arpack-ng.sh --prefix=/path/to/install/location where the --prefix specifies a suitable location. See the Archer2 github repository for further options and details. Resources Original ARPACK page ARPACK-NG github site","title":"ARPACK"},{"location":"software-libraries/arpack/#arpack-ng","text":"The Arnoldi Package (ARPACK) was designed to compute eigenvalues and eigenvectors of large sparse matrices. Originally from Rice University, an open source version (ARPACK-NG) is available under a BSD license and is made available here.","title":"ARPACK-NG"},{"location":"software-libraries/arpack/#compiling-and-linking-with-arpack","text":"module load arpack-ng To compile an application against the ARPACK-NG libraries, load the arpack-ng module and use the compiler wrappers cc , CC , and ftn in the usual way. The arpack-ng module defines ARPACK_NG_DIR which locates the root of the installation for the current programming environment.","title":"Compiling and linking with ARPACK"},{"location":"software-libraries/arpack/#version-history","text":"Full system 4-cabinet system Module arpack-ng/3.8.0 installed October 2021 (PE 21.04) Not available","title":"Version history"},{"location":"software-libraries/arpack/#compiling-your-own-version","text":"The current supported version of MUMPS on Archer2 can be compiled using a script available from the Archer githug repository. $ git clone https://github.com/ARCHER2-HPC/pe-scripts.git $ cd pe-scripts $ git checkout modules-2021-10 $ ./sh/arpack-ng.sh --prefix=/path/to/install/location where the --prefix specifies a suitable location. See the Archer2 github repository for further options and details.","title":"Compiling your own version"},{"location":"software-libraries/arpack/#resources","text":"Original ARPACK page ARPACK-NG github site","title":"Resources"},{"location":"software-libraries/boost/","text":"Boost Boost provide portable C++ libraries useful in a broad range of contexts. The libraries are freely available under the terms of the Boost Software license . Compiling and linking module load boost The C++ compiler wrapper CC will introduce the appropriate options to compile an application against the Boost libraries. The other compiler wrappers ( cc and ftn ) do not introduce these options. To check exactly what options are introduced type, e.g., $ CC --cray-print-opts The boost module also defines the environment variable BOOST_DIR as the root of the installation for the current programming environment if this information is needed. Version history Full system 4-cabinet system Module boost/1.72 installed October 2021 (PE 21.04) Module boost/1.72.0 installed January 2021 The following libraries are installed: atomic chrono container context contract coroutine date_time exception fiber filesystem graph_parallel graph iostreams locale log math mpi program_options random regex serialization stacktrace system test thread timer type_erasure wave Compiling Boost The ARCHER2 Github repository contains a recipe for compiling Boost for the different programming environments. $ git clone https://github.com/ARCHER2-HPC/pe-scripts.git $ cd pe-scripts $ git checkout cse-develop $ ./sh/boost.sh --prefix=/path/to/install/location where the --prefix determines the install location. The list of libraries compiled is specified in the boost.sh script. See the ARCHER2 Github repository for further information. Resources Boost home page . Documentation (HTML) for the current version . Boost GitHub repository .","title":"Boost"},{"location":"software-libraries/boost/#boost","text":"Boost provide portable C++ libraries useful in a broad range of contexts. The libraries are freely available under the terms of the Boost Software license .","title":"Boost"},{"location":"software-libraries/boost/#compiling-and-linking","text":"module load boost The C++ compiler wrapper CC will introduce the appropriate options to compile an application against the Boost libraries. The other compiler wrappers ( cc and ftn ) do not introduce these options. To check exactly what options are introduced type, e.g., $ CC --cray-print-opts The boost module also defines the environment variable BOOST_DIR as the root of the installation for the current programming environment if this information is needed.","title":"Compiling and linking"},{"location":"software-libraries/boost/#version-history","text":"Full system 4-cabinet system Module boost/1.72 installed October 2021 (PE 21.04) Module boost/1.72.0 installed January 2021 The following libraries are installed: atomic chrono container context contract coroutine date_time exception fiber filesystem graph_parallel graph iostreams locale log math mpi program_options random regex serialization stacktrace system test thread timer type_erasure wave","title":"Version history"},{"location":"software-libraries/boost/#compiling-boost","text":"The ARCHER2 Github repository contains a recipe for compiling Boost for the different programming environments. $ git clone https://github.com/ARCHER2-HPC/pe-scripts.git $ cd pe-scripts $ git checkout cse-develop $ ./sh/boost.sh --prefix=/path/to/install/location where the --prefix determines the install location. The list of libraries compiled is specified in the boost.sh script. See the ARCHER2 Github repository for further information.","title":"Compiling Boost"},{"location":"software-libraries/boost/#resources","text":"Boost home page . Documentation (HTML) for the current version . Boost GitHub repository .","title":"Resources"},{"location":"software-libraries/eigen/","text":"Eigen Eigen is a C++ template library for linear algebra: matrices, vectors, numerical solvers, and related algorithms. Compiling with Eigen module load eigen To compile an application with the Eigen header files, load the eigen module and use the compiler wrappers cc , CC , or ftn in the usual way. The relevant header files will be introduced automatically. The header files are located in /work/y07/shared/libs/core/eigen/3.4.0/ , and can be included manually at compilation without loading the module if required. Version history Module eigen/3.4.0 installed October 2021 Compiling your own version The current supported version on Archer2 can be built using the following script $ wget https://gitlab.com/libeigen/eigen/-/archive/3.4.0/eigen-3.4.0.tar.gz $ tar xvf eigen-3.4.0.tar.gz $ cmake eigen-3.4.0/ -DCMAKE_INSTALL_PREFIX=/path/to/install/location $ make install where the -DCMAKE_INSTALL_PREFIX option determines the install directory. Installing in this way will also build the Eigen documentation and unit-tests. Resources Eigen home page Getting Started guide","title":"Eigen"},{"location":"software-libraries/eigen/#eigen","text":"Eigen is a C++ template library for linear algebra: matrices, vectors, numerical solvers, and related algorithms.","title":"Eigen"},{"location":"software-libraries/eigen/#compiling-with-eigen","text":"module load eigen To compile an application with the Eigen header files, load the eigen module and use the compiler wrappers cc , CC , or ftn in the usual way. The relevant header files will be introduced automatically. The header files are located in /work/y07/shared/libs/core/eigen/3.4.0/ , and can be included manually at compilation without loading the module if required.","title":"Compiling with Eigen"},{"location":"software-libraries/eigen/#version-history","text":"Module eigen/3.4.0 installed October 2021","title":"Version history"},{"location":"software-libraries/eigen/#compiling-your-own-version","text":"The current supported version on Archer2 can be built using the following script $ wget https://gitlab.com/libeigen/eigen/-/archive/3.4.0/eigen-3.4.0.tar.gz $ tar xvf eigen-3.4.0.tar.gz $ cmake eigen-3.4.0/ -DCMAKE_INSTALL_PREFIX=/path/to/install/location $ make install where the -DCMAKE_INSTALL_PREFIX option determines the install directory. Installing in this way will also build the Eigen documentation and unit-tests.","title":"Compiling your own version"},{"location":"software-libraries/eigen/#resources","text":"Eigen home page Getting Started guide","title":"Resources"},{"location":"software-libraries/fftw/","text":"FFTW Provides: FFTW v3 Access: module load cray-fftw FFTW is a C subroutine library (which includes a Fortran interface) for computing the discrete Fourier transform (DFT) in one or more dimensions, of arbitrary input size, and of both real and complex data (as well as of even/odd data, i.e. the discrete cosine/sine transforms or DCT/DST). Only the version 3 interface is available on ARCHER2.","title":"FFTW"},{"location":"software-libraries/fftw/#fftw","text":"Provides: FFTW v3 Access: module load cray-fftw FFTW is a C subroutine library (which includes a Fortran interface) for computing the discrete Fourier transform (DFT) in one or more dimensions, of arbitrary input size, and of both real and complex data (as well as of even/odd data, i.e. the discrete cosine/sine transforms or DCT/DST). Only the version 3 interface is available on ARCHER2.","title":"FFTW"},{"location":"software-libraries/glm/","text":"GLM OpenGL Mathemetics (GLM) is a header-only C++ library which performs operations typically encountered in graphics applications, but can also be relevant to scientific applications. GLM is freely available under an MIT license. Compiling with GLM module load glm The compiler wrapper CC will automatically location the required include directory when the module is loaded. The glm module also defines the environment variable GLM_DIR which carries the root of the installation, if needed. Version history Full system 4-cabinet system Module glm/0.9.9.6 installed October 2021 (PE 21.04) Module glm/0.9.9.6 installed January 2021 Install your own version One can follow the instructions used to install the current version on ARCHER2 via the ARCHER2 Github repository : $ git clone https://github.com/ARCHER2-HPC/pe-scripts.git $ cd pe-scripts $ git checkout modules-2021-10 $ ./sh/glm.sh --prefix=/path/to/install/location where the --prefix option sets the install location. See the ARCHER2 Github repository for further details. Resources The GLM Github repository .","title":"GLM"},{"location":"software-libraries/glm/#glm","text":"OpenGL Mathemetics (GLM) is a header-only C++ library which performs operations typically encountered in graphics applications, but can also be relevant to scientific applications. GLM is freely available under an MIT license.","title":"GLM"},{"location":"software-libraries/glm/#compiling-with-glm","text":"module load glm The compiler wrapper CC will automatically location the required include directory when the module is loaded. The glm module also defines the environment variable GLM_DIR which carries the root of the installation, if needed.","title":"Compiling with GLM"},{"location":"software-libraries/glm/#version-history","text":"Full system 4-cabinet system Module glm/0.9.9.6 installed October 2021 (PE 21.04) Module glm/0.9.9.6 installed January 2021","title":"Version history"},{"location":"software-libraries/glm/#install-your-own-version","text":"One can follow the instructions used to install the current version on ARCHER2 via the ARCHER2 Github repository : $ git clone https://github.com/ARCHER2-HPC/pe-scripts.git $ cd pe-scripts $ git checkout modules-2021-10 $ ./sh/glm.sh --prefix=/path/to/install/location where the --prefix option sets the install location. See the ARCHER2 Github repository for further details.","title":"Install your own version"},{"location":"software-libraries/glm/#resources","text":"The GLM Github repository .","title":"Resources"},{"location":"software-libraries/hdf5/","text":"HDF5 The Hierarchical Data Format HDF5 (and its parallel manifestation HDF5 parallel) is a standard library and data format developed and supported by The HDF Group , and is released under a BSD-like license. Both serial and parallel versions are available on ARCHER2 as standard modules: module load cray-hdf5 (serial version) module load cray-hdf5-parallel (MPI parallel version) Use module help to locate cray- specific release notes on a particular version. Known issues: Full system 4-cabinet system There is currently a problem with the module file which means cray-hdf5-parallel will not operate correctly in PrgEnv-aocc . One can load module epcc-cray-hdf5-parallel instead as a work-around if PrgEnv-aocc is required. There are no currently known issues. Some general comments and information on serial and parallel I/O to ARCHER2 are given in the section on I/O and file systems . Resources Tutorials and introduction to HDF5 at the HDF5 Group pages. General information for developers of HDF5.","title":"HDF5"},{"location":"software-libraries/hdf5/#hdf5","text":"The Hierarchical Data Format HDF5 (and its parallel manifestation HDF5 parallel) is a standard library and data format developed and supported by The HDF Group , and is released under a BSD-like license. Both serial and parallel versions are available on ARCHER2 as standard modules: module load cray-hdf5 (serial version) module load cray-hdf5-parallel (MPI parallel version) Use module help to locate cray- specific release notes on a particular version. Known issues: Full system 4-cabinet system There is currently a problem with the module file which means cray-hdf5-parallel will not operate correctly in PrgEnv-aocc . One can load module epcc-cray-hdf5-parallel instead as a work-around if PrgEnv-aocc is required. There are no currently known issues. Some general comments and information on serial and parallel I/O to ARCHER2 are given in the section on I/O and file systems .","title":"HDF5"},{"location":"software-libraries/hdf5/#resources","text":"Tutorials and introduction to HDF5 at the HDF5 Group pages. General information for developers of HDF5.","title":"Resources"},{"location":"software-libraries/hypre/","text":"HYPRE HYPRE is a library of linear solvers for structured and unstructured problems with a particular emphasis on multigrid. It is a product of the Lawrence Livermore National Laboratory and is distrubted under either the MIT license or the Apache license. Compiling and linking with HYPRE module load hypre To compile and link an application with the HYPRE libraries, load the hypre module and use the compiler wrappers cc , CC , or ftn in the usual way. The relevant include files and libraries will be introduced automatically. Two versions of HYPRE are included: one with, and one without, OpenMP. The relevant version will be selected if e.g., -fopenmp is included in the compile or link stage. The hypre module defines the environment variable HYPRE_DIR which will show the root of the installation for the current programming environment if required. Version history Full system 4-cabinet system Module hypre/2.18.0 installed October 2021 (PE 21.04) Module hypre/2.18.0 installed January 2021 Compiling your own version The current supported version on Archer2 can be built using the script from the Archer2 repository: $ git clone https://github.com/ARCHER2-HPC/pe-scripts.git $ cd pe-scripts $ git checkout modules-2021-10 $ ./sh/tpsl/hypre.sh --prefix=/path/to/install/location where the --prefix option determines the install directory. See the Archer2 github repository for more information. Resources HYPRE home page The latest HYPRE user manual (HTML) An older pdf version HYPRE github repository","title":"HYPRE"},{"location":"software-libraries/hypre/#hypre","text":"HYPRE is a library of linear solvers for structured and unstructured problems with a particular emphasis on multigrid. It is a product of the Lawrence Livermore National Laboratory and is distrubted under either the MIT license or the Apache license.","title":"HYPRE"},{"location":"software-libraries/hypre/#compiling-and-linking-with-hypre","text":"module load hypre To compile and link an application with the HYPRE libraries, load the hypre module and use the compiler wrappers cc , CC , or ftn in the usual way. The relevant include files and libraries will be introduced automatically. Two versions of HYPRE are included: one with, and one without, OpenMP. The relevant version will be selected if e.g., -fopenmp is included in the compile or link stage. The hypre module defines the environment variable HYPRE_DIR which will show the root of the installation for the current programming environment if required.","title":"Compiling and linking with HYPRE"},{"location":"software-libraries/hypre/#version-history","text":"Full system 4-cabinet system Module hypre/2.18.0 installed October 2021 (PE 21.04) Module hypre/2.18.0 installed January 2021","title":"Version history"},{"location":"software-libraries/hypre/#compiling-your-own-version","text":"The current supported version on Archer2 can be built using the script from the Archer2 repository: $ git clone https://github.com/ARCHER2-HPC/pe-scripts.git $ cd pe-scripts $ git checkout modules-2021-10 $ ./sh/tpsl/hypre.sh --prefix=/path/to/install/location where the --prefix option determines the install directory. See the Archer2 github repository for more information.","title":"Compiling your own version"},{"location":"software-libraries/hypre/#resources","text":"HYPRE home page The latest HYPRE user manual (HTML) An older pdf version HYPRE github repository","title":"Resources"},{"location":"software-libraries/libsci/","text":"HPE Cray LibSci Provides: BLAS, LAPACK, CBLAS, LAPACKE, BLACS, ScaLAPACK Access: module load cray-libsci (note: loaded by default for all users) Cray scientific libraries, available for all compiler choices provides access to the Fortran BLAS and LAPACK interface for basic linear algebra, the corresponding C interfaces CBLAS and LAPACKE , and BLACS and ScaLAPACK for parallel linear algebra. Type man intro_libsci for further details.","title":"HPE Cray LibSci: BLAS, LAPACK, ScaLAPACK"},{"location":"software-libraries/libsci/#hpe-cray-libsci","text":"Provides: BLAS, LAPACK, CBLAS, LAPACKE, BLACS, ScaLAPACK Access: module load cray-libsci (note: loaded by default for all users) Cray scientific libraries, available for all compiler choices provides access to the Fortran BLAS and LAPACK interface for basic linear algebra, the corresponding C interfaces CBLAS and LAPACKE , and BLACS and ScaLAPACK for parallel linear algebra. Type man intro_libsci for further details.","title":"HPE Cray LibSci"},{"location":"software-libraries/matio/","text":"Matio Matio is a library which allows reading and writing matrices in MATLAB MAT format. It is an open source development released under a BSD license. Compiling and linking agaisnt Matio module load matio Load the matio module and use the standard compiler wrappers cc , CC , or ftn in the usual way. The appropriate header files and libraries will be included automatically via the compiler wrappers. The matio module set the PATH variable so that the stand-alone utility matdump can be used. The module also defines MATIO_PATH which gives the root of the installation if this is needed. Version history Full system 4-cabinet system Module matio/1.5.18 installed October 2021 (PE 21.04) Module matio/1.5.18 installed January 2021 Compiling your own version A version of Matio as currently installed on Archer2 can be compiled using the script avaailable from the Archer2 github repository : $ git clone https://github.com/ARCHER2-HPC/pe-scripts.git $ cd pe-scripts $ git checkout modules-2021-10 $ ./sh/tpsl/matio.sh --prefix=/path/to/install/location where --prefix defines the location of the installation. Resources Matio github repository","title":"Matio"},{"location":"software-libraries/matio/#matio","text":"Matio is a library which allows reading and writing matrices in MATLAB MAT format. It is an open source development released under a BSD license.","title":"Matio"},{"location":"software-libraries/matio/#compiling-and-linking-agaisnt-matio","text":"module load matio Load the matio module and use the standard compiler wrappers cc , CC , or ftn in the usual way. The appropriate header files and libraries will be included automatically via the compiler wrappers. The matio module set the PATH variable so that the stand-alone utility matdump can be used. The module also defines MATIO_PATH which gives the root of the installation if this is needed.","title":"Compiling and linking agaisnt Matio"},{"location":"software-libraries/matio/#version-history","text":"Full system 4-cabinet system Module matio/1.5.18 installed October 2021 (PE 21.04) Module matio/1.5.18 installed January 2021","title":"Version history"},{"location":"software-libraries/matio/#compiling-your-own-version","text":"A version of Matio as currently installed on Archer2 can be compiled using the script avaailable from the Archer2 github repository : $ git clone https://github.com/ARCHER2-HPC/pe-scripts.git $ cd pe-scripts $ git checkout modules-2021-10 $ ./sh/tpsl/matio.sh --prefix=/path/to/install/location where --prefix defines the location of the installation.","title":"Compiling your own version"},{"location":"software-libraries/matio/#resources","text":"Matio github repository","title":"Resources"},{"location":"software-libraries/metis/","text":"Metis and Parmetis The University of Minnesota provide a family of libraries for partitioning graphs and meshes, and computing fill-reducing ordering of sparse matrices. These libraries coming broadly under the label of \"Metis\". They are free to use for educational and research purposes. Metis module load metis Metis is the sequential library for partitioning problems; it also supplies a number of simple stand-alone utility programs to access the Metis API for graph and mesh partioning, and graph and mesh manipulation. The stand alone programs typically read a graph or mesh from file which must be in \"metis\" format. Compiling and linking with Metis The Metis library available via module load metis comes both with and without support for OpenMP. When using the compiler wrappers cc , CC , and ftn , the appropriate version will be selected based on the presence or absence of, e.g., -fopenmp in the compile or link invocation. Use, e.g., $ cc --cray-print-opts or $ cc -fopenmp --cray-print-opts to see exactly what options are being issued by the compiler wrapper when the metis module is loaded. Metis is currently provided as static libraries, so it should not be necessary to re-load the metis module at run time. The serial utilities (e.g. gpmetis for graph partitioning) are supplied without OpenMP. These may then be run on the front end for small problems if the metis module is loaded. The metis module defines the environment variable METIS_DIR which indicates the current location of the Metis installation. Parmetis module load parmetis Parmetis is the distributed memory incarnation of the Metis functionality. As for the metis module, Parmetis is integrated with use of the compiler wrappers cc , CC , and ftn . Parmetis depends on the metis module, which is loaded automatically by the parmetis module. The parmetis module defines the environment variable PARMETIS_DIR which holds the current location of the Parmetis installation. This variable may not respond to a change of compiler version within a given programming environment. If you wish to use PARMETIS_DIR in such a context, you may need to (re-)load the parmetis module after the change of compiler version. Module version history Full system 4-cabinet system module metis/5.1.0 installed October 2021 (PE21.04) module parmetis/4.0.3 installed January 2021 (PE21.04) module metis/5.1.0 installed January 2021 module parmetis/4.0.3 installed January 2021 Compile your own version The build procedure used for the Metis and Parmetis libraries on Archer2 is available via github. Metis The latest Archer2 version of Metis can be installed $ git clone https://github.com/ARCHER2-HPC/pe-scripts.git $ cd pe-scripts $ git checkout modules-2021-10 $ ./sh/tpsl/metis.sh --prefix=/path/to/install/location where --prefix determines the install location. This will download and install the default version for the current programming environment. Parmetis Parmetis can be installed in via the same mechanism as Metis: $ ./sh/tpsl/parmetis.sh --prefix=/path/to/install/location The Metis package should be installed first (as above) using the same location. See the Archer2 repository for further details and options. Resources Metis at George Karypis' site Metis homepage Metis manual (pdf) Parmetis homepage Parmetis manual (pdf)","title":"Metis/Parmetis"},{"location":"software-libraries/metis/#metis-and-parmetis","text":"The University of Minnesota provide a family of libraries for partitioning graphs and meshes, and computing fill-reducing ordering of sparse matrices. These libraries coming broadly under the label of \"Metis\". They are free to use for educational and research purposes.","title":"Metis and Parmetis"},{"location":"software-libraries/metis/#metis","text":"module load metis Metis is the sequential library for partitioning problems; it also supplies a number of simple stand-alone utility programs to access the Metis API for graph and mesh partioning, and graph and mesh manipulation. The stand alone programs typically read a graph or mesh from file which must be in \"metis\" format.","title":"Metis"},{"location":"software-libraries/metis/#compiling-and-linking-with-metis","text":"The Metis library available via module load metis comes both with and without support for OpenMP. When using the compiler wrappers cc , CC , and ftn , the appropriate version will be selected based on the presence or absence of, e.g., -fopenmp in the compile or link invocation. Use, e.g., $ cc --cray-print-opts or $ cc -fopenmp --cray-print-opts to see exactly what options are being issued by the compiler wrapper when the metis module is loaded. Metis is currently provided as static libraries, so it should not be necessary to re-load the metis module at run time. The serial utilities (e.g. gpmetis for graph partitioning) are supplied without OpenMP. These may then be run on the front end for small problems if the metis module is loaded. The metis module defines the environment variable METIS_DIR which indicates the current location of the Metis installation.","title":"Compiling and linking with Metis"},{"location":"software-libraries/metis/#parmetis","text":"module load parmetis Parmetis is the distributed memory incarnation of the Metis functionality. As for the metis module, Parmetis is integrated with use of the compiler wrappers cc , CC , and ftn . Parmetis depends on the metis module, which is loaded automatically by the parmetis module. The parmetis module defines the environment variable PARMETIS_DIR which holds the current location of the Parmetis installation. This variable may not respond to a change of compiler version within a given programming environment. If you wish to use PARMETIS_DIR in such a context, you may need to (re-)load the parmetis module after the change of compiler version.","title":"Parmetis"},{"location":"software-libraries/metis/#module-version-history","text":"Full system 4-cabinet system module metis/5.1.0 installed October 2021 (PE21.04) module parmetis/4.0.3 installed January 2021 (PE21.04) module metis/5.1.0 installed January 2021 module parmetis/4.0.3 installed January 2021","title":"Module version history"},{"location":"software-libraries/metis/#compile-your-own-version","text":"The build procedure used for the Metis and Parmetis libraries on Archer2 is available via github.","title":"Compile your own version"},{"location":"software-libraries/metis/#metis_1","text":"The latest Archer2 version of Metis can be installed $ git clone https://github.com/ARCHER2-HPC/pe-scripts.git $ cd pe-scripts $ git checkout modules-2021-10 $ ./sh/tpsl/metis.sh --prefix=/path/to/install/location where --prefix determines the install location. This will download and install the default version for the current programming environment.","title":"Metis"},{"location":"software-libraries/metis/#parmetis_1","text":"Parmetis can be installed in via the same mechanism as Metis: $ ./sh/tpsl/parmetis.sh --prefix=/path/to/install/location The Metis package should be installed first (as above) using the same location. See the Archer2 repository for further details and options.","title":"Parmetis"},{"location":"software-libraries/metis/#resources","text":"Metis at George Karypis' site Metis homepage Metis manual (pdf) Parmetis homepage Parmetis manual (pdf)","title":"Resources"},{"location":"software-libraries/mkl/","text":"Intel Math Kernel Library (MKL) The Intel Maths Kernel Libraries (MKL) contain a variety of optimised numerical libraries including BLAS, LAPACK, ScaLAPACK and FFTW. In general, the exact commands required to build against MKL depend on the details of compiler, environment, requirements for parallelism, and so on. The Intel MKL link line advisor should be consulted. Some examples are given below. Note that loading the mkl module will provide the environment variable MKLROOT which holds the location of the various MKL components. Warning The ARCHER2 CSE team have seen that using MKL on ARCHER2 for some software leads to failed regression tests due to numerical differences between refernece results and those produced with software using MKL. We strongly recommend that you use the HPE Cray LibSci and HPE Cray FFTW libraries for software if at all possible rather than MKL. If you do decide to use MKL on ARCHER2, then you should carefully validate results from your software to ensure that it is giving the expected results. Important The cray-libsci module is loaded by default for all users and this module also contains definitions of BLAS, LAPACK and ScaLAPACK routines that conflict with those in MKL. The mkl module automatically unloads cray-libsci . Important The mkl module needs to be loaded both at compile time and at runtime (usually in your job submission script). Tip MKL only supports the GCC programming environment ( PrgEnv-gnu ). Other programming environments may work but this is untested and unsupported on ARCHER2. Note Loading the mkl/19.5-281 module sets the environment variable MKL_DEBUG_CPU_TYPE=5 which is required to get good performance on AMD systems. More recent versions of MKL do not support this option. Serial MKL with GCC Swap modules: === Full system === module load PrgEnv-gnu module load mkl Language Compile options Link options Fortran -m64 -I\"${MKLROOT}/include\" -L${MKLROOT}/lib/intel64 -Wl,--no-as-needed -lmkl_gf_lp64 -lmkl_sequential -lmkl_core -lpthread -lm -ldl C/C++ -m64 -I\"${MKLROOT}/include\" -L${MKLROOT}/lib/intel64 -Wl,--no-as-needed -lmkl_intel_lp64 -lmkl_sequential -lmkl_core -lpthread -lm -ldl Threaded MKL with GCC Swap modules: === Full system === module load PrgEnv-gnu module load mkl Language Compile options Link options Fortran -m64 -I\"${MKLROOT}/include\" -L${MKLROOT}/lib/intel64 -Wl,--no-as-needed -lmkl_gf_lp64 -lmkl_gnu_thread -lmkl_core -lgomp -lpthread -lm -ldl C/C++ -m64 -I\"${MKLROOT}/include\" -L${MKLROOT}/lib/intel64 -Wl,--no-as-needed -lmkl_intel_lp64 -lmkl_gnu_thread -lmkl_core -lgomp -lpthread -lm -ldl MKL parallel ScaLAPACK with GCC Swap modules: === Full system === module load PrgEnv-gnu module load mkl Language Compile options Link options Fortran -m64 -I\"${MKLROOT}/include\" -L${MKLROOT}/lib/intel64 -lmkl_scalapack_lp64 -Wl,--no-as-needed -lmkl_gf_lp64 -lmkl_gnu_thread -lmkl_core -lmkl_blacs_intelmpi_lp64 -lgomp -lpthread -lm -ldl C/C++ -m64 -I\"${MKLROOT}/include\" -L${MKLROOT}/lib/intel64 -lmkl_scalapack_lp64 -Wl,--no-as-needed -lmkl_intel_lp64 -lmkl_gnu_thread -lmkl_core -lmkl_blacs_intelmpi_lp64 -lgomp -lpthread -lm -ldl","title":"Intel MKL"},{"location":"software-libraries/mkl/#intel-math-kernel-library-mkl","text":"The Intel Maths Kernel Libraries (MKL) contain a variety of optimised numerical libraries including BLAS, LAPACK, ScaLAPACK and FFTW. In general, the exact commands required to build against MKL depend on the details of compiler, environment, requirements for parallelism, and so on. The Intel MKL link line advisor should be consulted. Some examples are given below. Note that loading the mkl module will provide the environment variable MKLROOT which holds the location of the various MKL components. Warning The ARCHER2 CSE team have seen that using MKL on ARCHER2 for some software leads to failed regression tests due to numerical differences between refernece results and those produced with software using MKL. We strongly recommend that you use the HPE Cray LibSci and HPE Cray FFTW libraries for software if at all possible rather than MKL. If you do decide to use MKL on ARCHER2, then you should carefully validate results from your software to ensure that it is giving the expected results. Important The cray-libsci module is loaded by default for all users and this module also contains definitions of BLAS, LAPACK and ScaLAPACK routines that conflict with those in MKL. The mkl module automatically unloads cray-libsci . Important The mkl module needs to be loaded both at compile time and at runtime (usually in your job submission script). Tip MKL only supports the GCC programming environment ( PrgEnv-gnu ). Other programming environments may work but this is untested and unsupported on ARCHER2. Note Loading the mkl/19.5-281 module sets the environment variable MKL_DEBUG_CPU_TYPE=5 which is required to get good performance on AMD systems. More recent versions of MKL do not support this option.","title":"Intel Math Kernel Library (MKL)"},{"location":"software-libraries/mkl/#serial-mkl-with-gcc","text":"Swap modules: === Full system === module load PrgEnv-gnu module load mkl Language Compile options Link options Fortran -m64 -I\"${MKLROOT}/include\" -L${MKLROOT}/lib/intel64 -Wl,--no-as-needed -lmkl_gf_lp64 -lmkl_sequential -lmkl_core -lpthread -lm -ldl C/C++ -m64 -I\"${MKLROOT}/include\" -L${MKLROOT}/lib/intel64 -Wl,--no-as-needed -lmkl_intel_lp64 -lmkl_sequential -lmkl_core -lpthread -lm -ldl","title":"Serial MKL with GCC"},{"location":"software-libraries/mkl/#threaded-mkl-with-gcc","text":"Swap modules: === Full system === module load PrgEnv-gnu module load mkl Language Compile options Link options Fortran -m64 -I\"${MKLROOT}/include\" -L${MKLROOT}/lib/intel64 -Wl,--no-as-needed -lmkl_gf_lp64 -lmkl_gnu_thread -lmkl_core -lgomp -lpthread -lm -ldl C/C++ -m64 -I\"${MKLROOT}/include\" -L${MKLROOT}/lib/intel64 -Wl,--no-as-needed -lmkl_intel_lp64 -lmkl_gnu_thread -lmkl_core -lgomp -lpthread -lm -ldl","title":"Threaded MKL with GCC"},{"location":"software-libraries/mkl/#mkl-parallel-scalapack-with-gcc","text":"Swap modules: === Full system === module load PrgEnv-gnu module load mkl Language Compile options Link options Fortran -m64 -I\"${MKLROOT}/include\" -L${MKLROOT}/lib/intel64 -lmkl_scalapack_lp64 -Wl,--no-as-needed -lmkl_gf_lp64 -lmkl_gnu_thread -lmkl_core -lmkl_blacs_intelmpi_lp64 -lgomp -lpthread -lm -ldl C/C++ -m64 -I\"${MKLROOT}/include\" -L${MKLROOT}/lib/intel64 -lmkl_scalapack_lp64 -Wl,--no-as-needed -lmkl_intel_lp64 -lmkl_gnu_thread -lmkl_core -lmkl_blacs_intelmpi_lp64 -lgomp -lpthread -lm -ldl","title":"MKL parallel ScaLAPACK with GCC"},{"location":"software-libraries/mumps/","text":"MUMPS MUMPS is a parallel solver for large sparse systems and features a 'multifrontal' method and is developed largely at CERFCAS, ENS Lyon, IRIT Toulouse, INRIA, and the University of Bordeaux. It is provided free of charge and is largely under a CeCILL-C lisence. Compiling and linking with MUMPS module load mumps To compile an application against the MUMPS libraries, load the mumps module and use the compiler wrappers cc , CC , and ftn in the usual way. MUMPS is configured to allow Pord, Metis, Parmetis, and Scotch orderings. Two versions of MUMPS are provided: one with, and one without, OpenMP. The relevant version will be selected if the relevant option is included at the compile stage. The mumps module defines MUMPS_DIR which locates the root of the installation for the current programming environment. Version history Full system 4-cabinet system Module mumps/5.3.5 installed October 2021 (PE 21.04) Module mumps/5.2.1 installed January 2021 Known issues: The OpenMP version in PrgEnv-aocc is not available at the moment. Compiling your own version The current supported version of MUMPS on Archer2 can be compiled using a script available from the Archer githug repository. $ git clone https://github.com/ARCHER2-HPC/pe-scripts.git $ cd pe-scripts $ git checkout modules-2021-10 $ ./sh/tpsl/metis.sh --prefix=/path/to/install/location $ ./sh/tpsl/parmetis.sh --prefix=/path/to/install/location $ ./sh/tpsl/scotch.sh --prefix=/path/to/install/location $ ./sh/tpsl/mumps.sh --prefix=/path/to/install/location where the --prefix option should be the same for MUMPS at the three dependecies (Metis, Parmetis, and Scotch). See the Archer2 github repository for further options and details. Resources The MUMPS home page MUMPS user manual (pdf)","title":"MUMPS"},{"location":"software-libraries/mumps/#mumps","text":"MUMPS is a parallel solver for large sparse systems and features a 'multifrontal' method and is developed largely at CERFCAS, ENS Lyon, IRIT Toulouse, INRIA, and the University of Bordeaux. It is provided free of charge and is largely under a CeCILL-C lisence.","title":"MUMPS"},{"location":"software-libraries/mumps/#compiling-and-linking-with-mumps","text":"module load mumps To compile an application against the MUMPS libraries, load the mumps module and use the compiler wrappers cc , CC , and ftn in the usual way. MUMPS is configured to allow Pord, Metis, Parmetis, and Scotch orderings. Two versions of MUMPS are provided: one with, and one without, OpenMP. The relevant version will be selected if the relevant option is included at the compile stage. The mumps module defines MUMPS_DIR which locates the root of the installation for the current programming environment.","title":"Compiling and linking with MUMPS"},{"location":"software-libraries/mumps/#version-history","text":"Full system 4-cabinet system Module mumps/5.3.5 installed October 2021 (PE 21.04) Module mumps/5.2.1 installed January 2021 Known issues: The OpenMP version in PrgEnv-aocc is not available at the moment.","title":"Version history"},{"location":"software-libraries/mumps/#compiling-your-own-version","text":"The current supported version of MUMPS on Archer2 can be compiled using a script available from the Archer githug repository. $ git clone https://github.com/ARCHER2-HPC/pe-scripts.git $ cd pe-scripts $ git checkout modules-2021-10 $ ./sh/tpsl/metis.sh --prefix=/path/to/install/location $ ./sh/tpsl/parmetis.sh --prefix=/path/to/install/location $ ./sh/tpsl/scotch.sh --prefix=/path/to/install/location $ ./sh/tpsl/mumps.sh --prefix=/path/to/install/location where the --prefix option should be the same for MUMPS at the three dependecies (Metis, Parmetis, and Scotch). See the Archer2 github repository for further options and details.","title":"Compiling your own version"},{"location":"software-libraries/mumps/#resources","text":"The MUMPS home page MUMPS user manual (pdf)","title":"Resources"},{"location":"software-libraries/netcdf/","text":"NetCDF The Network Common Data Form NetCDF (and its parallel manifestation NetCDF parallel) is a standard library and data format developed and supported by UCAR is released under a BSD-like license. Both serial and parallel versions are available on ARCHER2 as standard modules: module load cray-netcdf (serial version) module load cray-netcdf-hdf5parallel (MPI parallel version) Note that one should first load the relevant HDF module file, e.g., $ module load cray-hdf5 $ module load cray-netcdf for the serial version. Use module spider to locate available versions, and use module help to locate cray- specific release notes on a particular version. Known issues: Full system 4-cabinet system There is currently a problem with the module file which means cray-netcdf-hdf5parallel will not operate correctly in PrgEnv-aocc . One can load module epcc-netcdf-hdf5parallel instead as a work-around if PrgEnv-aocc is required. There are no currently known issues. Some general comments and information on serial and parallel I/O to ARCHER2 are given in the section on I/O and file systems . Resources The NetCDF home page .","title":"NetCDF"},{"location":"software-libraries/netcdf/#netcdf","text":"The Network Common Data Form NetCDF (and its parallel manifestation NetCDF parallel) is a standard library and data format developed and supported by UCAR is released under a BSD-like license. Both serial and parallel versions are available on ARCHER2 as standard modules: module load cray-netcdf (serial version) module load cray-netcdf-hdf5parallel (MPI parallel version) Note that one should first load the relevant HDF module file, e.g., $ module load cray-hdf5 $ module load cray-netcdf for the serial version. Use module spider to locate available versions, and use module help to locate cray- specific release notes on a particular version. Known issues: Full system 4-cabinet system There is currently a problem with the module file which means cray-netcdf-hdf5parallel will not operate correctly in PrgEnv-aocc . One can load module epcc-netcdf-hdf5parallel instead as a work-around if PrgEnv-aocc is required. There are no currently known issues. Some general comments and information on serial and parallel I/O to ARCHER2 are given in the section on I/O and file systems .","title":"NetCDF"},{"location":"software-libraries/netcdf/#resources","text":"The NetCDF home page .","title":"Resources"},{"location":"software-libraries/petsc/","text":"PETSc PETSc is a suite of parallel tools for solution of partial differential equations. PETSc is developed at Argonne National Laboratory and is freely available under a BSD 2-clause license. Build module load petsc Applications may be linked against PETSc by loading the petsc module and using the compiler wrappers cc , CC , and ftn in the usual way. Details of options introduced by the compiler wrappers can be examined via, e.g., $ cc --cray-print-opts PETSC is configured with Metis, Parmetis, and Scotch orderings, and to support HYPRE, MUMPS, SuperLU, and SuperLU-DIST. PETSc is compiled without OpenMP. The petsc module defines the environment variable PETSC_DIR as the root of the installation if this is required. Version history Full system 4-cabinet system Module petsc/3.14.2 installed October 2021 (PE 21.04) Module petsc/3.13.3 installed January 2021 Known issues: PETSc is not currently available for PrgEnv-aocc . There is no HYPRE support in this version. Compile your own version It is possible to follow the steps used to build the current version on Archer2. These steps are codified at the Archer2 github repository and include a number of dependencies to be built in the correct order: $ git clone https://github.com/ARCHER2-HPC/pe-scripts.git $ cd pe-scripts $ git checkout modules-2021-10 $ ./sh/tpsl/metis.sh --prefix=/path/to/install/location $ ./sh/tpsl/parmetis.sh --prefix=/path/to/install/location $ ./sh/tpsl/hypre.sh --prefix=/path/to/install/location $ ./sh/tpsl/scotch.sh --prefix=/path/to/install/location $ ./sh/tpsl/mumps.sh --prefix=/path/to/install/location $ ./sh/tpsl/superlu.sh --prefix=/path/to/install/location $ ./sh/tpsl/superlu-dist.sh --prefix=/path/to/install/location $ module load cray-hdf5 $ ./sh/petsc.sh --prefix=/path/to/install/location The --prefix option indicating the install directory should be the same in all cases. See the Archer2 github repository for further details (and options). Resources PETSc home page PETSc documentation (HTML) Current user manual (pdf)","title":"PETSc"},{"location":"software-libraries/petsc/#petsc","text":"PETSc is a suite of parallel tools for solution of partial differential equations. PETSc is developed at Argonne National Laboratory and is freely available under a BSD 2-clause license.","title":"PETSc"},{"location":"software-libraries/petsc/#build","text":"module load petsc Applications may be linked against PETSc by loading the petsc module and using the compiler wrappers cc , CC , and ftn in the usual way. Details of options introduced by the compiler wrappers can be examined via, e.g., $ cc --cray-print-opts PETSC is configured with Metis, Parmetis, and Scotch orderings, and to support HYPRE, MUMPS, SuperLU, and SuperLU-DIST. PETSc is compiled without OpenMP. The petsc module defines the environment variable PETSC_DIR as the root of the installation if this is required.","title":"Build"},{"location":"software-libraries/petsc/#version-history","text":"Full system 4-cabinet system Module petsc/3.14.2 installed October 2021 (PE 21.04) Module petsc/3.13.3 installed January 2021 Known issues: PETSc is not currently available for PrgEnv-aocc . There is no HYPRE support in this version.","title":"Version history"},{"location":"software-libraries/petsc/#compile-your-own-version","text":"It is possible to follow the steps used to build the current version on Archer2. These steps are codified at the Archer2 github repository and include a number of dependencies to be built in the correct order: $ git clone https://github.com/ARCHER2-HPC/pe-scripts.git $ cd pe-scripts $ git checkout modules-2021-10 $ ./sh/tpsl/metis.sh --prefix=/path/to/install/location $ ./sh/tpsl/parmetis.sh --prefix=/path/to/install/location $ ./sh/tpsl/hypre.sh --prefix=/path/to/install/location $ ./sh/tpsl/scotch.sh --prefix=/path/to/install/location $ ./sh/tpsl/mumps.sh --prefix=/path/to/install/location $ ./sh/tpsl/superlu.sh --prefix=/path/to/install/location $ ./sh/tpsl/superlu-dist.sh --prefix=/path/to/install/location $ module load cray-hdf5 $ ./sh/petsc.sh --prefix=/path/to/install/location The --prefix option indicating the install directory should be the same in all cases. See the Archer2 github repository for further details (and options).","title":"Compile your own version"},{"location":"software-libraries/petsc/#resources","text":"PETSc home page PETSc documentation (HTML) Current user manual (pdf)","title":"Resources"},{"location":"software-libraries/scotch/","text":"Scotch and PT-Scotch Scotch and its parallel version PT-Scotch are provided by Labri at the University of Bordeaux and INRIA Bordeaux South-West. They are used for graph partitioning and ordering problems. The libraries are freely available for scientific use under a license similar to the LGPL license. Scotch and PT-Scotch module load scotch The scotch module provides access to both the Scotch and PT-Scotch libraies via the compiler system. A number of stand-alone utilities are also provided as part of the package. Compiling and linking If the scotch module is loaded, then applications may be automatically compiled and linked against the libraries for the current programming environment. Check, e.g., $ cc --cray-print-opts if you wish to see exactly what options are generated by the compiler wrappers. Scotch and PT-Scotch libraries are provides as static archvies only. The compiler wrappers do not give access to the libraries libscotcherrexit.a or libptscotcherrexit.a . If you wish to perform your own error handling these libraries must be linked manually. The scotch module defines the environment SCOTCH_DIR which holds the root of the installation for a given programming environment. Libraries are present in ${SCOTCH_DIR}/lib . Stand-alone applications are also avaialble. See the Scotch and PT-Scotch user manuals for further details. Module version history Full system 4-cabinet system Module `scotch/6.1.0 installed October 2021 (PE 21.04) Known issue: a small number of the standard PT-Scotch tests are failing (all programming environments). Symptoms include truncated MPI_Recvs . This is currently being investigated. Module scotch/6.0.10 installed January 2021 Known issue: a small number of the standard PT-Scotch tests are failing (all programming environments). Symptoms include truncated MPI_Recvs . This is currently being investigated. Compiling your own version The build procedure for the Scotch package on Archer2 is available via github. Scotch and PT-Scotch The latest Scotch and PT-Scotch libraries are installed on Archer using the following mechanism: $ git clone https://github.com/ARCHER2-HPC/pe-scripts.git $ cd pe-scripts $ git checkout modules-2021-10 $ ./sh/tpsl/scotch.sh --prefix=/path/to/install/location where the --prefix option defines the destination for the install. This script will download, compile and install the librairies. Resources The Scotch home page Scotch user manual (pdf) PT-Scotch user manual (pdf)","title":"Scotch/PT-Scotch"},{"location":"software-libraries/scotch/#scotch-and-pt-scotch","text":"Scotch and its parallel version PT-Scotch are provided by Labri at the University of Bordeaux and INRIA Bordeaux South-West. They are used for graph partitioning and ordering problems. The libraries are freely available for scientific use under a license similar to the LGPL license.","title":"Scotch and PT-Scotch"},{"location":"software-libraries/scotch/#scotch-and-pt-scotch_1","text":"module load scotch The scotch module provides access to both the Scotch and PT-Scotch libraies via the compiler system. A number of stand-alone utilities are also provided as part of the package.","title":"Scotch and PT-Scotch"},{"location":"software-libraries/scotch/#compiling-and-linking","text":"If the scotch module is loaded, then applications may be automatically compiled and linked against the libraries for the current programming environment. Check, e.g., $ cc --cray-print-opts if you wish to see exactly what options are generated by the compiler wrappers. Scotch and PT-Scotch libraries are provides as static archvies only. The compiler wrappers do not give access to the libraries libscotcherrexit.a or libptscotcherrexit.a . If you wish to perform your own error handling these libraries must be linked manually. The scotch module defines the environment SCOTCH_DIR which holds the root of the installation for a given programming environment. Libraries are present in ${SCOTCH_DIR}/lib . Stand-alone applications are also avaialble. See the Scotch and PT-Scotch user manuals for further details.","title":"Compiling and linking"},{"location":"software-libraries/scotch/#module-version-history","text":"Full system 4-cabinet system Module `scotch/6.1.0 installed October 2021 (PE 21.04) Known issue: a small number of the standard PT-Scotch tests are failing (all programming environments). Symptoms include truncated MPI_Recvs . This is currently being investigated. Module scotch/6.0.10 installed January 2021 Known issue: a small number of the standard PT-Scotch tests are failing (all programming environments). Symptoms include truncated MPI_Recvs . This is currently being investigated.","title":"Module version history"},{"location":"software-libraries/scotch/#compiling-your-own-version","text":"The build procedure for the Scotch package on Archer2 is available via github.","title":"Compiling your own version"},{"location":"software-libraries/scotch/#scotch-and-pt-scotch_2","text":"The latest Scotch and PT-Scotch libraries are installed on Archer using the following mechanism: $ git clone https://github.com/ARCHER2-HPC/pe-scripts.git $ cd pe-scripts $ git checkout modules-2021-10 $ ./sh/tpsl/scotch.sh --prefix=/path/to/install/location where the --prefix option defines the destination for the install. This script will download, compile and install the librairies.","title":"Scotch and PT-Scotch"},{"location":"software-libraries/scotch/#resources","text":"The Scotch home page Scotch user manual (pdf) PT-Scotch user manual (pdf)","title":"Resources"},{"location":"software-libraries/slepc/","text":"SLEPC The Scalable Library for Eigenvalue Problem computations is an extension of PETSc developed at the Universitat Politecnica de Valencia. SLEPc is freely available under a 2-clause BSD license. Compiling and linking with SLEPc module load slepc To compile an application against the SLEPc libaries, load the slepc module and use the compiler wrappers cc , CC , and ftn in the usual way. Static libraries are available so no module is required at run time. The SLEPc module defines SLEPC_DIR which locates the root of the installation. Version history Full system 4-cabinet system Module slepc/3.14.1 installed October 2021 (PE 21.04) Module slepc/3.13.2 installed January 2021 Compiling your own version The version of SLEPc currently available on ARCHER2 can be compiled using a script available from the ARCHER2 github repository: $ git clone https://github.com/ARCHER2-HPC/pe-scripts.git $ cd pe-scripts $ git checkout modules-2021-10 $ ./sh/slepc.sh --prefix=/path/to/install/location The dependencies (including PETSc) can be built in the same way, or taken from the existing modules. See the ARCHER2 github repository for further information. Resources SLEPc home page SLEPc user manual (PDF) SLEPc Gitlab repository","title":"SLEPc"},{"location":"software-libraries/slepc/#slepc","text":"The Scalable Library for Eigenvalue Problem computations is an extension of PETSc developed at the Universitat Politecnica de Valencia. SLEPc is freely available under a 2-clause BSD license.","title":"SLEPC"},{"location":"software-libraries/slepc/#compiling-and-linking-with-slepc","text":"module load slepc To compile an application against the SLEPc libaries, load the slepc module and use the compiler wrappers cc , CC , and ftn in the usual way. Static libraries are available so no module is required at run time. The SLEPc module defines SLEPC_DIR which locates the root of the installation.","title":"Compiling and linking with SLEPc"},{"location":"software-libraries/slepc/#version-history","text":"Full system 4-cabinet system Module slepc/3.14.1 installed October 2021 (PE 21.04) Module slepc/3.13.2 installed January 2021","title":"Version history"},{"location":"software-libraries/slepc/#compiling-your-own-version","text":"The version of SLEPc currently available on ARCHER2 can be compiled using a script available from the ARCHER2 github repository: $ git clone https://github.com/ARCHER2-HPC/pe-scripts.git $ cd pe-scripts $ git checkout modules-2021-10 $ ./sh/slepc.sh --prefix=/path/to/install/location The dependencies (including PETSc) can be built in the same way, or taken from the existing modules. See the ARCHER2 github repository for further information.","title":"Compiling your own version"},{"location":"software-libraries/slepc/#resources","text":"SLEPc home page SLEPc user manual (PDF) SLEPc Gitlab repository","title":"Resources"},{"location":"software-libraries/superlu/","text":"SuperLU and SuperLU_DIST SuperLU and SuperLU_DIST are libraies for the direct solution of large sparse non-symmetric systems of linear equations, typically by factorisation and back-substitution. The libraries are provided by Lawrence Berkeley National Laboratory and are freely available under a slightly modified BSD-style license. Two separate modules are provided for SuperLU and SuperLU_DIST. SuperLU module load superlu This module provides the serial library SuperLU. Compiling and linking with SuperLU Compiling and linking SuperLU applications requires no special action beyond module load superlu and using the standard compiler wrappers cc , CC , or ftn . The exact options issued by the compiler wrapper can be examined via, e.g., $ cc --cray-print-opts while the module is loaded. The module defines the environment variable SUPERLU_DIR as the root location of the installation for a given programming environment. Version history Full system 4-cabinet system Module superlu/5.2.2 installed October 2021 (PE 21.04) Module superle/5.2.1 installed January 2021 SuperLU_DIST module load superlu-dist This modules provides the distrubuted memory parallel library SuperLU_DIST both with and without OpenMP. Compiling and linking SuperLU_DIST Use the standard compiler wrappers: $ cc my_superlu_dist_application.c or $ cc -fopenmp my_superlu_dist_application.c to compile the and link against the appropriate libraries. The superlu-dist module defines the environment variable SUPERLU_DIST_DIR as the root of the installation for the current programming environment. Version history Full system 4-cabinet system Module superlu-dist/6.4.0 installed October 2021 (PE 21.04) Module superlu-dist/6.1.1 installed January 2021 Compiling your own version The build used for Archer2 can be replicated by using the scripts provided at the Archer2 repository. SuperLU The current Archer2 supported version may be built via $ git clone https://github.com/ARCHER2-HPC/pe-scripts.git $ cd pe-scripts $ git checkout modules-2021-10 $ ./sh/tpsl/superlu.sh --prefix=/path/to/install/location where the --prefix option controls the install destination. SuperLU_DIST SuperLU_DIST is configured using Metis and Parmetis, so these should be installed first: $ ./sh/tpsl/metis.sh --prefix=/path/to/install/location $ ./sh/tpsl/parmetis.sh --prefix=/path/to/install/location $ ./sh/tpsl/superlu_dist.sh --prefix=/path/to/install/location will download, compile, and install the relevant libraries. The install location should be the same for all three packages. See the Archer2 github repository for further options and details. Resources The Supernodal LU project home page The SuperLU User guide (pdf). This describes both SuperLU and SuperLU_DIST. The SuperLU github repository The SuperLU_DIST github repository","title":"SuperLU/SuperLU_DIST"},{"location":"software-libraries/superlu/#superlu-and-superlu_dist","text":"SuperLU and SuperLU_DIST are libraies for the direct solution of large sparse non-symmetric systems of linear equations, typically by factorisation and back-substitution. The libraries are provided by Lawrence Berkeley National Laboratory and are freely available under a slightly modified BSD-style license. Two separate modules are provided for SuperLU and SuperLU_DIST.","title":"SuperLU and SuperLU_DIST"},{"location":"software-libraries/superlu/#superlu","text":"module load superlu This module provides the serial library SuperLU.","title":"SuperLU"},{"location":"software-libraries/superlu/#compiling-and-linking-with-superlu","text":"Compiling and linking SuperLU applications requires no special action beyond module load superlu and using the standard compiler wrappers cc , CC , or ftn . The exact options issued by the compiler wrapper can be examined via, e.g., $ cc --cray-print-opts while the module is loaded. The module defines the environment variable SUPERLU_DIR as the root location of the installation for a given programming environment.","title":"Compiling and linking with SuperLU"},{"location":"software-libraries/superlu/#version-history","text":"Full system 4-cabinet system Module superlu/5.2.2 installed October 2021 (PE 21.04) Module superle/5.2.1 installed January 2021","title":"Version history"},{"location":"software-libraries/superlu/#superlu_dist","text":"module load superlu-dist This modules provides the distrubuted memory parallel library SuperLU_DIST both with and without OpenMP.","title":"SuperLU_DIST"},{"location":"software-libraries/superlu/#compiling-and-linking-superlu_dist","text":"Use the standard compiler wrappers: $ cc my_superlu_dist_application.c or $ cc -fopenmp my_superlu_dist_application.c to compile the and link against the appropriate libraries. The superlu-dist module defines the environment variable SUPERLU_DIST_DIR as the root of the installation for the current programming environment.","title":"Compiling and linking SuperLU_DIST"},{"location":"software-libraries/superlu/#version-history_1","text":"Full system 4-cabinet system Module superlu-dist/6.4.0 installed October 2021 (PE 21.04) Module superlu-dist/6.1.1 installed January 2021","title":"Version history"},{"location":"software-libraries/superlu/#compiling-your-own-version","text":"The build used for Archer2 can be replicated by using the scripts provided at the Archer2 repository.","title":"Compiling your own version"},{"location":"software-libraries/superlu/#superlu_1","text":"The current Archer2 supported version may be built via $ git clone https://github.com/ARCHER2-HPC/pe-scripts.git $ cd pe-scripts $ git checkout modules-2021-10 $ ./sh/tpsl/superlu.sh --prefix=/path/to/install/location where the --prefix option controls the install destination.","title":"SuperLU"},{"location":"software-libraries/superlu/#superlu_dist_1","text":"SuperLU_DIST is configured using Metis and Parmetis, so these should be installed first: $ ./sh/tpsl/metis.sh --prefix=/path/to/install/location $ ./sh/tpsl/parmetis.sh --prefix=/path/to/install/location $ ./sh/tpsl/superlu_dist.sh --prefix=/path/to/install/location will download, compile, and install the relevant libraries. The install location should be the same for all three packages. See the Archer2 github repository for further options and details.","title":"SuperLU_DIST"},{"location":"software-libraries/superlu/#resources","text":"The Supernodal LU project home page The SuperLU User guide (pdf). This describes both SuperLU and SuperLU_DIST. The SuperLU github repository The SuperLU_DIST github repository","title":"Resources"},{"location":"software-libraries/trilinos/","text":"Trilinos Trilinos is a large collection of packages with software components that can be used for scientific and engineering problems. Most of the package are released under a BSD license (and some under LGPL). Compiling and linking against Trilinos module load trilinos Applications may be built against the module version of Trilinos by using the using the compiler wrappers CC or ftn in the normal way. The appropriate include files and library paths will be inserted automatically. Trilinos is build with OpenPM enabled. The trilinos module defines the environment variable TRILINOS_DIR as the root of the installation for the current programming environment. Trilinos also provides a small number of stand-alone execuables which are available via the standard PATH mechanism while the module is loaded. Version history Full system 4-cabinet system Module trilinos/12.18.1 installed October 2021 (PE 21.04) If using AMD compilers, module version aocc/3.0.0 is required. module trilinos/12.18.1 installed January 2021 Known issue Trilinos is not available in PrgEnv-aocc at the moment. Known issue The ForTrilinos package is not available in this version. Packages enabled are: Amesos, Amesos2, Anasazi, AztecOO Belos Epetra EpretExt FEI Galeri GlobiPack Ifpack Ifpack2 Intrepid Isorropia Kokkos Komplex Mesquite ML Moertel MueLu NOX OptiPack Pamgen Phalanx Piro Pliris ROL RTOp Rythmos Sacado Shards ShyLU STK STKSearch STKTopology STKUtil Stratimikos Teko Teuchos Thyra Tpetra TrilinosCouplings Triutils Xpetra Zoltan Zoltan2 Compiling Trilinos A script which has details of the relevant configuration options for Trilinos is available at the ARCHER2 Github repository . The script will build a static-only version of the libraries. $ git clone https://github.com/ARCHER2-HPC/pe-scripts.git $ cd pe-scripts $ git checkout modules-2021-10 $ ... $ ./sh/trilinos.sh --prefix=/path/to/install/location where --prefix sets the installation location. The ellipsis ... is standing for the dependencies used to build Trilinos, which here are: metis, parmetis, superlu, superlu-dist, scotch, mumps, glm, boost . These packages should be built as described in their corresponding pages linked in the menu on the left. See the ARCHER2 Github repository for further details. Note that Trilinos may take up to one hour to compile on its own, and so the compilation is best performed as a batch job. Resources Trilinos home page Trilinos Github repository","title":"Trilinos"},{"location":"software-libraries/trilinos/#trilinos","text":"Trilinos is a large collection of packages with software components that can be used for scientific and engineering problems. Most of the package are released under a BSD license (and some under LGPL).","title":"Trilinos"},{"location":"software-libraries/trilinos/#compiling-and-linking-against-trilinos","text":"module load trilinos Applications may be built against the module version of Trilinos by using the using the compiler wrappers CC or ftn in the normal way. The appropriate include files and library paths will be inserted automatically. Trilinos is build with OpenPM enabled. The trilinos module defines the environment variable TRILINOS_DIR as the root of the installation for the current programming environment. Trilinos also provides a small number of stand-alone execuables which are available via the standard PATH mechanism while the module is loaded.","title":"Compiling and linking against Trilinos"},{"location":"software-libraries/trilinos/#version-history","text":"Full system 4-cabinet system Module trilinos/12.18.1 installed October 2021 (PE 21.04) If using AMD compilers, module version aocc/3.0.0 is required. module trilinos/12.18.1 installed January 2021 Known issue Trilinos is not available in PrgEnv-aocc at the moment. Known issue The ForTrilinos package is not available in this version. Packages enabled are: Amesos, Amesos2, Anasazi, AztecOO Belos Epetra EpretExt FEI Galeri GlobiPack Ifpack Ifpack2 Intrepid Isorropia Kokkos Komplex Mesquite ML Moertel MueLu NOX OptiPack Pamgen Phalanx Piro Pliris ROL RTOp Rythmos Sacado Shards ShyLU STK STKSearch STKTopology STKUtil Stratimikos Teko Teuchos Thyra Tpetra TrilinosCouplings Triutils Xpetra Zoltan Zoltan2","title":"Version history"},{"location":"software-libraries/trilinos/#compiling-trilinos","text":"A script which has details of the relevant configuration options for Trilinos is available at the ARCHER2 Github repository . The script will build a static-only version of the libraries. $ git clone https://github.com/ARCHER2-HPC/pe-scripts.git $ cd pe-scripts $ git checkout modules-2021-10 $ ... $ ./sh/trilinos.sh --prefix=/path/to/install/location where --prefix sets the installation location. The ellipsis ... is standing for the dependencies used to build Trilinos, which here are: metis, parmetis, superlu, superlu-dist, scotch, mumps, glm, boost . These packages should be built as described in their corresponding pages linked in the menu on the left. See the ARCHER2 Github repository for further details. Note that Trilinos may take up to one hour to compile on its own, and so the compilation is best performed as a batch job.","title":"Compiling Trilinos"},{"location":"software-libraries/trilinos/#resources","text":"Trilinos home page Trilinos Github repository","title":"Resources"},{"location":"user-guide/","text":"User and Best Practice Guide The ARCHER2 User and Best Practice Guide covers all aspects of use of the ARCHER2 service. This includes fundamentals (required by all users to use the system effectively), best practice for getting the most out of ARCHER2 and more technical topics. The User and Best Practice Guide contains the following sections: Connecting to ARCHER2 Data management and transfer Software environment Running jobs on ARCHER2 I/O and file systems Application development environment Containers Using Python Data analysis Debugging Profiling Performance tuning ARCHER2 hardware Energy monitoring","title":"Overview"},{"location":"user-guide/#user-and-best-practice-guide","text":"The ARCHER2 User and Best Practice Guide covers all aspects of use of the ARCHER2 service. This includes fundamentals (required by all users to use the system effectively), best practice for getting the most out of ARCHER2 and more technical topics. The User and Best Practice Guide contains the following sections: Connecting to ARCHER2 Data management and transfer Software environment Running jobs on ARCHER2 I/O and file systems Application development environment Containers Using Python Data analysis Debugging Profiling Performance tuning ARCHER2 hardware Energy monitoring","title":"User and Best Practice Guide"},{"location":"user-guide/analysis/","text":"Data analysis As well as being used for scientific simulations, ARCHER2 can also be used for data pre-/post-processing and analysis. This page provides an overview of the different options for doing so. Using the login nodes The easiest way to run non-computationally intensive data analysis is to run directly on the login nodes. However, please remember that the login nodes are a shared resource and should not be used for long-running tasks. Example: Running an R script on a login node module load cray-R Rscript example.R Using the compute nodes If running on the login nodes is not feasible (e.g. due to memory requirements or computationally intensive analysis), the compute nodes can also be used for data analysis. Important This is a more expensive option, as you will be charged for using the entire node, even though your analysis may only be using one core. Example: Running an R script on a compute node Full system #!/bin/bash #SBATCH --job-name=data_analysis #SBATCH --time=0:10:0 #SBATCH --nodes=1 #SBATCH --tasks-per-node=1 #SBATCH --cpus-per-task=1 #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard module load cray-R Rscript example.R An advantage of this method is that you can use Job chaining to automate the process of analysing your output data once your compute job has finished. Using interactive jobs For more interactive analysis, it may be useful to use salloc to reserve a compute node on which to do your analysis. This allows you to run jobs directly on the compute nodes from the command line without using a job submission script. More information on interactive jobs can be found here . Example: Reserving a single node for 20 minutes for interactive analysis Full system auser@ln01:> salloc --nodes = 1 --tasks-per-node = 1 --cpus-per-task = 1 \\ --time = 00 :20:00 --partition = standard --qos = short \\ --account =[ budget code ] Note If you want to run for longer than 20 minutes, you will need to use a different QoS as the maximum runtime for the short QoS is 20 mins. Data analysis nodes The data analysis nodes on the ARCHER2 system are designed for large compilations, post-calculation analysis and data manipulation. They should be used for jobs which are too small to require a whole compute node, but which would have an adverse impact on the operation of the login nodes if they were run interactively. Unlike compute nodes, the data analysis nodes are able to access the home, work, and the RDFaaS file systems. They can also be used to transfer data from a remote system to ARCHER2 and vice versa (using e.g. scp or rsync ). This can be useful when transferring large amounts of data that might take hours to complete. Requesting resources on the data analysis nodes using Slurm The ARCHER2 data analysis nodes can be reached by using the serial partition and the serial QoS. Unlike other nodes on ARCHER2, you may only request part of a single node and you will likely be sharing the node with other users. The data analysis nodes are set up such that you can specify the number of cores you want to use (up to 32 physical cores) and the amount of memory you want for your job (up to 125 GB). You can have multiple jobs running on the data analysis nodes at the same time, but the total number of cores used by those jobs cannot exceed 32, and the total memory used by jobs currently running from a single user cannot exceed 125 GB -- any jobs above this limit will remain pending until your previous jobs are finished. You do not need to specify both number of cores and memory for jobs on the data analysis nodes. By default, you will get 1984 MiB of memory per core (which is a little less than 2 GB), when specifying cores only, and 1 core when specifying the memory only. Note Each data analysis node is fitted with 512 GB of memory. However, a small amount of this memory is needed for system processes, which is why we set an upper limit of 125 GB per user (a user is limited to one quarter of the RAM on a node). This is also why the per-core default memory allocation is slightly less than 2 GB. Note When running on the data analysis nodes, you must always specify either the number of cores you want, the amount of memory you want, or both. The examples shown below specify the number of cores with the --ntasks flag and the memory with the --mem flag. If you are only wanting to specify one of the two, please remember to delete the other one. Example: Running a serial batch script on the data analysis nodes A Slurm batch script for the data analysis nodes looks very similar to one for the compute nodes. The main differences are that you need to use --partition=serial and --qos=serial , specify the number of tasks (rather than the number of nodes) and/or specify the amount of memory you want. For example, to use a single core and 4 GB of memory, you would use something like: #!/bin/bash # Slurm job options (job-name, job time) #SBATCH --job-name=data_analysis #SBATCH --time=0:20:0 #SBATCH --ntasks=1 # Replace [budget code] below with your budget code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=serial #SBATCH --qos=serial # Define memory required for this jobs. By default, you would # get just under 2 GB, but you can ask for up to 125 GB. #SBATCH --mem=4G # Set the number of threads to 1 # This prevents any threaded system libraries from automatically # using threading. export OMP_NUM_THREADS = 1 module load cray-python python my_analysis_script.py Interactive session on the data analysis nodes There are two ways to start an interactive session on the data analysis nodes: you can either use salloc to reserve a part of a data analysis node for interactive jobs; or, you can use srun to open a terminal on the node and run things on the node directly. You can find out more information on the advantages and disadvantages of both of these methods in the Running jobs on ARCHER2 section of the User and Best Practice Guide. Using salloc for interactive access You can reserve resources on a data analysis node using salloc . For example, to request 1 core and 4 GB of memory for 20 minutes, you would use: auser@ln01:~> salloc --time = 00 :20:00 --partition = serial --qos = serial \\ --account =[ budget code ] --ntasks = 1 \\ --mem = 4G When you submit this job, your terminal will display something like: salloc: Pending job allocation 523113 salloc: job 523113 queued and waiting for resources salloc: job 523113 has been allocated resources salloc: Granted job allocation 523113 salloc: Waiting for resource configuration salloc: Nodes dvn01 are ready for job auser@ln01:~> It may take some time for your interactive job to start. Once it runs you will enter a standard interactive terminal session (a new shell). Note that this shell is still on the front end (the prompt has not changed). Whilst the interactive session lasts you will be able to run jobs on the data analysis nodes by issuing the srun command directly at your command prompt. The maximum number of cores and memory you can use is limited by resources requested in the salloc command (or by the defaults if you did not explicitly ask for particular amounts of resource). Your session will end when you hit the requested walltime. If you wish to finish before this you should use the exit command - this will return you to your prompt before you issued the salloc command. Using srun for interactive access You can get a command prompt directly on the data analysis nodes by using the srun command directly. For example, to reserve 1 core and 8 GB of memory, you would use: auser@ln01:~> srun --time = 00 :20:00 --partition = serial --qos = serial \\ --account =[ budget code ] \\ --ntasks = 1 --mem = 8G \\ --pty /bin/bash The --pty /bin/bash will cause a new shell to be started on the data analysis node. (This is perhaps closer to what many people consider an 'interactive' job than the method using the salloc method described above.) One can now issue shell commands in the usual way. When finished, type exit to relinquish the allocation and control will be returned to the front end. By default, the interactive shell will retain the environment of the parent. If you want a clean shell, remember to specify the --export=none option to the srun command. Visualising data using the data analysis nodes using X You can view data on the data analysis nodes by starting an interactive srun session with the --x11 flag to export the X display back to your local system. For 1 core with * GB of memory: auser@ln01:~> srun --time = 00 :20:00 --partition = serial --qos = serial \\ --hint = nomultithread --account =[ budget code ] \\ --ntasks = 1 --mem = 8G --x11 --pty /bin/bash Tip Data visualisation on ARCHER2 is only possible if you used the -X or -Y flag to the ssh command when when logging in to the system. Using Singularity Singularity can be useful for data analysis, as sites such as DockerHub or SingularityHub contain many pre-built images of data analysis tools that can be simply downloaded and used on ARCHER2. More information about Singularity on ARCHER2 can be found in the Containers section section of the User and Best Practice Guide. Data analysis tools Useful tools for data analysis can be found on the Data Analysis and Tools page.","title":"Data analysis"},{"location":"user-guide/analysis/#data-analysis","text":"As well as being used for scientific simulations, ARCHER2 can also be used for data pre-/post-processing and analysis. This page provides an overview of the different options for doing so.","title":"Data analysis"},{"location":"user-guide/analysis/#using-the-login-nodes","text":"The easiest way to run non-computationally intensive data analysis is to run directly on the login nodes. However, please remember that the login nodes are a shared resource and should not be used for long-running tasks.","title":"Using the login nodes"},{"location":"user-guide/analysis/#example-running-an-r-script-on-a-login-node","text":"module load cray-R Rscript example.R","title":"Example: Running an R script on a login node"},{"location":"user-guide/analysis/#using-the-compute-nodes","text":"If running on the login nodes is not feasible (e.g. due to memory requirements or computationally intensive analysis), the compute nodes can also be used for data analysis. Important This is a more expensive option, as you will be charged for using the entire node, even though your analysis may only be using one core.","title":"Using the compute nodes"},{"location":"user-guide/analysis/#example-running-an-r-script-on-a-compute-node","text":"Full system #!/bin/bash #SBATCH --job-name=data_analysis #SBATCH --time=0:10:0 #SBATCH --nodes=1 #SBATCH --tasks-per-node=1 #SBATCH --cpus-per-task=1 #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard module load cray-R Rscript example.R An advantage of this method is that you can use Job chaining to automate the process of analysing your output data once your compute job has finished.","title":"Example: Running an R script on a compute node"},{"location":"user-guide/analysis/#using-interactive-jobs","text":"For more interactive analysis, it may be useful to use salloc to reserve a compute node on which to do your analysis. This allows you to run jobs directly on the compute nodes from the command line without using a job submission script. More information on interactive jobs can be found here .","title":"Using interactive jobs"},{"location":"user-guide/analysis/#example-reserving-a-single-node-for-20-minutes-for-interactive-analysis","text":"Full system auser@ln01:> salloc --nodes = 1 --tasks-per-node = 1 --cpus-per-task = 1 \\ --time = 00 :20:00 --partition = standard --qos = short \\ --account =[ budget code ] Note If you want to run for longer than 20 minutes, you will need to use a different QoS as the maximum runtime for the short QoS is 20 mins.","title":"Example: Reserving a single node for 20 minutes for interactive analysis"},{"location":"user-guide/analysis/#data-analysis-nodes","text":"The data analysis nodes on the ARCHER2 system are designed for large compilations, post-calculation analysis and data manipulation. They should be used for jobs which are too small to require a whole compute node, but which would have an adverse impact on the operation of the login nodes if they were run interactively. Unlike compute nodes, the data analysis nodes are able to access the home, work, and the RDFaaS file systems. They can also be used to transfer data from a remote system to ARCHER2 and vice versa (using e.g. scp or rsync ). This can be useful when transferring large amounts of data that might take hours to complete.","title":"Data analysis nodes"},{"location":"user-guide/analysis/#requesting-resources-on-the-data-analysis-nodes-using-slurm","text":"The ARCHER2 data analysis nodes can be reached by using the serial partition and the serial QoS. Unlike other nodes on ARCHER2, you may only request part of a single node and you will likely be sharing the node with other users. The data analysis nodes are set up such that you can specify the number of cores you want to use (up to 32 physical cores) and the amount of memory you want for your job (up to 125 GB). You can have multiple jobs running on the data analysis nodes at the same time, but the total number of cores used by those jobs cannot exceed 32, and the total memory used by jobs currently running from a single user cannot exceed 125 GB -- any jobs above this limit will remain pending until your previous jobs are finished. You do not need to specify both number of cores and memory for jobs on the data analysis nodes. By default, you will get 1984 MiB of memory per core (which is a little less than 2 GB), when specifying cores only, and 1 core when specifying the memory only. Note Each data analysis node is fitted with 512 GB of memory. However, a small amount of this memory is needed for system processes, which is why we set an upper limit of 125 GB per user (a user is limited to one quarter of the RAM on a node). This is also why the per-core default memory allocation is slightly less than 2 GB. Note When running on the data analysis nodes, you must always specify either the number of cores you want, the amount of memory you want, or both. The examples shown below specify the number of cores with the --ntasks flag and the memory with the --mem flag. If you are only wanting to specify one of the two, please remember to delete the other one.","title":"Requesting resources on the data analysis nodes using Slurm"},{"location":"user-guide/analysis/#example-running-a-serial-batch-script-on-the-data-analysis-nodes","text":"A Slurm batch script for the data analysis nodes looks very similar to one for the compute nodes. The main differences are that you need to use --partition=serial and --qos=serial , specify the number of tasks (rather than the number of nodes) and/or specify the amount of memory you want. For example, to use a single core and 4 GB of memory, you would use something like: #!/bin/bash # Slurm job options (job-name, job time) #SBATCH --job-name=data_analysis #SBATCH --time=0:20:0 #SBATCH --ntasks=1 # Replace [budget code] below with your budget code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=serial #SBATCH --qos=serial # Define memory required for this jobs. By default, you would # get just under 2 GB, but you can ask for up to 125 GB. #SBATCH --mem=4G # Set the number of threads to 1 # This prevents any threaded system libraries from automatically # using threading. export OMP_NUM_THREADS = 1 module load cray-python python my_analysis_script.py","title":"Example: Running a serial batch script on the data analysis nodes"},{"location":"user-guide/analysis/#interactive-session-on-the-data-analysis-nodes","text":"There are two ways to start an interactive session on the data analysis nodes: you can either use salloc to reserve a part of a data analysis node for interactive jobs; or, you can use srun to open a terminal on the node and run things on the node directly. You can find out more information on the advantages and disadvantages of both of these methods in the Running jobs on ARCHER2 section of the User and Best Practice Guide.","title":"Interactive session on the data analysis nodes"},{"location":"user-guide/analysis/#using-salloc-for-interactive-access","text":"You can reserve resources on a data analysis node using salloc . For example, to request 1 core and 4 GB of memory for 20 minutes, you would use: auser@ln01:~> salloc --time = 00 :20:00 --partition = serial --qos = serial \\ --account =[ budget code ] --ntasks = 1 \\ --mem = 4G When you submit this job, your terminal will display something like: salloc: Pending job allocation 523113 salloc: job 523113 queued and waiting for resources salloc: job 523113 has been allocated resources salloc: Granted job allocation 523113 salloc: Waiting for resource configuration salloc: Nodes dvn01 are ready for job auser@ln01:~> It may take some time for your interactive job to start. Once it runs you will enter a standard interactive terminal session (a new shell). Note that this shell is still on the front end (the prompt has not changed). Whilst the interactive session lasts you will be able to run jobs on the data analysis nodes by issuing the srun command directly at your command prompt. The maximum number of cores and memory you can use is limited by resources requested in the salloc command (or by the defaults if you did not explicitly ask for particular amounts of resource). Your session will end when you hit the requested walltime. If you wish to finish before this you should use the exit command - this will return you to your prompt before you issued the salloc command.","title":"Using salloc for interactive access"},{"location":"user-guide/analysis/#using-srun-for-interactive-access","text":"You can get a command prompt directly on the data analysis nodes by using the srun command directly. For example, to reserve 1 core and 8 GB of memory, you would use: auser@ln01:~> srun --time = 00 :20:00 --partition = serial --qos = serial \\ --account =[ budget code ] \\ --ntasks = 1 --mem = 8G \\ --pty /bin/bash The --pty /bin/bash will cause a new shell to be started on the data analysis node. (This is perhaps closer to what many people consider an 'interactive' job than the method using the salloc method described above.) One can now issue shell commands in the usual way. When finished, type exit to relinquish the allocation and control will be returned to the front end. By default, the interactive shell will retain the environment of the parent. If you want a clean shell, remember to specify the --export=none option to the srun command.","title":"Using srun for interactive access"},{"location":"user-guide/analysis/#visualising-data-using-the-data-analysis-nodes-using-x","text":"You can view data on the data analysis nodes by starting an interactive srun session with the --x11 flag to export the X display back to your local system. For 1 core with * GB of memory: auser@ln01:~> srun --time = 00 :20:00 --partition = serial --qos = serial \\ --hint = nomultithread --account =[ budget code ] \\ --ntasks = 1 --mem = 8G --x11 --pty /bin/bash Tip Data visualisation on ARCHER2 is only possible if you used the -X or -Y flag to the ssh command when when logging in to the system.","title":"Visualising data using the data analysis nodes using X"},{"location":"user-guide/analysis/#using-singularity","text":"Singularity can be useful for data analysis, as sites such as DockerHub or SingularityHub contain many pre-built images of data analysis tools that can be simply downloaded and used on ARCHER2. More information about Singularity on ARCHER2 can be found in the Containers section section of the User and Best Practice Guide.","title":"Using Singularity"},{"location":"user-guide/analysis/#data-analysis-tools","text":"Useful tools for data analysis can be found on the Data Analysis and Tools page.","title":"Data analysis tools"},{"location":"user-guide/connecting/","text":"Connecting to ARCHER2 This section covers the basic connection methods. On the ARCHER2 system, interactive access is achieved using SSH, either directly from a command-line terminal or using an SSH client. In addition, data can be transferred to and from the ARCHER2 system using scp from the command line or by using a file-transfer client. Before following the process below, we assume you have set up an account on ARCHER2 through the EPCC SAFE. Documentation on how to do this can be found at: SAFE Guide for Users Command line terminal Linux Linux distributions include a terminal application that can be used for SSH access to the ARCHER2 login nodes. Linux users will have different terminals depending on their distribution and window manager (e.g., GNOME Terminal in GNOME, Konsole in KDE). Consult your Linux distribution's documentation for details on how to load a terminal. MacOS MacOS users can use the Terminal application, located in the Utilities folder within the Applications folder. Windows A typical Windows installation will not include a terminal client, though there are various clients available. We recommend Windows users download and install MobaXterm to access ARCHER2. It is very easy to use and includes an integrated X Server, which allows you to run graphical applications on ARCHER2. You can download MobaXterm Home Edition (Installer Edition) from the following link: Install MobaXterm Double-click the downloaded Microsoft Installer file (.msi) and follow the instructions from the Windows Installation Wizard. Note, you might need to have administrator rights to install on some versions of Windows. Also, make sure to check whether Windows Firewall has blocked any features of this program after installation (Windows will warn you if the built-in firewall blocks an action, and gives you the opportunity to override the behaviour). Once installed, start MobaXterm and then click \"Start local terminal\". Tips If you download the .zip file rather than the .msi, make sure you unzip it before attempting to run the installer. If you do not have administrator rights, you can use the Portable edition of MobaXterm . If this is your first time using MobaXterm, you should check that a permanent /home directory has been set up (otherwise, all saved info will be lost from session to session). Go to \"Settings\" -> \"Configuration\" and check that a path is set in the field marked \"Persistent home directory\". If prompted, make sure path is set as \"private\". Any SSH key generated in MobaXterm will, by default, be stored in the permanent /home directory (see above). That is, if your /home directory is _MyDocuments_\\MobaXterm\\home then within that folder you will find a folder named _MyDocuments_\\MobaXterm\\home\\.ssh containing your keys. This folder will be 'hidden' by default, so you may need to tick 'Hidden items' under 'View' in Windows Explorer to see it. MobaXterm also allows you to set up pre-configured SSH sessions with the username, login host and key details saved. You are welcome to use this, rather than using the \"Local terminal\", but we are not able to assist with debugging connection issues if you choose this method. Access credentials To access ARCHER2, you need to use two sets of credentials: a password and an SSH key pair protected by a passphrase. You can find more detailed instructions on how to set up your credentials to access ARCHER2 from Windows, MacOS and Linux below. SSH Key Pairs You will need to generate an SSH key pair protected by a passphrase to access ARCHER2. Using a terminal (the command line), set up a key pair that contains your e-mail address and enter a passphrase you will use to unlock the key: $ ssh-keygen -t rsa -C \"your@email.com\" ... -bash-4.1$ ssh-keygen -t rsa -C \"your@email.com\" Generating public/private rsa key pair. Enter file in which to save the key (/Home/user/.ssh/id_rsa): [Enter] Enter passphrase (empty for no passphrase): [Passphrase] Enter same passphrase again: [Passphrase] Your identification has been saved in /Home/user/.ssh/id_rsa. Your public key has been saved in /Home/user/.ssh/id_rsa.pub. The key fingerprint is: 03:d4:c4:6d:58:0a:e2:4a:f8:73:9a:e8:e3:07:16:c8 your@email.com The key's randomart image is: +--[ RSA 2048]----+ | . ...+o++++. | | . . . =o.. | |+ . . .......o o | |oE . . | |o = . S | |. +.+ . | |. oo | |. . | | .. | +-----------------+ (remember to replace \"your@email.com\" with your e-mail address). Upload public part of key pair to SAFE You should now upload the public part of your SSH key pair to the SAFE by following the instructions at: Login to SAFE . Then: Go to the Menu Login accounts and select the ARCHER2 account you want to add the SSH key to. On the subsequent Login Account details page, click the Add Credential button. Select SSH public key as the Credential Type and click Next Either copy and paste the public part of your SSH key into the SSH Public key box or use the button to select the public key file on your computer. Click Add to associate the public SSH key with your account. Once you have done this, your SSH key will be added to your ARCHER2 account. Remember, you need both an SSH key and a password to log in to ARCHER2. You will need to collect an initial password before you can log into ARCHER2. We cover this next. Note If you want to connect to ARCHER2 from more than one machine---for example, from your home laptop as well as your work laptop---you should generate an SSH key on each machine, and add each of the public keys into SAFE. Initial passwords The SAFE web interface is used to provide your initial password for logging onto ARCHER2 (see the SAFE Documentation for more details on requesting accounts and picking up passwords). Note You will be prompted to change your password the first time that you log in to ARCHER2. You may also change your password, at any time, on ARCHER2, using the passwd command. This change is not be reflected in SAFE so, if you forget your password, you should use SAFE to request a new one-shot password. SSH Clients As noted above, you interact with ARCHER2, over an encrypted communication channel (specifically, Secure Shell version 2 (SSH-2)). This allows command-line access to one of the login nodes of ARCHER2, from which you can run commands or use a command-line text editor to edit files. SSH can also be used to run graphical programs such as GUI text editors and debuggers, when used in conjunction with an X Server. Logging in The login addresses for ARCHER2 are: ARCHER2 full system: login.archer2.ac.uk You can use the following command from the terminal window to log in to ARCHER2: Full system ssh username@login.archer2.ac.uk The order in which you are asked for credentials depends on the system you are accessing: Full system You will first be prompted for the passphrase associated with your SSH key pair. Once you have entered this passphrase successfully, you will then be prompted for your machine account password. You need to enter both credentials correctly to be able to access ARCHER2. Tip If you previously logged into the 4-cabinet system with your account you may see an error from SSH that looks like @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ @ WARNING: POSSIBLE DNS SPOOFING DETECTED! @ @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ The ECDSA host key for login.archer2.ac.uk has changed, and the key for the corresponding IP address 193.62.216.43 has a different value. This could either mean that DNS SPOOFING is happening or the IP address for the host and its host key have changed at the same time. Offending key for IP in /Users/auser/.ssh/known_hosts:11 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ @ WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED! @ @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY! Someone could be eavesdropping on you right now (man-in-the-middle attack)! It is also possible that a host key has just been changed. The fingerprint for the ECDSA key sent by the remote host is SHA256:UGS+LA8I46LqnD58WiWNlaUFY3uD1WFr+V8RCG09fUg. Please contact your system administrator. If you see this, you should delete the offending host key from your ~/.ssh/known_hosts file (in the example above the offending line is line #11) Warning If your SSH key pair is not stored in the default location (usually ~/.ssh/id_rsa ) on your local system, you may need to specify the path to the private part of the key wih the -i option to ssh . For example, if your key is in a file called keys/id_rsa_ARCHER2 you would use the command ssh -i keys/id_rsa_ARCHER2 username@login.archer2.ac.uk to log in (or the equivalent for the 4-cabinet system). Tip When you first log into ARCHER2, you will be prompted to change your initial password. This is a three-step process: When promoted to enter your ldap password : Re-enter the password you retrieved from SAFE. When prompted to enter your new password: type in a new password. When prompted to re-enter the new password: re-enter the new password. Your password will now have been changed To allow remote programs, especially graphical applications, to control your local display, such as for a debugger, use: Full system ssh -X username@login.archer2.ac.uk Some sites recommend using the -Y flag. While this can fix some compatibility issues, the -X flag is more secure. Current MacOS systems do not have an X window system. Users should install the XQuartz package to allow for SSH with X11 forwarding on MacOS systems: XQuartz website Host Keys Adding the host keys to your SSH configuration file provides an extra level of security for your connections to ARCHER2. The host keys are checked against the login nodes when you login to ARCHER2 and if the remote server key does not match the one in the configuration file, the connection will be refused. This provides protection against potential malicious servers masquerading as the ARCHER2 login nodes. login.archer2.ac.uk login.archer2.ac.uk ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBEnMeFf1TPZ4pbupWeD4IeahEeeqJMAhrCv1znyQGAL45yOIArVltscW8GNhzfaWk5vKb9sIAm2mJZPc3b7te3c= login.archer2.ac.uk ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCZKpFN25u13uSTOun8jOKEO+4Y/98DW9/8dxoGYOf8Q7qZEyQUGk5QUuJCiB7ZzCOJ01Lxl+ghYpQ13oiebZWkTWUdypSCBH5f4/y5z+f87fDqOjkHKhpYb90RlpbP+Ik+6IapQOTYKGBPFfwkbp2LYh3ktV7ocpKVCNst0k5IELNufNBgsGNFYNyRYIR6hHoH2kUqDvrN8IXf8085vKbKdMQPdAtIEX7sOX+UNUpR/46zcAyn8VRn/CGA5WA39nKKOiPzJn8pFtKDUmme/DA9/Y+Z/jJS55coHxV81Qws5WYmg7bzgVDkZJvtzQ6haJAOhsWNYzNtrEwNNDCc610z login.archer2.ac.uk ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAII/OY5bYUBnnLr0B7keiT97WtzSGtTsTexpgmxkdCI+b Host key verification can fail if this key is out of date, a problem which can be fixed by removing the offending entry in ~/.ssh/known_hosts and replacing it with the new key published here. We recommend users should check this page for any key updates and not just accept a new key from the server without confirmation. Making access more convenient using the SSH configuration file Typing in the full command to log in or transfer data to ARCHER2 can become tedious as it often has to be repeated several times. You can use the SSH configuration file, usually located on your local machine at .ssh/config to make the process more convenient. Each remote site (or group of sites) can have an entry in this file, which may look something like: Full system Host archer2 HostName login.archer2.ac.uk User username (remember to replace username with your actual username!). Taking the full-system example: the Host line defines a short name for the entry. In this case, instead of typing ssh username@login.archer2.ac.uk to access the ARCHER2 login nodes, you could use ssh archer2 instead. The remaining lines define the options for the host. Hostname login.archer2.ac.uk --- defines the full address of the host User username --- defines the username to use by default for this host (replace username with your own username on the remote host) Now you can use SSH to access ARCHER2 without needing to enter your username or the full hostname every time: ssh archer2 You can set up as many of these entries as you need in your local configuration file. Other options are available. See the ssh_config manual page (or man ssh_config on any machine with SSH installed) for a description of the SSH configuration file. For example, you may find the IdentityFile option useful if you have to manage multiple SSH key pairs for different systems as this allows you to specify which SSH key to use for each system. Bug There is a known bug with Windows ssh-agent. If you get the error message: Warning: agent returned different signature type ssh-rsa (expected rsa-sha2-512) , you will need to either specify the path to your ssh key in the command line (using the -i option as described above) or add that path to your SSH config file by using the IdentityFile option. SSH debugging tips If you find you are unable to connect to ARCHER2, there are some simple checks you may use to diagnose the issue, which are described below. If you are having difficulties connecting, we suggest trying these before contacting the ARCHER2 Service Desk. Use the user@login.archer2.ac.uk syntax rather than -l user login.archer2.ac.uk We have seen a number of instances where people using the syntax ssh -l user login.archer2.ac.uk have not been able to connect properly and get prompted for a password many times. We have found that using the alternative syntax: ssh user@login.archer2.ac.uk works more reliably. Can you connect to the login node? Try the command ping -c 3 login.archer2.ac.uk , on Linux or MacOS, or ping -n 3 login.archer2.ac.uk on Windows. If you successfully connect to the login node, the output should include: --- login.archer2.ac.uk ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 38ms (the ping time '38ms' is not important). If not all packets are received there could be a problem with your Internet connection, or the login node could be unavailable. Password If you are having trouble entering your password, consider using a password manager, from which you can copy and paste it. If you need to reset your password, instructions for doing so can be found in the SAFE documentation Windows users should note that the Ctrl+V shortcut does not work to paste in to PuTTY, MobaXterm, or PowerShell. Instead use Shift+Ins to paste. Alternatively, right-click and select 'Paste' in PuTTY and MobaXterm, or simply right-click to paste in PowerShell. SSH key If you get the error message Permission denied (publickey) , this may indicate a problem with your SSH key. Some things to check: Have you uploaded the key to SAFE? Please note that if the same key is re-uploaded, SAFE will not map the \"new\" key to ARCHER2. If for some reason this is required, please delete the key first, then re-upload. Is SSH using the correct key? You can check which keys are being found and offered by SSH using ssh -vvv . If your private key has a non-default name, you should use the -i option to provide it to ssh. For example, ssh -i path/to/key username@login.archer2.ac.uk . Are you entering the passphrase correctly? You will be asked for your private key's passphrase first. If you enter it incorrectly you will usually be asked to enter it again (usually you will get three chances, after which SSH will fail with Permission denied (publickey) ). If you would like to confirm your passphrase without attempting to connect, you can use ssh-keygen -y -f /path/to/private/key . If successful, this command will print the corresponding public key. You can also use this to check that you have uploaded the correct public key to SAFE. Are permissions correct on the SSH key? One common issue is that the permissions are set incorrectly on either the key files or the directory it is contained in. On Linux and MacOS, if your private keys are held in ~/.ssh/ you can check this with ls -al ~/.ssh . This should give something similar to the following output: $ ls -al ~/.ssh/ drwx------. 2 user group 48 Jul 15 20:24 . drwx------. 12 user group 4096 Oct 13 12:11 .. -rw-------. 1 user group 113 Jul 15 20:23 authorized_keys -rw-------. 1 user group 12686 Jul 15 20:23 id_rsa -rw-r--r--. 1 user group 2785 Jul 15 20:23 id_rsa.pub -rw-r--r--. 1 user group 1967 Oct 13 14:11 known_hosts The important section here is the string of letters and dashes at the start, for the lines ending in . , id_rsa , and id_rsa.pub , which indicate permissions on the containing directory, private key, and public key, respectively. If your permissions are not correct, they can be set with chmod . Consult the table below for the relevant chmod command. Target Permissions chmod Code Directory drwx------ 700 Private Key -rw------- 600 Public Key -rw-r--r-- 644 chmod can be used to set permissions on the target in the following way: chmod <code> <target> . So for example to set correct permissions on the private key file id_rsa_ARCHER2 , use the command chmod 600 id_rsa_ARCHER2 . On Windows, permissions are handled differently but can be set by right-clicking on the file and selecting Properties > Security > Advanced. The user, SYSTEM, and Administrators should have Full control , and no other permissions should exist for both the public and private key files, as well as the containing folder. Tip Unix file permissions can be understood in the following way. There are three groups that can have file permissions: (owning) users , (owning) groups , and others . The available permissions are read , write , and execute . The first character indicates whether the target is a file - , or directory d . The next three characters indicate the owning user's permissions. The first character is r if they have read permission, - if they don't, the second character is w if they have write permission, - if they don't, the third character is x if they have execute permission, - if they don't. This pattern is then repeated for group , and other permissions. For example the pattern -rw-r--r-- indicates that the owning user can read and write the file, members of the owning group can read it, and anyone else can also read it. The chmod codes are constructed by treating the user, group, and owner permission strings as binary numbers, then converting them to decimal. For example the permission string -rwx------ becomes 111 000 000 -> 700 . SSH verbose output The verbose-debugging output from ssh can be very useful for diagnosing issues. In particular, it can be used to distinguish between problems with the SSH key and password. To enable verbose output, add the -vvv flag to your SSH command. For example: ssh -vvv username@login.archer2.ac.uk The output is lengthy, but somewhere in there you should see lines similar to the following: debug1: Next authentication method: publickey debug1: Offering public key: RSA SHA256:<key_hash> <path_to_private_key> debug3: send_pubkey_test debug3: send packet: type 50 debug2: we sent a publickey packet, wait for reply debug3: receive packet: type 60 debug1: Server accepts key: pkalg rsa-sha2-512 blen 2071 debug2: input_userauth_pk_ok: fp SHA256:<key_hash> debug3: sign_and_send_pubkey: RSA SHA256:<key_hash> Enter passphrase for key '<path_to_private_key>': debug3: send packet: type 50 debug3: receive packet: type 51 Authenticated with partial success. debug1: Authentications that can continue: password, keyboard-interactive In the text above, you can see which files ssh has checked for private keys, and you can see if any key is accepted. The line Authenticated succeeded indicates that the SSH key has been accepted. By default SSH will go through a list of standard private-key files, as well as any you have specified with -i or a config file. To succeed, one of these private keys needs to match to the public key uploaded to SAFE. If your SSH key passphrase is incorrect, you will be asked to try again up to three times in total, before being disconnected with Permission denied (publickey) . If you enter your passphrase correctly, but still see this error message, please consider the advice under SSH key above. You should next see something similiar to: debug1: Next authentication method: keyboard-interactive debug2: userauth_kbdint debug3: send packet: type 50 debug2: we sent a keyboard-interactive packet, wait for reply debug3: receive packet: type 60 debug2: input_userauth_info_req debug2: input_userauth_info_req: num_prompts 1 Password: debug3: send packet: type 61 debug3: receive packet: type 60 debug2: input_userauth_info_req debug2: input_userauth_info_req: num_prompts 0 debug3: send packet: type 61 debug3: receive packet: type 52 debug1: Authentication succeeded (keyboard-interactive). If you do not see the Password: prompt you may have connection issues, or there could be a problem with the ARCHER2 login nodes. If you do not see Authenticated with partial success it means your password was not accepted. You will be asked to re-enter your password, usually two more times before the connection will be rejected. Consider the suggestions under Password above. If you do see Authenticated with partial success , it means your password was accepted, and your SSH key will now be checked. The equivalent information can be obtained in PuTTY by enabling All Logging in settings. Related Software tmux tmux is a multiplexer application available on the ARCHER2 login nodes. It allows for multiple sessions to be open concurrently and these sessions can be detached and run in the background. Furthermore, sessions will continue to run after a user logs off and can be reattached to upon logging in again. It is particularly useful if you are connecting to ARCHER2 on an unstable Internet connection or if you wish to keep an arrangement of terminal applications running while you disconnect your client from the Internet -- for example, when moving between your home and workplace.","title":"Connecting to ARCHER2"},{"location":"user-guide/connecting/#connecting-to-archer2","text":"This section covers the basic connection methods. On the ARCHER2 system, interactive access is achieved using SSH, either directly from a command-line terminal or using an SSH client. In addition, data can be transferred to and from the ARCHER2 system using scp from the command line or by using a file-transfer client. Before following the process below, we assume you have set up an account on ARCHER2 through the EPCC SAFE. Documentation on how to do this can be found at: SAFE Guide for Users","title":"Connecting to ARCHER2"},{"location":"user-guide/connecting/#command-line-terminal","text":"","title":"Command line terminal"},{"location":"user-guide/connecting/#linux","text":"Linux distributions include a terminal application that can be used for SSH access to the ARCHER2 login nodes. Linux users will have different terminals depending on their distribution and window manager (e.g., GNOME Terminal in GNOME, Konsole in KDE). Consult your Linux distribution's documentation for details on how to load a terminal.","title":"Linux"},{"location":"user-guide/connecting/#macos","text":"MacOS users can use the Terminal application, located in the Utilities folder within the Applications folder.","title":"MacOS"},{"location":"user-guide/connecting/#windows","text":"A typical Windows installation will not include a terminal client, though there are various clients available. We recommend Windows users download and install MobaXterm to access ARCHER2. It is very easy to use and includes an integrated X Server, which allows you to run graphical applications on ARCHER2. You can download MobaXterm Home Edition (Installer Edition) from the following link: Install MobaXterm Double-click the downloaded Microsoft Installer file (.msi) and follow the instructions from the Windows Installation Wizard. Note, you might need to have administrator rights to install on some versions of Windows. Also, make sure to check whether Windows Firewall has blocked any features of this program after installation (Windows will warn you if the built-in firewall blocks an action, and gives you the opportunity to override the behaviour). Once installed, start MobaXterm and then click \"Start local terminal\". Tips If you download the .zip file rather than the .msi, make sure you unzip it before attempting to run the installer. If you do not have administrator rights, you can use the Portable edition of MobaXterm . If this is your first time using MobaXterm, you should check that a permanent /home directory has been set up (otherwise, all saved info will be lost from session to session). Go to \"Settings\" -> \"Configuration\" and check that a path is set in the field marked \"Persistent home directory\". If prompted, make sure path is set as \"private\". Any SSH key generated in MobaXterm will, by default, be stored in the permanent /home directory (see above). That is, if your /home directory is _MyDocuments_\\MobaXterm\\home then within that folder you will find a folder named _MyDocuments_\\MobaXterm\\home\\.ssh containing your keys. This folder will be 'hidden' by default, so you may need to tick 'Hidden items' under 'View' in Windows Explorer to see it. MobaXterm also allows you to set up pre-configured SSH sessions with the username, login host and key details saved. You are welcome to use this, rather than using the \"Local terminal\", but we are not able to assist with debugging connection issues if you choose this method.","title":"Windows"},{"location":"user-guide/connecting/#access-credentials","text":"To access ARCHER2, you need to use two sets of credentials: a password and an SSH key pair protected by a passphrase. You can find more detailed instructions on how to set up your credentials to access ARCHER2 from Windows, MacOS and Linux below.","title":"Access credentials"},{"location":"user-guide/connecting/#ssh-key-pairs","text":"You will need to generate an SSH key pair protected by a passphrase to access ARCHER2. Using a terminal (the command line), set up a key pair that contains your e-mail address and enter a passphrase you will use to unlock the key: $ ssh-keygen -t rsa -C \"your@email.com\" ... -bash-4.1$ ssh-keygen -t rsa -C \"your@email.com\" Generating public/private rsa key pair. Enter file in which to save the key (/Home/user/.ssh/id_rsa): [Enter] Enter passphrase (empty for no passphrase): [Passphrase] Enter same passphrase again: [Passphrase] Your identification has been saved in /Home/user/.ssh/id_rsa. Your public key has been saved in /Home/user/.ssh/id_rsa.pub. The key fingerprint is: 03:d4:c4:6d:58:0a:e2:4a:f8:73:9a:e8:e3:07:16:c8 your@email.com The key's randomart image is: +--[ RSA 2048]----+ | . ...+o++++. | | . . . =o.. | |+ . . .......o o | |oE . . | |o = . S | |. +.+ . | |. oo | |. . | | .. | +-----------------+ (remember to replace \"your@email.com\" with your e-mail address).","title":"SSH Key Pairs"},{"location":"user-guide/connecting/#upload-public-part-of-key-pair-to-safe","text":"You should now upload the public part of your SSH key pair to the SAFE by following the instructions at: Login to SAFE . Then: Go to the Menu Login accounts and select the ARCHER2 account you want to add the SSH key to. On the subsequent Login Account details page, click the Add Credential button. Select SSH public key as the Credential Type and click Next Either copy and paste the public part of your SSH key into the SSH Public key box or use the button to select the public key file on your computer. Click Add to associate the public SSH key with your account. Once you have done this, your SSH key will be added to your ARCHER2 account. Remember, you need both an SSH key and a password to log in to ARCHER2. You will need to collect an initial password before you can log into ARCHER2. We cover this next. Note If you want to connect to ARCHER2 from more than one machine---for example, from your home laptop as well as your work laptop---you should generate an SSH key on each machine, and add each of the public keys into SAFE.","title":"Upload public part of key pair to SAFE"},{"location":"user-guide/connecting/#initial-passwords","text":"The SAFE web interface is used to provide your initial password for logging onto ARCHER2 (see the SAFE Documentation for more details on requesting accounts and picking up passwords). Note You will be prompted to change your password the first time that you log in to ARCHER2. You may also change your password, at any time, on ARCHER2, using the passwd command. This change is not be reflected in SAFE so, if you forget your password, you should use SAFE to request a new one-shot password.","title":"Initial passwords"},{"location":"user-guide/connecting/#ssh-clients","text":"As noted above, you interact with ARCHER2, over an encrypted communication channel (specifically, Secure Shell version 2 (SSH-2)). This allows command-line access to one of the login nodes of ARCHER2, from which you can run commands or use a command-line text editor to edit files. SSH can also be used to run graphical programs such as GUI text editors and debuggers, when used in conjunction with an X Server.","title":"SSH Clients"},{"location":"user-guide/connecting/#logging-in","text":"The login addresses for ARCHER2 are: ARCHER2 full system: login.archer2.ac.uk You can use the following command from the terminal window to log in to ARCHER2: Full system ssh username@login.archer2.ac.uk The order in which you are asked for credentials depends on the system you are accessing: Full system You will first be prompted for the passphrase associated with your SSH key pair. Once you have entered this passphrase successfully, you will then be prompted for your machine account password. You need to enter both credentials correctly to be able to access ARCHER2. Tip If you previously logged into the 4-cabinet system with your account you may see an error from SSH that looks like @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ @ WARNING: POSSIBLE DNS SPOOFING DETECTED! @ @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ The ECDSA host key for login.archer2.ac.uk has changed, and the key for the corresponding IP address 193.62.216.43 has a different value. This could either mean that DNS SPOOFING is happening or the IP address for the host and its host key have changed at the same time. Offending key for IP in /Users/auser/.ssh/known_hosts:11 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ @ WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED! @ @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY! Someone could be eavesdropping on you right now (man-in-the-middle attack)! It is also possible that a host key has just been changed. The fingerprint for the ECDSA key sent by the remote host is SHA256:UGS+LA8I46LqnD58WiWNlaUFY3uD1WFr+V8RCG09fUg. Please contact your system administrator. If you see this, you should delete the offending host key from your ~/.ssh/known_hosts file (in the example above the offending line is line #11) Warning If your SSH key pair is not stored in the default location (usually ~/.ssh/id_rsa ) on your local system, you may need to specify the path to the private part of the key wih the -i option to ssh . For example, if your key is in a file called keys/id_rsa_ARCHER2 you would use the command ssh -i keys/id_rsa_ARCHER2 username@login.archer2.ac.uk to log in (or the equivalent for the 4-cabinet system). Tip When you first log into ARCHER2, you will be prompted to change your initial password. This is a three-step process: When promoted to enter your ldap password : Re-enter the password you retrieved from SAFE. When prompted to enter your new password: type in a new password. When prompted to re-enter the new password: re-enter the new password. Your password will now have been changed To allow remote programs, especially graphical applications, to control your local display, such as for a debugger, use: Full system ssh -X username@login.archer2.ac.uk Some sites recommend using the -Y flag. While this can fix some compatibility issues, the -X flag is more secure. Current MacOS systems do not have an X window system. Users should install the XQuartz package to allow for SSH with X11 forwarding on MacOS systems: XQuartz website","title":"Logging in"},{"location":"user-guide/connecting/#host-keys","text":"Adding the host keys to your SSH configuration file provides an extra level of security for your connections to ARCHER2. The host keys are checked against the login nodes when you login to ARCHER2 and if the remote server key does not match the one in the configuration file, the connection will be refused. This provides protection against potential malicious servers masquerading as the ARCHER2 login nodes.","title":"Host Keys"},{"location":"user-guide/connecting/#loginarcher2acuk","text":"login.archer2.ac.uk ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBEnMeFf1TPZ4pbupWeD4IeahEeeqJMAhrCv1znyQGAL45yOIArVltscW8GNhzfaWk5vKb9sIAm2mJZPc3b7te3c= login.archer2.ac.uk ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCZKpFN25u13uSTOun8jOKEO+4Y/98DW9/8dxoGYOf8Q7qZEyQUGk5QUuJCiB7ZzCOJ01Lxl+ghYpQ13oiebZWkTWUdypSCBH5f4/y5z+f87fDqOjkHKhpYb90RlpbP+Ik+6IapQOTYKGBPFfwkbp2LYh3ktV7ocpKVCNst0k5IELNufNBgsGNFYNyRYIR6hHoH2kUqDvrN8IXf8085vKbKdMQPdAtIEX7sOX+UNUpR/46zcAyn8VRn/CGA5WA39nKKOiPzJn8pFtKDUmme/DA9/Y+Z/jJS55coHxV81Qws5WYmg7bzgVDkZJvtzQ6haJAOhsWNYzNtrEwNNDCc610z login.archer2.ac.uk ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAII/OY5bYUBnnLr0B7keiT97WtzSGtTsTexpgmxkdCI+b Host key verification can fail if this key is out of date, a problem which can be fixed by removing the offending entry in ~/.ssh/known_hosts and replacing it with the new key published here. We recommend users should check this page for any key updates and not just accept a new key from the server without confirmation.","title":"login.archer2.ac.uk"},{"location":"user-guide/connecting/#making-access-more-convenient-using-the-ssh-configuration-file","text":"Typing in the full command to log in or transfer data to ARCHER2 can become tedious as it often has to be repeated several times. You can use the SSH configuration file, usually located on your local machine at .ssh/config to make the process more convenient. Each remote site (or group of sites) can have an entry in this file, which may look something like: Full system Host archer2 HostName login.archer2.ac.uk User username (remember to replace username with your actual username!). Taking the full-system example: the Host line defines a short name for the entry. In this case, instead of typing ssh username@login.archer2.ac.uk to access the ARCHER2 login nodes, you could use ssh archer2 instead. The remaining lines define the options for the host. Hostname login.archer2.ac.uk --- defines the full address of the host User username --- defines the username to use by default for this host (replace username with your own username on the remote host) Now you can use SSH to access ARCHER2 without needing to enter your username or the full hostname every time: ssh archer2 You can set up as many of these entries as you need in your local configuration file. Other options are available. See the ssh_config manual page (or man ssh_config on any machine with SSH installed) for a description of the SSH configuration file. For example, you may find the IdentityFile option useful if you have to manage multiple SSH key pairs for different systems as this allows you to specify which SSH key to use for each system. Bug There is a known bug with Windows ssh-agent. If you get the error message: Warning: agent returned different signature type ssh-rsa (expected rsa-sha2-512) , you will need to either specify the path to your ssh key in the command line (using the -i option as described above) or add that path to your SSH config file by using the IdentityFile option.","title":"Making access more convenient using the SSH configuration file"},{"location":"user-guide/connecting/#ssh-debugging-tips","text":"If you find you are unable to connect to ARCHER2, there are some simple checks you may use to diagnose the issue, which are described below. If you are having difficulties connecting, we suggest trying these before contacting the ARCHER2 Service Desk.","title":"SSH debugging tips"},{"location":"user-guide/connecting/#use-the-userloginarcher2acuk-syntax-rather-than-l-user-loginarcher2acuk","text":"We have seen a number of instances where people using the syntax ssh -l user login.archer2.ac.uk have not been able to connect properly and get prompted for a password many times. We have found that using the alternative syntax: ssh user@login.archer2.ac.uk works more reliably.","title":"Use the user@login.archer2.ac.uk syntax rather than -l user login.archer2.ac.uk"},{"location":"user-guide/connecting/#can-you-connect-to-the-login-node","text":"Try the command ping -c 3 login.archer2.ac.uk , on Linux or MacOS, or ping -n 3 login.archer2.ac.uk on Windows. If you successfully connect to the login node, the output should include: --- login.archer2.ac.uk ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 38ms (the ping time '38ms' is not important). If not all packets are received there could be a problem with your Internet connection, or the login node could be unavailable.","title":"Can you connect to the login node?"},{"location":"user-guide/connecting/#password","text":"If you are having trouble entering your password, consider using a password manager, from which you can copy and paste it. If you need to reset your password, instructions for doing so can be found in the SAFE documentation Windows users should note that the Ctrl+V shortcut does not work to paste in to PuTTY, MobaXterm, or PowerShell. Instead use Shift+Ins to paste. Alternatively, right-click and select 'Paste' in PuTTY and MobaXterm, or simply right-click to paste in PowerShell.","title":"Password"},{"location":"user-guide/connecting/#ssh-key","text":"If you get the error message Permission denied (publickey) , this may indicate a problem with your SSH key. Some things to check: Have you uploaded the key to SAFE? Please note that if the same key is re-uploaded, SAFE will not map the \"new\" key to ARCHER2. If for some reason this is required, please delete the key first, then re-upload. Is SSH using the correct key? You can check which keys are being found and offered by SSH using ssh -vvv . If your private key has a non-default name, you should use the -i option to provide it to ssh. For example, ssh -i path/to/key username@login.archer2.ac.uk . Are you entering the passphrase correctly? You will be asked for your private key's passphrase first. If you enter it incorrectly you will usually be asked to enter it again (usually you will get three chances, after which SSH will fail with Permission denied (publickey) ). If you would like to confirm your passphrase without attempting to connect, you can use ssh-keygen -y -f /path/to/private/key . If successful, this command will print the corresponding public key. You can also use this to check that you have uploaded the correct public key to SAFE. Are permissions correct on the SSH key? One common issue is that the permissions are set incorrectly on either the key files or the directory it is contained in. On Linux and MacOS, if your private keys are held in ~/.ssh/ you can check this with ls -al ~/.ssh . This should give something similar to the following output: $ ls -al ~/.ssh/ drwx------. 2 user group 48 Jul 15 20:24 . drwx------. 12 user group 4096 Oct 13 12:11 .. -rw-------. 1 user group 113 Jul 15 20:23 authorized_keys -rw-------. 1 user group 12686 Jul 15 20:23 id_rsa -rw-r--r--. 1 user group 2785 Jul 15 20:23 id_rsa.pub -rw-r--r--. 1 user group 1967 Oct 13 14:11 known_hosts The important section here is the string of letters and dashes at the start, for the lines ending in . , id_rsa , and id_rsa.pub , which indicate permissions on the containing directory, private key, and public key, respectively. If your permissions are not correct, they can be set with chmod . Consult the table below for the relevant chmod command. Target Permissions chmod Code Directory drwx------ 700 Private Key -rw------- 600 Public Key -rw-r--r-- 644 chmod can be used to set permissions on the target in the following way: chmod <code> <target> . So for example to set correct permissions on the private key file id_rsa_ARCHER2 , use the command chmod 600 id_rsa_ARCHER2 . On Windows, permissions are handled differently but can be set by right-clicking on the file and selecting Properties > Security > Advanced. The user, SYSTEM, and Administrators should have Full control , and no other permissions should exist for both the public and private key files, as well as the containing folder. Tip Unix file permissions can be understood in the following way. There are three groups that can have file permissions: (owning) users , (owning) groups , and others . The available permissions are read , write , and execute . The first character indicates whether the target is a file - , or directory d . The next three characters indicate the owning user's permissions. The first character is r if they have read permission, - if they don't, the second character is w if they have write permission, - if they don't, the third character is x if they have execute permission, - if they don't. This pattern is then repeated for group , and other permissions. For example the pattern -rw-r--r-- indicates that the owning user can read and write the file, members of the owning group can read it, and anyone else can also read it. The chmod codes are constructed by treating the user, group, and owner permission strings as binary numbers, then converting them to decimal. For example the permission string -rwx------ becomes 111 000 000 -> 700 .","title":"SSH key"},{"location":"user-guide/connecting/#ssh-verbose-output","text":"The verbose-debugging output from ssh can be very useful for diagnosing issues. In particular, it can be used to distinguish between problems with the SSH key and password. To enable verbose output, add the -vvv flag to your SSH command. For example: ssh -vvv username@login.archer2.ac.uk The output is lengthy, but somewhere in there you should see lines similar to the following: debug1: Next authentication method: publickey debug1: Offering public key: RSA SHA256:<key_hash> <path_to_private_key> debug3: send_pubkey_test debug3: send packet: type 50 debug2: we sent a publickey packet, wait for reply debug3: receive packet: type 60 debug1: Server accepts key: pkalg rsa-sha2-512 blen 2071 debug2: input_userauth_pk_ok: fp SHA256:<key_hash> debug3: sign_and_send_pubkey: RSA SHA256:<key_hash> Enter passphrase for key '<path_to_private_key>': debug3: send packet: type 50 debug3: receive packet: type 51 Authenticated with partial success. debug1: Authentications that can continue: password, keyboard-interactive In the text above, you can see which files ssh has checked for private keys, and you can see if any key is accepted. The line Authenticated succeeded indicates that the SSH key has been accepted. By default SSH will go through a list of standard private-key files, as well as any you have specified with -i or a config file. To succeed, one of these private keys needs to match to the public key uploaded to SAFE. If your SSH key passphrase is incorrect, you will be asked to try again up to three times in total, before being disconnected with Permission denied (publickey) . If you enter your passphrase correctly, but still see this error message, please consider the advice under SSH key above. You should next see something similiar to: debug1: Next authentication method: keyboard-interactive debug2: userauth_kbdint debug3: send packet: type 50 debug2: we sent a keyboard-interactive packet, wait for reply debug3: receive packet: type 60 debug2: input_userauth_info_req debug2: input_userauth_info_req: num_prompts 1 Password: debug3: send packet: type 61 debug3: receive packet: type 60 debug2: input_userauth_info_req debug2: input_userauth_info_req: num_prompts 0 debug3: send packet: type 61 debug3: receive packet: type 52 debug1: Authentication succeeded (keyboard-interactive). If you do not see the Password: prompt you may have connection issues, or there could be a problem with the ARCHER2 login nodes. If you do not see Authenticated with partial success it means your password was not accepted. You will be asked to re-enter your password, usually two more times before the connection will be rejected. Consider the suggestions under Password above. If you do see Authenticated with partial success , it means your password was accepted, and your SSH key will now be checked. The equivalent information can be obtained in PuTTY by enabling All Logging in settings.","title":"SSH verbose output"},{"location":"user-guide/connecting/#related-software","text":"","title":"Related Software"},{"location":"user-guide/connecting/#tmux","text":"tmux is a multiplexer application available on the ARCHER2 login nodes. It allows for multiple sessions to be open concurrently and these sessions can be detached and run in the background. Furthermore, sessions will continue to run after a user logs off and can be reattached to upon logging in again. It is particularly useful if you are connecting to ARCHER2 on an unstable Internet connection or if you wish to keep an arrangement of terminal applications running while you disconnect your client from the Internet -- for example, when moving between your home and workplace.","title":"tmux"},{"location":"user-guide/containers/","text":"Containers This page was originally based on the documentation at the University of Sheffield HPC service Designed around the notion of mobility of compute and reproducible science, Singularity enables users to have full control of their operating system environment. This means that a non-privileged user can \"swap out\" the Linux operating system and environment on the host for a Linux OS and environment that they control. So if the host system is running CentOS Linux but your application runs in Ubuntu Linux with a particular software stack, you can create an Ubuntu image, install your software into that image, copy the image to another host (e.g. ARCHER2), and run your application on that host in its native Ubuntu environment. Singularity also allows you to leverage the resources of whatever host you are on. This includes high-speed interconnects (e.g. Slingshot on ARCHER2), file systems (e.g. /home and /work on ARCHER2) and potentially other resources. Note Singularity only supports Linux containers. You cannot create images that use Windows or macOS (this is a restriction of the containerisation model rather than Singularity). Useful Links Singularity website Singularity documentation About Singularity Containers (Images) Similar to Docker, a Singularity container (or, more commonly, image ) is a self-contained software stack. As Singularity does not require a root-level daemon to run its images (as is required by Docker) it is suitable for use on multi-user HPC systems such as ARCHER2. Within the container/image, you have exactly the same permissions as you do in a standard login session on the system. In practice, this means that an image created on your local machine with all your research software installed for local development will also run on ARCHER2. Pre-built images (such as those on DockerHub or SingularityHub archive can simply be downloaded and used on ARCHER2 (or anywhere else Singularity is installed). Creating and modifying images requires root permission and so must be done on a system where you have such access (in practice, this is usually within a virtual machine on your laptop/workstation). Note SingularityHub was a publicly available cloud service for Singularity Containers active from 2016 to 2021. It built container recipes from Github repositories on Google Cloud, and containers were available via the command line Singularity or sregistry software. These containers are still available now in the SingularityHub Archive Using Singularity Images on ARCHER2 Singularity images can be used on ARCHER2 in a number of ways, including: Interactively on the login nodes Interactively on compute nodes As serial processes within a non-interactive batch script As parallel processes within a non-interactive batch script We provide information on each of these scenarios below. First, we describe briefly how to get existing images onto ARCHER2 so that you can use them. Getting existing images onto ARCHER2 Singularity images are files, so, if you already have an image, you can use scp to copy the file to ARCHER2 as you would with any other file. If you wish to get a file from one of the container image repositories then Singularity allows you to do this from ARCHER2 itself. For example, to retrieve an image from SingularityHub on ARCHER2 we can simply issue a Singularity command to pull the image. auser@ln03:~> singularity pull hello-world.sif shub://vsoch/hello-world The image located at the shub URI is written to a Singularity Image File (SIF) called hello-world.sif . Interactive use on the login nodes Once you have an image file, using it on the login nodes in an interactive way is extremely simple: you use the singularity shell command. Using the image we built in the example above: auser@ln03:~> singularity shell hello-world.sif Singularity> Within a Singularity image your home directory will be available. Once you have finished using your image, you can return to the ARCHER2 login node prompt with the exit command: Singularity> exit exit auser@ln03:~> Interactive use on the compute nodes The process for using an image interactively on the compute nodes is very similar to that for the login nodes. The only difference is that you first have to submit an interactive serial job (from a location on /work ) in order to get interactive access to the compute node. For example, to reserve a full node for you to work on interactively you would use: auser@ln03:/work/t01/t01/auser> srun --nodes=1 --exclusive --time=00:20:00 \\ --account=[budget code] \\ --partition=standard --qos=standard \\ --pty /bin/bash ...wait until job starts... auser@nid00001:/work/t01/t01/auser> Note that the prompt has changed to show you are on a compute node. Now you can use the image in the same way as on the login node. auser@nid00001:/work/t01/t01/auser> singularity shell hello-world.sif Singularity> exit exit auser@nid00001:/work/t01/t01/auser> exit auser@ln03:/work/t01/t01/auser> Note We used exit to leave the interactive image shell and then exit again to leave the interactive job on the compute node. Serial processes within a non-interactive batch script You can also use Singularity images within a non-interactive batch script as you would any other command. If your image contains a runscript then you can use singularity run to execute the runscript in the job. You can also use singularity exec to execute arbitrary commands (or scripts) within the image. An example job submission script to run a serial job that executes the runscript within the hello-world.sif image that we downloaded previously to an ARCHER2 login node would be as follows. Full system #!/bin/bash --login # Slurm job options (name, compute nodes, job time) #SBATCH --job-name=helloworld #SBATCH --nodes=1 #SBATCH --ntasks-per-node=1 #SBATCH --cpus-per-task=1 #SBATCH --time=00:10:00 #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Run the serial executable singularity run $SLURM_SUBMIT_DIR /hello-world.sif You submit this in the usual way and the standard output and error should be written to slurm-... , where the output filename ends with the job number. Parallel processes within a non-interactive batch script Running a Singularity container in parallel across a number of compute nodes requires some preparation. In general though, Singularity can be run within the parallel job launcher ( srun ). srun <options> \\ singularity <options> /path/to/image/file \\ app <options> The code snippet above shows the launch command as having three nested parts, srun , the singularity environment and the containerized application. The Singularity image must be compatible with the MPI environment on the host; either, the containerized app has been built against the appropriate MPI libraries or the container itself contains an MPI library that is compatible with the host MPI. The latter situation is known as the hybrid model ; this is the approach taken in the sections that follow. Creating Your Own Singularity Images Note This information is based on that in the Introduction to Singularity lesson from the Carpentries Incubator . Citation J. Cohen and A. Turner. \"Reproducible computational environments using containers: Introduction to Singularity\". Version 2020.08a, August 2020. Carpentries Incubator. https://github.com/carpentries-incubator/singularity-introduction As we saw above, you can create Singularity images by importing from DockerHub or Singularity Hub on ARCHER2 itself. If you wish to create your own custom image using Singularity then you must install Singularity on a system where you have root (or administrator) privileges - often your own laptop or workstation. There are three different options to install Singularity on your local system: install Docker and use the Docker Singularity image to build Singularity containers, install a virtual machine that you can use to build Singularity images, or install Singularity on your local system. For macOS and Windows users we recommend installing Docker Desktop and using the official Singularity image to build your own images. For Linux users, we recommend installing Singularity directly on your local system. We cover the mechanism that uses Docker in more detail below for macOS and Windows users. If your local system is Linux, you can find information on installing Singularity on Linux distribution at: Installing Singularity on Linux Building Singularity images using Docker (macOS/Windows) Tip You should install Docker Desktop to allow you to build Singularity images using Docker. Instructions can be found at: Installing Docker Desktop on Windows Home/Professional Installing Docker Desktop on macOS Once you have installed Docker, you can build Singularity images with a command similar to: docker run -it --privileged --rm -v ${ PWD } :/home/singularity quay.io/singularity/singularity:v3.7.0-slim build /home/singularity/my_image.sif /home/singularity/my_image.def Tip You can setup an alias to the long docker run command above to make it easier to use. For example, if you are using bash or zsh you could use a command like alias dsingularity=\"docker run -it --privileged --rm -v ${PWD}:/home/singularity quay.io/singularity/singularity:v3.7.0-slim\" For information on how to write Singularity image definition files, see the Singularity documentation . Tip You can, of course, also get to a Singularity image via a Docker image (as Singularity can use images directly from the DockerHub). In this workflow you would create a Docker image, upload it to the DockerHub and then pull it using Singularity on ARCHER2. Using Singularity with MPI on ARCHER2 Note This information is based on that in the Introduction to Singularity lesson from the Carpentries Incubator . Citation J. Cohen and A. Turner. \"Reproducible computational environments using containers: Introduction to Singularity\". Version 2020.08a, August 2020. Carpentries Incubator. https://github.com/carpentries-incubator/singularity-introduction MPI on ARCHER2 is provided by the Cray MPICH libraries with the interface to the high-performance Slingshot interconnect provided via the OFI interface. Therefore, as per the Singularity MPI Hybrid model , we will build our Singularity image such that it contains a version of the MPICH MPI library compiled with support for OFI. Below, we provide instructions on creating a container with a version of MPICH compiled in this way, but, for convenience, we also provide a base image file that already has a suitable MPICH, and so, you can skip this step if you wish. Instructions on how to use this image as the base for your own images are also provided. Finally, we provide an example of how to run a Singularity container with MPI over multiple ARCHER2 compute nodes. Building an image with MPI from scratch Note These instructions are based on those used in the Reproducible computational environments using containers: Introduction to Singularity . Warning Remember, all these steps should be executed on your local system where you have administrator privileges, not on ARCHER2 . We will illustrate the process of building a Singularity image with MPI from scratch by building an image that contains MPI provided by MPICH and the OSU MPI benchmarks. In order to do this, we first need to download the source code for both MPICH and the OSU benchmarks. At the time of writing, the stable MPICH release is 3.4.2 and the stable OSU benchmark release is 5.8 - this may have changed by the time you are following these instructions. Go ahead and download the source code: wget http://www.mpich.org/static/downloads/3.4.2/mpich-3.4.2.tar.gz wget https://mvapich.cse.ohio-state.edu/download/mvapich/osu-micro-benchmarks-5.8.tgz Now create a Singularity image definition file that describes how to build the image: Bootstrap: docker From: ubuntu:20.04 %files /home/singularity/osu-micro-benchmarks-5.8.tar.gz /root/ /home/singularity/mpich-3.4.2.tar.gz /root/ %environment export SINGULARITY_MPICH_DIR=/usr %post apt-get -y update && DEBIAN_FRONTEND=noninteractive apt-get -y install build-essential libfabric-dev libibverbs-dev gfortran cd /root tar zxvf mpich-3.4.2.tar.gz && cd mpich-3.4.2 echo \"Configuring and building MPICH...\" ./configure --prefix=/usr --with-device=ch3:nemesis:ofi && make -j2 && make install cd /root tar zxvf osu-micro-benchmarks-5.8.tar.gz cd osu-micro-benchmarks-5.8/ echo \"Configuring and building OSU Micro-Benchmarks...\" ./configure --prefix=/usr/local/osu CC=/usr/bin/mpicc CXX=/usr/bin/mpicxx make -j2 && make install %runscript exec /usr/local/osu/libexec/osu-micro-benchmarks/mpi/$* A quick overview of what the above definition file is doing: The image is being bootstrapped from the ubuntu:20.04 Docker image. In the %files section: The OSU Micro-Benchmarks and MPICH tar files are copied from the current directory into the /root directory in the image. In the %environment section: Set an environment variable that will be available within all containers run from the generated image. In the %post section: Ubuntu's apt-get package manager is used to update the package directory and then install the compilers and other libraries required for the MPICH build. The MPICH .tar.gz file is extracted and the configure, build and install steps are run. Note the use of the --with-device option to configure MPICH to use the correct driver to support improved communication performance on a high performance cluster. The OSU Micro-Benchmarks tar.gz file is extracted and the configure, build and install steps are run to build the benchmark code from source. In the %runscript section: A runscript is set up that will echo the rank number of the current process and then run the command provided as a command line argument. Note that the base path of the the executable to run is hardcoded in the run script , so the command line parameter to provide when running a container based on this image is relative to this base path, for example, startup/osu_hello , collective/osu_allgather , pt2pt/osu_latency , one-sided/osu_put_latency . Info You can find more information on Singularity definition file syntax in the Singularity documentation . Now go ahead and build the Singularity image using Singularity via Docker: docker run -it --privileged --rm -v ${ PWD } :/home/singularity quay.io/singularity/singularity:v3.7.0-slim build /home/singularity/osu_benchmarks.sif /home/singularity/osu_benchmarks.def Once you have successfully created your Singularity image file, osu_benchmarks.sif , use scp to copy it to ARCHER2 and you can test as described in the section below. Tip You can setup an alias to the long docker run command above to make it easier to use. For example, if you are using bash or zsh you could use a command like alias dsingularity=\"docker run -it --privileged --rm -v ${PWD}:/home/singularity quay.io/singularity/singularity:v3.7.0-slim\" Tip You can find a copy of the osu_benchmarks.sif image on ARCHER2 in the directory $EPCC_SINGULARITY_DIR if you do not want to build it yourself but still want to test. Creating an image based on the base ARCHER2 MPI Singularity image We have built an image with MPICH 3.4.1 built against OFI that you can use as a base image to install further software and create your own images. The image can be found on ARCHER2 at $EPCC_SINGULARITY_DIR/mpich_base.sif . To use this image in your own image builds you should download it from ARCHER2 to the system where you are building your images. You can then add the following lines to your image definition file to start from this base image: Bootstrap: localimage From: /path/to/container/mpich_base.sif (Remember to put the actual path to the mpich_base.sif image file that you have downloaded to your system.) Running parallel MPI jobs using Singularity containers Tip These instructions assume you have a Singularity image uploaded to ARCHER2 that includes MPI provided by MPICH with the OFI interface. See the sections above for how to create such images. Once you have uploaded your Singularity image that includes MPICH built with OFI for ARCHER2, you can use it to run parallel jobs in a similar way to non-Singularity jobs. The example job submission script below uses the image we built above with MPICH and the OSU benchmarks to run the Allreduce benchmark on two nodes where all 128 cores on each node are used for MPI processes (so, 256 MPI processes in total). Full system #!/bin/bash # Slurm job options (name, compute nodes, job time) #SBATCH --job-name=singularity_parallel #SBATCH --time=0:10:0 #SBATCH --nodes=2 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 # Replace [budget code] below with your budget code (e.g. t01) #SBATCH --partition=standard #SBATCH --qos=standard #SBATCH --account=[budget code] # Set the number of threads to 1. # This prevents any threaded system libraries from automatically using threading. export OMP_NUM_THREADS = 1 # Set the LD_LIBRARY_PATH environment variable within the Singularity container # to ensure that it used the correct MPI libraries. export SINGULARITYENV_LD_LIBRARY_PATH = /opt/cray/pe/mpich/8.1.4/ofi/gnu/9.1/lib-abi-mpich:/opt/cray/pe/pmi/6.0.10/lib:/opt/cray/libfabric/1.11.0.4.71/lib64:/usr/lib64/host:/usr/lib/x86_64-linux-gnu/libibverbs:/.singularity.d/libs:/opt/cray/pe/gcc-libs # This makes sure HPE Cray Slingshot interconnect libraries are available # from inside the container. export SINGULARITY_BIND = \"/opt/cray,/usr/lib64/libibverbs.so.1,/usr/lib64/librdmacm.so.1, /usr/lib64/libnl-3.so.200,/usr/lib64/libnl-route-3.so.200,/usr/lib64/libpals.so.0, /var/spool/slurmd/mpi_cray_shasta,/usr/lib64/libibverbs/libmlx5-rdmav25.so,/etc/libibverbs.d,/opt/gcc\" # Launch the parallel job. srun --hint = nomultithread --distribution = block:block \\ singularity run osu_benchmarks.sif \\ collective/osu_allreduce The only changes from a standard submission script are: We set the environment variable SINGULARITY_LD_LIBRARY_PATH to ensure that the excutable can find the correct libraries are available within the container to be able to use HPE Cray Slingshot interconnect. We set the environment variable SINGULARITY_BIND to ensure that the correct libraries are available within the container to be able to use HPE Cray Slingshot interconnect. srun calls the singularity software with the image file we created rather than the parallel program directly. Important Remember that the image file must be located on /work to run jobs on the compute nodes. If the job runs correctly, you should see output similar to the following in your slurm-*.out file: # OSU MPI Allreduce Latency Test v5.7 # Size Avg Latency(us) 4 6.81 8 6.86 16 6.90 32 7.09 64 10.79 128 10.97 256 13.07 512 15.50 1024 17.35 2048 22.49 4096 41.13 8192 54.00 16384 75.09 32768 120.40 65536 243.26 131072 614.60 262144 618.11 524288 877.92 1048576 2981.61","title":"Containers"},{"location":"user-guide/containers/#containers","text":"This page was originally based on the documentation at the University of Sheffield HPC service Designed around the notion of mobility of compute and reproducible science, Singularity enables users to have full control of their operating system environment. This means that a non-privileged user can \"swap out\" the Linux operating system and environment on the host for a Linux OS and environment that they control. So if the host system is running CentOS Linux but your application runs in Ubuntu Linux with a particular software stack, you can create an Ubuntu image, install your software into that image, copy the image to another host (e.g. ARCHER2), and run your application on that host in its native Ubuntu environment. Singularity also allows you to leverage the resources of whatever host you are on. This includes high-speed interconnects (e.g. Slingshot on ARCHER2), file systems (e.g. /home and /work on ARCHER2) and potentially other resources. Note Singularity only supports Linux containers. You cannot create images that use Windows or macOS (this is a restriction of the containerisation model rather than Singularity).","title":"Containers"},{"location":"user-guide/containers/#useful-links","text":"Singularity website Singularity documentation","title":"Useful Links"},{"location":"user-guide/containers/#about-singularity-containers-images","text":"Similar to Docker, a Singularity container (or, more commonly, image ) is a self-contained software stack. As Singularity does not require a root-level daemon to run its images (as is required by Docker) it is suitable for use on multi-user HPC systems such as ARCHER2. Within the container/image, you have exactly the same permissions as you do in a standard login session on the system. In practice, this means that an image created on your local machine with all your research software installed for local development will also run on ARCHER2. Pre-built images (such as those on DockerHub or SingularityHub archive can simply be downloaded and used on ARCHER2 (or anywhere else Singularity is installed). Creating and modifying images requires root permission and so must be done on a system where you have such access (in practice, this is usually within a virtual machine on your laptop/workstation). Note SingularityHub was a publicly available cloud service for Singularity Containers active from 2016 to 2021. It built container recipes from Github repositories on Google Cloud, and containers were available via the command line Singularity or sregistry software. These containers are still available now in the SingularityHub Archive","title":"About Singularity Containers (Images)"},{"location":"user-guide/containers/#using-singularity-images-on-archer2","text":"Singularity images can be used on ARCHER2 in a number of ways, including: Interactively on the login nodes Interactively on compute nodes As serial processes within a non-interactive batch script As parallel processes within a non-interactive batch script We provide information on each of these scenarios below. First, we describe briefly how to get existing images onto ARCHER2 so that you can use them.","title":"Using Singularity Images on ARCHER2"},{"location":"user-guide/containers/#getting-existing-images-onto-archer2","text":"Singularity images are files, so, if you already have an image, you can use scp to copy the file to ARCHER2 as you would with any other file. If you wish to get a file from one of the container image repositories then Singularity allows you to do this from ARCHER2 itself. For example, to retrieve an image from SingularityHub on ARCHER2 we can simply issue a Singularity command to pull the image. auser@ln03:~> singularity pull hello-world.sif shub://vsoch/hello-world The image located at the shub URI is written to a Singularity Image File (SIF) called hello-world.sif .","title":"Getting existing images onto ARCHER2"},{"location":"user-guide/containers/#interactive-use-on-the-login-nodes","text":"Once you have an image file, using it on the login nodes in an interactive way is extremely simple: you use the singularity shell command. Using the image we built in the example above: auser@ln03:~> singularity shell hello-world.sif Singularity> Within a Singularity image your home directory will be available. Once you have finished using your image, you can return to the ARCHER2 login node prompt with the exit command: Singularity> exit exit auser@ln03:~>","title":"Interactive use on the login nodes"},{"location":"user-guide/containers/#interactive-use-on-the-compute-nodes","text":"The process for using an image interactively on the compute nodes is very similar to that for the login nodes. The only difference is that you first have to submit an interactive serial job (from a location on /work ) in order to get interactive access to the compute node. For example, to reserve a full node for you to work on interactively you would use: auser@ln03:/work/t01/t01/auser> srun --nodes=1 --exclusive --time=00:20:00 \\ --account=[budget code] \\ --partition=standard --qos=standard \\ --pty /bin/bash ...wait until job starts... auser@nid00001:/work/t01/t01/auser> Note that the prompt has changed to show you are on a compute node. Now you can use the image in the same way as on the login node. auser@nid00001:/work/t01/t01/auser> singularity shell hello-world.sif Singularity> exit exit auser@nid00001:/work/t01/t01/auser> exit auser@ln03:/work/t01/t01/auser> Note We used exit to leave the interactive image shell and then exit again to leave the interactive job on the compute node.","title":"Interactive use on the compute nodes"},{"location":"user-guide/containers/#serial-processes-within-a-non-interactive-batch-script","text":"You can also use Singularity images within a non-interactive batch script as you would any other command. If your image contains a runscript then you can use singularity run to execute the runscript in the job. You can also use singularity exec to execute arbitrary commands (or scripts) within the image. An example job submission script to run a serial job that executes the runscript within the hello-world.sif image that we downloaded previously to an ARCHER2 login node would be as follows. Full system #!/bin/bash --login # Slurm job options (name, compute nodes, job time) #SBATCH --job-name=helloworld #SBATCH --nodes=1 #SBATCH --ntasks-per-node=1 #SBATCH --cpus-per-task=1 #SBATCH --time=00:10:00 #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Run the serial executable singularity run $SLURM_SUBMIT_DIR /hello-world.sif You submit this in the usual way and the standard output and error should be written to slurm-... , where the output filename ends with the job number.","title":"Serial processes within a non-interactive batch script"},{"location":"user-guide/containers/#parallel-processes-within-a-non-interactive-batch-script","text":"Running a Singularity container in parallel across a number of compute nodes requires some preparation. In general though, Singularity can be run within the parallel job launcher ( srun ). srun <options> \\ singularity <options> /path/to/image/file \\ app <options> The code snippet above shows the launch command as having three nested parts, srun , the singularity environment and the containerized application. The Singularity image must be compatible with the MPI environment on the host; either, the containerized app has been built against the appropriate MPI libraries or the container itself contains an MPI library that is compatible with the host MPI. The latter situation is known as the hybrid model ; this is the approach taken in the sections that follow.","title":"Parallel processes within a non-interactive batch script"},{"location":"user-guide/containers/#creating-your-own-singularity-images","text":"Note This information is based on that in the Introduction to Singularity lesson from the Carpentries Incubator . Citation J. Cohen and A. Turner. \"Reproducible computational environments using containers: Introduction to Singularity\". Version 2020.08a, August 2020. Carpentries Incubator. https://github.com/carpentries-incubator/singularity-introduction As we saw above, you can create Singularity images by importing from DockerHub or Singularity Hub on ARCHER2 itself. If you wish to create your own custom image using Singularity then you must install Singularity on a system where you have root (or administrator) privileges - often your own laptop or workstation. There are three different options to install Singularity on your local system: install Docker and use the Docker Singularity image to build Singularity containers, install a virtual machine that you can use to build Singularity images, or install Singularity on your local system. For macOS and Windows users we recommend installing Docker Desktop and using the official Singularity image to build your own images. For Linux users, we recommend installing Singularity directly on your local system. We cover the mechanism that uses Docker in more detail below for macOS and Windows users. If your local system is Linux, you can find information on installing Singularity on Linux distribution at: Installing Singularity on Linux","title":"Creating Your Own Singularity Images"},{"location":"user-guide/containers/#building-singularity-images-using-docker-macoswindows","text":"Tip You should install Docker Desktop to allow you to build Singularity images using Docker. Instructions can be found at: Installing Docker Desktop on Windows Home/Professional Installing Docker Desktop on macOS Once you have installed Docker, you can build Singularity images with a command similar to: docker run -it --privileged --rm -v ${ PWD } :/home/singularity quay.io/singularity/singularity:v3.7.0-slim build /home/singularity/my_image.sif /home/singularity/my_image.def Tip You can setup an alias to the long docker run command above to make it easier to use. For example, if you are using bash or zsh you could use a command like alias dsingularity=\"docker run -it --privileged --rm -v ${PWD}:/home/singularity quay.io/singularity/singularity:v3.7.0-slim\" For information on how to write Singularity image definition files, see the Singularity documentation . Tip You can, of course, also get to a Singularity image via a Docker image (as Singularity can use images directly from the DockerHub). In this workflow you would create a Docker image, upload it to the DockerHub and then pull it using Singularity on ARCHER2.","title":"Building Singularity images using Docker (macOS/Windows)"},{"location":"user-guide/containers/#using-singularity-with-mpi-on-archer2","text":"Note This information is based on that in the Introduction to Singularity lesson from the Carpentries Incubator . Citation J. Cohen and A. Turner. \"Reproducible computational environments using containers: Introduction to Singularity\". Version 2020.08a, August 2020. Carpentries Incubator. https://github.com/carpentries-incubator/singularity-introduction MPI on ARCHER2 is provided by the Cray MPICH libraries with the interface to the high-performance Slingshot interconnect provided via the OFI interface. Therefore, as per the Singularity MPI Hybrid model , we will build our Singularity image such that it contains a version of the MPICH MPI library compiled with support for OFI. Below, we provide instructions on creating a container with a version of MPICH compiled in this way, but, for convenience, we also provide a base image file that already has a suitable MPICH, and so, you can skip this step if you wish. Instructions on how to use this image as the base for your own images are also provided. Finally, we provide an example of how to run a Singularity container with MPI over multiple ARCHER2 compute nodes.","title":"Using Singularity with MPI on ARCHER2"},{"location":"user-guide/containers/#building-an-image-with-mpi-from-scratch","text":"Note These instructions are based on those used in the Reproducible computational environments using containers: Introduction to Singularity . Warning Remember, all these steps should be executed on your local system where you have administrator privileges, not on ARCHER2 . We will illustrate the process of building a Singularity image with MPI from scratch by building an image that contains MPI provided by MPICH and the OSU MPI benchmarks. In order to do this, we first need to download the source code for both MPICH and the OSU benchmarks. At the time of writing, the stable MPICH release is 3.4.2 and the stable OSU benchmark release is 5.8 - this may have changed by the time you are following these instructions. Go ahead and download the source code: wget http://www.mpich.org/static/downloads/3.4.2/mpich-3.4.2.tar.gz wget https://mvapich.cse.ohio-state.edu/download/mvapich/osu-micro-benchmarks-5.8.tgz Now create a Singularity image definition file that describes how to build the image: Bootstrap: docker From: ubuntu:20.04 %files /home/singularity/osu-micro-benchmarks-5.8.tar.gz /root/ /home/singularity/mpich-3.4.2.tar.gz /root/ %environment export SINGULARITY_MPICH_DIR=/usr %post apt-get -y update && DEBIAN_FRONTEND=noninteractive apt-get -y install build-essential libfabric-dev libibverbs-dev gfortran cd /root tar zxvf mpich-3.4.2.tar.gz && cd mpich-3.4.2 echo \"Configuring and building MPICH...\" ./configure --prefix=/usr --with-device=ch3:nemesis:ofi && make -j2 && make install cd /root tar zxvf osu-micro-benchmarks-5.8.tar.gz cd osu-micro-benchmarks-5.8/ echo \"Configuring and building OSU Micro-Benchmarks...\" ./configure --prefix=/usr/local/osu CC=/usr/bin/mpicc CXX=/usr/bin/mpicxx make -j2 && make install %runscript exec /usr/local/osu/libexec/osu-micro-benchmarks/mpi/$* A quick overview of what the above definition file is doing: The image is being bootstrapped from the ubuntu:20.04 Docker image. In the %files section: The OSU Micro-Benchmarks and MPICH tar files are copied from the current directory into the /root directory in the image. In the %environment section: Set an environment variable that will be available within all containers run from the generated image. In the %post section: Ubuntu's apt-get package manager is used to update the package directory and then install the compilers and other libraries required for the MPICH build. The MPICH .tar.gz file is extracted and the configure, build and install steps are run. Note the use of the --with-device option to configure MPICH to use the correct driver to support improved communication performance on a high performance cluster. The OSU Micro-Benchmarks tar.gz file is extracted and the configure, build and install steps are run to build the benchmark code from source. In the %runscript section: A runscript is set up that will echo the rank number of the current process and then run the command provided as a command line argument. Note that the base path of the the executable to run is hardcoded in the run script , so the command line parameter to provide when running a container based on this image is relative to this base path, for example, startup/osu_hello , collective/osu_allgather , pt2pt/osu_latency , one-sided/osu_put_latency . Info You can find more information on Singularity definition file syntax in the Singularity documentation . Now go ahead and build the Singularity image using Singularity via Docker: docker run -it --privileged --rm -v ${ PWD } :/home/singularity quay.io/singularity/singularity:v3.7.0-slim build /home/singularity/osu_benchmarks.sif /home/singularity/osu_benchmarks.def Once you have successfully created your Singularity image file, osu_benchmarks.sif , use scp to copy it to ARCHER2 and you can test as described in the section below. Tip You can setup an alias to the long docker run command above to make it easier to use. For example, if you are using bash or zsh you could use a command like alias dsingularity=\"docker run -it --privileged --rm -v ${PWD}:/home/singularity quay.io/singularity/singularity:v3.7.0-slim\" Tip You can find a copy of the osu_benchmarks.sif image on ARCHER2 in the directory $EPCC_SINGULARITY_DIR if you do not want to build it yourself but still want to test.","title":"Building an image with MPI from scratch"},{"location":"user-guide/containers/#creating-an-image-based-on-the-base-archer2-mpi-singularity-image","text":"We have built an image with MPICH 3.4.1 built against OFI that you can use as a base image to install further software and create your own images. The image can be found on ARCHER2 at $EPCC_SINGULARITY_DIR/mpich_base.sif . To use this image in your own image builds you should download it from ARCHER2 to the system where you are building your images. You can then add the following lines to your image definition file to start from this base image: Bootstrap: localimage From: /path/to/container/mpich_base.sif (Remember to put the actual path to the mpich_base.sif image file that you have downloaded to your system.)","title":"Creating an image based on the base ARCHER2 MPI Singularity image"},{"location":"user-guide/containers/#running-parallel-mpi-jobs-using-singularity-containers","text":"Tip These instructions assume you have a Singularity image uploaded to ARCHER2 that includes MPI provided by MPICH with the OFI interface. See the sections above for how to create such images. Once you have uploaded your Singularity image that includes MPICH built with OFI for ARCHER2, you can use it to run parallel jobs in a similar way to non-Singularity jobs. The example job submission script below uses the image we built above with MPICH and the OSU benchmarks to run the Allreduce benchmark on two nodes where all 128 cores on each node are used for MPI processes (so, 256 MPI processes in total). Full system #!/bin/bash # Slurm job options (name, compute nodes, job time) #SBATCH --job-name=singularity_parallel #SBATCH --time=0:10:0 #SBATCH --nodes=2 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 # Replace [budget code] below with your budget code (e.g. t01) #SBATCH --partition=standard #SBATCH --qos=standard #SBATCH --account=[budget code] # Set the number of threads to 1. # This prevents any threaded system libraries from automatically using threading. export OMP_NUM_THREADS = 1 # Set the LD_LIBRARY_PATH environment variable within the Singularity container # to ensure that it used the correct MPI libraries. export SINGULARITYENV_LD_LIBRARY_PATH = /opt/cray/pe/mpich/8.1.4/ofi/gnu/9.1/lib-abi-mpich:/opt/cray/pe/pmi/6.0.10/lib:/opt/cray/libfabric/1.11.0.4.71/lib64:/usr/lib64/host:/usr/lib/x86_64-linux-gnu/libibverbs:/.singularity.d/libs:/opt/cray/pe/gcc-libs # This makes sure HPE Cray Slingshot interconnect libraries are available # from inside the container. export SINGULARITY_BIND = \"/opt/cray,/usr/lib64/libibverbs.so.1,/usr/lib64/librdmacm.so.1, /usr/lib64/libnl-3.so.200,/usr/lib64/libnl-route-3.so.200,/usr/lib64/libpals.so.0, /var/spool/slurmd/mpi_cray_shasta,/usr/lib64/libibverbs/libmlx5-rdmav25.so,/etc/libibverbs.d,/opt/gcc\" # Launch the parallel job. srun --hint = nomultithread --distribution = block:block \\ singularity run osu_benchmarks.sif \\ collective/osu_allreduce The only changes from a standard submission script are: We set the environment variable SINGULARITY_LD_LIBRARY_PATH to ensure that the excutable can find the correct libraries are available within the container to be able to use HPE Cray Slingshot interconnect. We set the environment variable SINGULARITY_BIND to ensure that the correct libraries are available within the container to be able to use HPE Cray Slingshot interconnect. srun calls the singularity software with the image file we created rather than the parallel program directly. Important Remember that the image file must be located on /work to run jobs on the compute nodes. If the job runs correctly, you should see output similar to the following in your slurm-*.out file: # OSU MPI Allreduce Latency Test v5.7 # Size Avg Latency(us) 4 6.81 8 6.86 16 6.90 32 7.09 64 10.79 128 10.97 256 13.07 512 15.50 1024 17.35 2048 22.49 4096 41.13 8192 54.00 16384 75.09 32768 120.40 65536 243.26 131072 614.60 262144 618.11 524288 877.92 1048576 2981.61","title":"Running parallel MPI jobs using Singularity containers"},{"location":"user-guide/data-migration/","text":"Data migration from ARCHER to ARCHER2 This content has been moved to archer-migration/data-migration","title":"Data migration from ARCHER to ARCHER2"},{"location":"user-guide/data-migration/#data-migration-from-archer-to-archer2","text":"This content has been moved to archer-migration/data-migration","title":"Data migration from ARCHER to ARCHER2"},{"location":"user-guide/data/","text":"Data management and transfer This section covers best practice and tools for data management on ARCHER2 along with a description of the different storage available on the service. The IO section has information on achieving good performance for reading and writing data to the ARCHER2 storage along with information and advice on different IO patterns. Information If you have any questions on data management and transfer please do not hesitate to contact the ARCHER2 service desk at support@archer2.ac.uk . Useful resources and links Harry Mangalam's guide on How to transfer large amounts of data via network . This provides lots of useful advice on transferring data. Data management We strongly recommend that you give some thought to how you use the various data storage facilities that are part of the ARCHER2 service. This will not only allow you to use the machine more effectively but also to ensure that your valuable data is protected. Here are the main points you should consider: Not all data are created equal, understand your data. Know what data you have. What is your critical data that needs to be copied to a secure location? Which data do you need in a different location to analyse? Which data would it be easier to regenerate rather than transfer? You should create a brief data management plan laying this out as this will allow you to understand which tools to use and when. Minimise the data you are transferring. Transferring large amounts of data is costly in both researcher time and actual time. Make sure you are only transferring the data you need to transfer. Minimise the number of files you are transferring. Each individual file has a static overhead in data transfers so it is efficient to bundle multiple files together into a single large archive file for transfer. Does compression help or hinder? Many tools have the option to use compression (e.g. rsync , tar , zip ) and generally encourage you to use them to reduce data volumes. However, in some cases, the time spent compressing the data can take longer than actually transferring the uncompressed data; particularly when transferring data between two locations that both have large data transfer bandwidth available. Be aware of encryption overheads. When transferring data using scp (and rsync over scp ) your data will be encrypted introducing a static overhead per file. This issue can be minimised by reducing the number files to be transferred by creating archives. You can also change the encryption algorithm to one that involves minimal encryption. The fastest performing cipher that is commonly available in SSH at the moment is generally aes128-ctr as most common processors provide a hardware implementation. ARCHER2 storage The ARCHER2 service, like many HPC systems, has a complex structure. There are a number of different data storage types available to users: Home file systems Work file systems RDFaaS (RDF as a Service) file systems ( /epsrc and /general ) Each type of storage has different characteristics and policies, and is suitable for different types of use. There are also two different types of node available to users: Login nodes Compute nodes Data analysis nodes Each type of node sees a different combination of the storage types. The following table shows which storage options are avalable on different node types: Storage Login Nodes Compute Nodes Data analysis nodes Notes /home yes no yes Backed up /work yes yes yes Not backed up, high performance RDFaaS yes no yes Backed up, high performance. Only available for projects that moved from ARCHER to ARCHER2. Home file systems There are four independent home file-systems. Every project has an allocation on one of the four. You do not need to know which one your project uses as your projects space can always be accessed via the path /home/project-code . Each home file-system is approximately 100 TB in size and is implemented using standard Network Attached Storage (NAS) technology. This means that these disks are not particularly high performance but are well suited to standard operations like compilation and file editing. These file systems are visible from the ARCHER2 login nodes. Accessing backups of home file systems The home file systems are fully backed up . Full backups are taken weekly (for each of the past two weeks), daily (for each of the past two days) and hourly (for each of the last 6 hours). You can access the snapshots at the /home1/.snapshot , /home2/.snapshot , /home3/.snapshot and /home4/.snapshot depending on which of the file systems you have your home directories on. You can find out which file system your home directory is on with the command: readlink -f $HOME These file-systems are a good location to keep source-code, copies of scripts and compiled binaries. Small amounts of important data can also be copied here for safe keeping though the file systems are not fast enough to manipulate large datasets effectively. Quotas on home file systems All projects are assigned a quota on the home file systems. The project PI or manager can split this quota up between users or groups of users if they wish. You can view any home file system quotas that apply to your account by logging into SAFE and navigating to the page for your ARCHER2 login account. Log into SAFE Use the \"Login accounts\" menu and select your ARCHER2 login account The \"Login account details\" table lists any user or group quotas that are linked with your account. (If there is no quota shown for a row then you have an unlimited quota for that item, but you may still may be limited by another quota.) Tip Quota and usage data on SAFE is updated twice daily so may not be exactly up to date with the situation on the systems themselves. Work file systems There are currently three work file systems on the full ARCHER2 service. Each of these file systems is 3.4 PB and a portion of one of these file systems is available to each project. All of these are high-performance, Lustre parallel file systems. They are designed to support data in large files. The performance for data stored in large numbers of small files is probably not going to be as good. These are the only file systems that are available on the compute nodes so all data read or written by jobs running on the compute nodes has to be hosted here. Warning There are no backups of any data on the work file systems. You should not rely on these file systems for long term storage. Ideally, these file systems should only contain data that is: actively in use; recently generated and in the process of being saved elsewhere; or being made ready for up-coming work. In practice it may be convenient to keep copies of datasets on the work file systems that you know will be needed at a later date. However, make sure that important data is always backed up elsewhere and that your work would not be significantly impacted if the data on the work file systems was lost. Large data sets can be moved to the RDF storage or transferred off the ARCHER2 service entirely. If you have data on the work file systems that you are not going to need in the future please delete it. Quotas on the work file systems As for the home file systems, all projects are assigned a quota on the work file systems. The project PI or manager can split this quota up between users or groups of users if they wish. You can view any work file system quotas that apply to your account by logging into SAFE and navigating to the page for your ARCHER2 login account. Log into SAFE Use the \"Login accounts\" menu and select your ARCHER2 login account The \"Login account details\" table lists any user or group quotas that are linked with your account. (If there is no quota shown for a row then you have an unlimited quota for that item, but you may still may be limited by another quota.) Tip Quota and usage data on SAFE is updated twice daily so may not be exactly up to date with the situation on the systems themselves. You can also examine up to date quotas and usage on the ARCHER2 systems themselves using the lfs quota command. To do this: Change directory to the work directory where you want to check the quota. For example, if I wanted to check the quota for user auser in project t01 then I would: cd /work/t01/t01/auser To check your user quota, you would use the command: auser@ln03:/work/t01/t01/auser> lfs quota -hu auser . Disk quotas for usr auser (uid 5496): Filesystem used quota limit grace files quota limit grace . 1.366G 0k 0k - 5486 0 0 - uid 5496 is using default block quota setting uid 5496 is using default file quota setting the quota and limit of 0k here indicate that no user quota is set for this user To check your project quota, you would use the command: auser@ln03:/work/t01/t01/auser> lfs quota -hp $(id -g) . Disk quotas for prj 1009 (pid 1009): Filesystem used quota limit grace files quota limit grace . 2.905G 0k 0k - 25300 0 0 - pid 1009 is using default block quota setting pid 1009 is using default file quota setting RDFaaS file system The data that was available on the RDF /epsrc and /general file systems is available on ARCHER2 via the RDFaaS (RDF as a Service) file system. If you have requested the same username in the same project as you had on ARCHER then you will be able to access your data at either: /epsrc/<project-code> or /general/<project-code> depending on which file system it was in on the RDF file systems. For example, if your username is auser and you are in the e05 project, then your RDF data will be on the RDFaaS file system at: /epsrc/e05/e05/auser The RDFaaS file systems are only available on the ARCHER2 login nodes. Important The data on the RDFaaS file system is currently available in read-only mode . You need to transfer data from the RDFaaS file system to /work (or /home if it is a small amount of data) if you wish to alter it or use it on the compute nodes. You can, of course, also use scp to transfer data from the RDFaaS file system to another system. Note We plan to make the RDFaaS file system read/write once the full ARCHER2 service is available. Tip If you are having issues accessing data on the RDFaaS file system then please contact the ARCHER2 Service Desk Copying data from RDFaaS to Work file systems You should use the standard Linux cp command to copy data from the RDFaaS file system to other ARCHER2 file systems (usually /work ). For example, to transfer the file important-data.tar.gz from the RDFaaS file system to /work you would use the following command (assuming you are user auser in project e05 ): cp /epsrc/e05/e05/auser/important-data.tar.gz /work/e05/e05/auser/ (remember to replace the project code and username with your own username and project code. You may also need to use /general if your data was there on the RDF file systems). Subprojects Some large projects may choose to split their resources into multiple subprojects. These subprojects will have identifiers appended to the main project ID. For example, the rse subgroup of the z19 project would have the ID z19-rse . If the main project has allocated storage quotas to the subproject the directories for this storage will be found at, for example: /home/z19/z19-rse/auser Your Linux home directory will generally not be changed when you are made a member of a subproject so you must change directories manually (or change the ownership of files) to make use of this different storage quota allocation. Sharing data with other ARCHER2 users How you share data with other ARCHER2 users depends on whether or not they belong to the same project as you. Each project has two shared folders that can be used for sharing data. Sharing data with ARCHER2 users in your project Each project has an inner shared folder. /work/[project code]/[project code]/shared This folder has read/write permissions for all project members. You can place any data you wish to share with other project members in this directory. For example, if your project code is x01 the inner shared folder would be located at /work/x01/x01/shared . Sharing data with all ARCHER2 users Each project also has an outer shared folder.: /work/[project code]/shared It is writable by all project members and readable by any user on the system. You can place any data you wish to share with other ARCHER2 users who are not members of your project in this directory. For example, if your project code is x01 the outer shared folder would be located at /work/x01/shared . Permissions You should check the permissions of any files that you place in the shared area, especially if those files were created in your own ARCHER2 account. Files of the latter type are likely to be readable by you only. The chmod command below shows how to make sure that a file placed in the outer shared folder is also readable by all ARCHER2 users. chmod a+r /work/x01/shared/your-shared-file.txt Similarly, for the inner shared folder, chmod can be called such that read permission is granted to all users within the x01 project. chmod g+r /work/x01/x01/shared/your-shared-file.txt If you're sharing a set of files stored within a folder hierarchy the chmod is slightly more complicated. chmod -R a+Xr /work/x01/shared/my-shared-folder chmod -R g+Xr /work/x01/x01/shared/my-shared-folder The -R option ensures that the read permission is enabled recursively and the +X guarantees that the user(s) you're sharing the folder with can access the subdirectories below my-shared-folder . Sharing data between projects and subprojects Every file has an owner group that specifies access permissions for users belonging to that group. It's usually the case that the group id is synonymous with the project code. Somewhat confusingly however, projects can contain groups of their own, called subprojects, which can be assigned disk space quotas distinct from the project. chown -R $USER:x01-subproject /work/x01/x01-subproject/$USER/my-folder The chown command above changes the owning group for all the files within my-folder to the x01-subproject group. This might be necessary if previously those files were owned by the x01 group and thereby using some of the x01 disk quota. Archiving and data transfer Data transfer speed may be limited by many different factors so the best data transfer mechanism to use depends on the type of data being transferred and where the data is going. Disk speed - The ARCHER2 /work file system is highly parallel, consisting of a very large number of high performance disk drives. This allows it to support a very high data bandwidth. Unless the remote system has a similar parallel file-system you may find your transfer speed limited by disk performance. Meta-data performance - Meta-data operations such as opening and closing files or listing the owner or size of a file are much less parallel than read/write operations. If your data consists of a very large number of small files you may find your transfer speed is limited by meta-data operations. Meta-data operations performed by other users of the system will interact strongly with those you perform so reducing the number of such operations you use, may reduce variability in your IO timings. Network speed - Data transfer performance can be limited by network speed. More importantly it is limited by the slowest section of the network between source and destination. Firewall speed - Most modern networks are protected by some form of firewall that filters out malicious traffic. This filtering has some overhead and can result in a reduction in data transfer performance. The needs of a general purpose network that hosts email/web-servers and desktop machines are quite different from a research network that needs to support high volume data transfers. If you are trying to transfer data to or from a host on a general purpose network you may find the firewall for that network will limit the transfer rate you can achieve. The method you use to transfer data to/from ARCHER2 will depend on how much you want to transfer and where to. The methods we cover in this guide are: scp/sftp/rsync - These are the simplest methods of transferring data and can be used up to moderate amounts of data. If you are transferring data to your workstation/laptop then this is the method you will use. GridFTP - It is sometimes more convenient to transfer large amounts of data (> 100 GBs) using GridFTP servers. Before discussing specific data transfer methods, we cover archiving which is an essential process for transferring data efficiently. Archiving If you have related data that consists of a large number of small files it is strongly recommended to pack the files into a larger \"archive\" file for ease of transfer and manipulation. A single large file makes more efficient use of the file system and is easier to move and copy and transfer because significantly fewer meta-data operations are required. Archive files can be created using tools like tar and zip . tar The tar command packs files into a \"tape archive\" format. The command has general form: tar [options] [file(s)] Common options include: -c create a new archive -v verbosely list files processed -W verify the archive after writing -l confirm all file hard links are included in the archive -f use an archive file (for historical reasons, tar writes its output to stdout by default rather than a file). Putting these together: tar -cvWlf mydata.tar mydata will create and verify an archive. To extract files from a tar file, the option -x is used. For example: tar -xf mydata.tar will recover the contents of mydata.tar to the current working directory. To verify an existing tar file against a set of data, the -d (diff) option can be used. By default, no output will be given if a verification succeeds and an example of a failed verification follows: $> tar -df mydata.tar mydata/* mydata/damaged_file: Mod time differs mydata/damaged_file: Size differs Note tar files do not store checksums with their data, requiring the original data to be present during verification. Tip Further information on using tar can be found in the tar manual (accessed via man tar or at man tar ). zip The zip file format is widely used for archiving files and is supported by most major operating systems. The utility to create zip files can be run from the command line as: zip [options] mydata.zip [file(s)] Common options are: -r used to zip up a directory -# where \"#\" represents a digit ranging from 0 to 9 to specify compression level, 0 being the least and 9 the most. Default compression is -6 but we recommend using -0 to speed up the archiving process. Together: zip -0r mydata.zip mydata will create an archive. Note Unlike tar, zip files do not preserve hard links. File data will be copied on archive creation, e.g. an uncompressed zip archive of a 100MB file and a hard link to that file will be approximately 200MB in size. This makes zip an unsuitable format if you wish to precisely reproduce the file system layout. The corresponding unzip command is used to extract data from the archive. The simplest use case is: unzip mydata.zip which recovers the contents of the archive to the current working directory. Files in a zip archive are stored with a CRC checksum to help detect data loss. unzip provides options for verifying this checksum against the stored files. The relevant flag is -t and is used as follows: $> unzip -t mydata.zip Archive: mydata.zip testing: mydata/ OK testing: mydata/file OK No errors detected in compressed data of mydata.zip. Tip Further information on using zip can be found in the zip manual (accessed via man zip or at man zip ). Data transfer via SSH The easiest way of transferring data to/from ARCHER2 is to use one of the standard programs based on the SSH protocol such as scp , sftp or rsync . These all use the same underlying mechanism (SSH) as you normally use to log-in to ARCHER2. So, once the the command has been executed via the command line, you will be prompted for your password for the specified account on the remote machine (ARCHER2 in this case). To avoid having to type in your password multiple times you can set up a SSH key pair and use an SSH agent as documented in the User Guide at connecting . SSH data transfer performance considerations The SSH protocol encrypts all traffic it sends. This means that file transfer using SSH consumes a relatively large amount of CPU time at both ends of the transfer (for encryption and decryption). The ARCHER2 login nodes have fairly fast processors that can sustain about 100 MB/s transfer. The encryption algorithm used is negotiated between the SSH client and the SSH server. There are command line flags that allow you to specify a preference for which encryption algorithm should be used. You may be able to improve transfer speeds by requesting a different algorithm than the default. The aes128-ctr or aes256-ctr algorithms are well supported and fast as they are implemented in hardware. These are not usually the default choice when using scp so you will need to manually specify them. A single SSH based transfer will usually not be able to saturate the available network bandwidth or the available disk bandwidth so you may see an overall improvement by running several data transfer operations in parallel. To reduce metadata interactions it is a good idea to overlap transfers of files from different directories. In addition, you should consider the following when transferring data: Only transfer those files that are required. Consider which data you really need to keep. Combine lots of small files into a single tar archive, to reduce the overheads associated in initiating many separate data transfers (over SSH, each file counts as an individual transfer). Compress data before transferring it, e.g. using gzip . scp The scp command creates a copy of a file, or if given the -r flag, a directory either from a local machine onto a remote machine or from a remote machine onto a local machine. For example, to transfer files to ARCHER2 from a local machine: scp [options] source user@login.archer2.ac.uk:[destination] (Remember to replace user with your ARCHER2 username in the example above.) In the above example, the [destination] is optional, as when left out scp will copy the source into your home directory. Also, the source should be the absolute path of the file/directory being copied or the command should be executed in the directory containing the source file/directory. If you want to request a different encryption algorithm add the -c [algorithm-name] flag to the scp options. For example, to use the (usually faster) arcfour encryption algorithm you would use: scp [options] -c aes128-ctr source user@login.archer2.ac.uk:[destination] (Remember to replace user with your ARCHER2 username in the example above.) rsync The rsync command can also transfer data between hosts using a ssh connection. It creates a copy of a file or, if given the -r flag, a directory at the given destination, similar to scp above. Given the -a option rsync can also make exact copies (including permissions), this is referred to as mirroring . In this case the rsync command is executed with ssh to create the copy on a remote machine. To transfer files to ARCHER2 using rsync with ssh the command has the form: rsync [options] -e ssh source user@login.archer2.ac.uk:[destination] (Remember to replace user with your ARCHER2 username in the example above.) In the above example, the [destination] is optional, as when left out rsync will copy the source into your home directory. Also the source should be the absolute path of the file/directory being copied or the command should be executed in the directory containing the source file/directory. Additional flags can be specified for the underlying ssh command by using a quoted string as the argument of the -e flag. e.g. rsync [options] -e \"ssh -c arcfour\" source user@login.archer2.ac.uk:[destination] (Remember to replace user with your ARCHER2 username in the example above.) Tip Further information on using rsync can be found in the rsync manual (accessed via man rsync or at man rsync ). Data transfer via GridFTP ARCHER2 provides a module for grid computing, gct/6.2 , otherwise known as the Globus Grid Community Toolkit v6.2.20201212. This toolkit provides a command line interface for moving data to and from GridFTP servers. Data transfers are managed by the globus-url-copy command. Full details concerning this command's use can be found in the GCT 6.2 GridFTP User's Guide . Please note, the GCT module does not yet support parallel streams. We anticipate having this feature available soon. Please consult the module help ( module help gct/6.2 ) for confirmation of when this work has been completed. SSH data transfer example: laptop/workstation to ARCHER2 Here we have a short example demonstrating transfer of data directly from a laptop/workstation to ARCHER2. Note This guide assumes you are using a command line interface to transfer data. This means the terminal on Linux or macOS, MobaXterm local terminal on Windows or Powershell. Before we can transfer of data to ARCHER2 we need to make sure we have an SSH key setup to access ARCHER2 from the system we are transferring data from. If you are using the same system that you use to log into ARCHER2 then you should be all set. If you want to use a different system you will need to generate a new SSH key there (or use SSH key forwarding) to allow you to connect to ARCHER2. Tip Remember that you will need to use both a key and your password to transfer data to ARCHER2. Once we know our keys are setup correctly, we are now ready to transfer data directly between the two machines. We begin by combining our important research data in to a single archive file using the following command: tar -czf all_my_files.tar.gz file1.txt file2.txt file3.txt We then initiate the data transfer from our system to ARCHER2, here using rsync to allow the transfer to be recommenced without needing to start again, in the event of a loss of connection or other failure. For example, using the SSH key in the file ~/.ssh/id_RSA_A2 on our local system: rsync -Pv -e\"ssh -c aes128-gcm@openssh.com -i $HOME/.ssh/id_RSA_A2\" ./all_my_files.tar.gz otbz19@login.archer2.ac.uk:/work/z19/z19/otbz19/ Note the use of the -P flag to allow partial transfer -- the same command could be used to restart the transfer after a loss of connection. The -e flag allows specification of the ssh command - we have used this to add the location of the identity file. The -c option specifies the cipher to be used as aes128-gcm which has been found to increase performance Unfortunately the ~ shortcut is not correctly expanded, so we have specified the full path. We move our research archive to our project work directory on ARCHER2. Note Remember to replace otbz19 with your username on ARCHER2. If we were unconcerned about being able to restart an interrupted transfer, we could instead use the scp command, scp -c aes128-gcm@openssh.com -i ~/.ssh/id_RSA_A2 all_my_files.tar.gz otbz19@login.archer2.ac.uk:/work/z19/z19/otbz19/ but rsync is recommended for larger transfers.","title":"Data management and transfer"},{"location":"user-guide/data/#data-management-and-transfer","text":"This section covers best practice and tools for data management on ARCHER2 along with a description of the different storage available on the service. The IO section has information on achieving good performance for reading and writing data to the ARCHER2 storage along with information and advice on different IO patterns. Information If you have any questions on data management and transfer please do not hesitate to contact the ARCHER2 service desk at support@archer2.ac.uk .","title":"Data management and transfer"},{"location":"user-guide/data/#useful-resources-and-links","text":"Harry Mangalam's guide on How to transfer large amounts of data via network . This provides lots of useful advice on transferring data.","title":"Useful resources and links"},{"location":"user-guide/data/#data-management","text":"We strongly recommend that you give some thought to how you use the various data storage facilities that are part of the ARCHER2 service. This will not only allow you to use the machine more effectively but also to ensure that your valuable data is protected. Here are the main points you should consider: Not all data are created equal, understand your data. Know what data you have. What is your critical data that needs to be copied to a secure location? Which data do you need in a different location to analyse? Which data would it be easier to regenerate rather than transfer? You should create a brief data management plan laying this out as this will allow you to understand which tools to use and when. Minimise the data you are transferring. Transferring large amounts of data is costly in both researcher time and actual time. Make sure you are only transferring the data you need to transfer. Minimise the number of files you are transferring. Each individual file has a static overhead in data transfers so it is efficient to bundle multiple files together into a single large archive file for transfer. Does compression help or hinder? Many tools have the option to use compression (e.g. rsync , tar , zip ) and generally encourage you to use them to reduce data volumes. However, in some cases, the time spent compressing the data can take longer than actually transferring the uncompressed data; particularly when transferring data between two locations that both have large data transfer bandwidth available. Be aware of encryption overheads. When transferring data using scp (and rsync over scp ) your data will be encrypted introducing a static overhead per file. This issue can be minimised by reducing the number files to be transferred by creating archives. You can also change the encryption algorithm to one that involves minimal encryption. The fastest performing cipher that is commonly available in SSH at the moment is generally aes128-ctr as most common processors provide a hardware implementation.","title":"Data management"},{"location":"user-guide/data/#archer2-storage","text":"The ARCHER2 service, like many HPC systems, has a complex structure. There are a number of different data storage types available to users: Home file systems Work file systems RDFaaS (RDF as a Service) file systems ( /epsrc and /general ) Each type of storage has different characteristics and policies, and is suitable for different types of use. There are also two different types of node available to users: Login nodes Compute nodes Data analysis nodes Each type of node sees a different combination of the storage types. The following table shows which storage options are avalable on different node types: Storage Login Nodes Compute Nodes Data analysis nodes Notes /home yes no yes Backed up /work yes yes yes Not backed up, high performance RDFaaS yes no yes Backed up, high performance. Only available for projects that moved from ARCHER to ARCHER2.","title":"ARCHER2 storage"},{"location":"user-guide/data/#home-file-systems","text":"There are four independent home file-systems. Every project has an allocation on one of the four. You do not need to know which one your project uses as your projects space can always be accessed via the path /home/project-code . Each home file-system is approximately 100 TB in size and is implemented using standard Network Attached Storage (NAS) technology. This means that these disks are not particularly high performance but are well suited to standard operations like compilation and file editing. These file systems are visible from the ARCHER2 login nodes.","title":"Home file systems"},{"location":"user-guide/data/#accessing-backups-of-home-file-systems","text":"The home file systems are fully backed up . Full backups are taken weekly (for each of the past two weeks), daily (for each of the past two days) and hourly (for each of the last 6 hours). You can access the snapshots at the /home1/.snapshot , /home2/.snapshot , /home3/.snapshot and /home4/.snapshot depending on which of the file systems you have your home directories on. You can find out which file system your home directory is on with the command: readlink -f $HOME These file-systems are a good location to keep source-code, copies of scripts and compiled binaries. Small amounts of important data can also be copied here for safe keeping though the file systems are not fast enough to manipulate large datasets effectively.","title":"Accessing backups of home file systems"},{"location":"user-guide/data/#quotas-on-home-file-systems","text":"All projects are assigned a quota on the home file systems. The project PI or manager can split this quota up between users or groups of users if they wish. You can view any home file system quotas that apply to your account by logging into SAFE and navigating to the page for your ARCHER2 login account. Log into SAFE Use the \"Login accounts\" menu and select your ARCHER2 login account The \"Login account details\" table lists any user or group quotas that are linked with your account. (If there is no quota shown for a row then you have an unlimited quota for that item, but you may still may be limited by another quota.) Tip Quota and usage data on SAFE is updated twice daily so may not be exactly up to date with the situation on the systems themselves.","title":"Quotas on home file systems"},{"location":"user-guide/data/#work-file-systems","text":"There are currently three work file systems on the full ARCHER2 service. Each of these file systems is 3.4 PB and a portion of one of these file systems is available to each project. All of these are high-performance, Lustre parallel file systems. They are designed to support data in large files. The performance for data stored in large numbers of small files is probably not going to be as good. These are the only file systems that are available on the compute nodes so all data read or written by jobs running on the compute nodes has to be hosted here. Warning There are no backups of any data on the work file systems. You should not rely on these file systems for long term storage. Ideally, these file systems should only contain data that is: actively in use; recently generated and in the process of being saved elsewhere; or being made ready for up-coming work. In practice it may be convenient to keep copies of datasets on the work file systems that you know will be needed at a later date. However, make sure that important data is always backed up elsewhere and that your work would not be significantly impacted if the data on the work file systems was lost. Large data sets can be moved to the RDF storage or transferred off the ARCHER2 service entirely. If you have data on the work file systems that you are not going to need in the future please delete it.","title":"Work file systems"},{"location":"user-guide/data/#quotas-on-the-work-file-systems","text":"As for the home file systems, all projects are assigned a quota on the work file systems. The project PI or manager can split this quota up between users or groups of users if they wish. You can view any work file system quotas that apply to your account by logging into SAFE and navigating to the page for your ARCHER2 login account. Log into SAFE Use the \"Login accounts\" menu and select your ARCHER2 login account The \"Login account details\" table lists any user or group quotas that are linked with your account. (If there is no quota shown for a row then you have an unlimited quota for that item, but you may still may be limited by another quota.) Tip Quota and usage data on SAFE is updated twice daily so may not be exactly up to date with the situation on the systems themselves. You can also examine up to date quotas and usage on the ARCHER2 systems themselves using the lfs quota command. To do this: Change directory to the work directory where you want to check the quota. For example, if I wanted to check the quota for user auser in project t01 then I would: cd /work/t01/t01/auser To check your user quota, you would use the command: auser@ln03:/work/t01/t01/auser> lfs quota -hu auser . Disk quotas for usr auser (uid 5496): Filesystem used quota limit grace files quota limit grace . 1.366G 0k 0k - 5486 0 0 - uid 5496 is using default block quota setting uid 5496 is using default file quota setting the quota and limit of 0k here indicate that no user quota is set for this user To check your project quota, you would use the command: auser@ln03:/work/t01/t01/auser> lfs quota -hp $(id -g) . Disk quotas for prj 1009 (pid 1009): Filesystem used quota limit grace files quota limit grace . 2.905G 0k 0k - 25300 0 0 - pid 1009 is using default block quota setting pid 1009 is using default file quota setting","title":"Quotas on the work file systems"},{"location":"user-guide/data/#rdfaas-file-system","text":"The data that was available on the RDF /epsrc and /general file systems is available on ARCHER2 via the RDFaaS (RDF as a Service) file system. If you have requested the same username in the same project as you had on ARCHER then you will be able to access your data at either: /epsrc/<project-code> or /general/<project-code> depending on which file system it was in on the RDF file systems. For example, if your username is auser and you are in the e05 project, then your RDF data will be on the RDFaaS file system at: /epsrc/e05/e05/auser The RDFaaS file systems are only available on the ARCHER2 login nodes. Important The data on the RDFaaS file system is currently available in read-only mode . You need to transfer data from the RDFaaS file system to /work (or /home if it is a small amount of data) if you wish to alter it or use it on the compute nodes. You can, of course, also use scp to transfer data from the RDFaaS file system to another system. Note We plan to make the RDFaaS file system read/write once the full ARCHER2 service is available. Tip If you are having issues accessing data on the RDFaaS file system then please contact the ARCHER2 Service Desk","title":"RDFaaS file system"},{"location":"user-guide/data/#copying-data-from-rdfaas-to-work-file-systems","text":"You should use the standard Linux cp command to copy data from the RDFaaS file system to other ARCHER2 file systems (usually /work ). For example, to transfer the file important-data.tar.gz from the RDFaaS file system to /work you would use the following command (assuming you are user auser in project e05 ): cp /epsrc/e05/e05/auser/important-data.tar.gz /work/e05/e05/auser/ (remember to replace the project code and username with your own username and project code. You may also need to use /general if your data was there on the RDF file systems).","title":"Copying data from RDFaaS to Work file systems"},{"location":"user-guide/data/#subprojects","text":"Some large projects may choose to split their resources into multiple subprojects. These subprojects will have identifiers appended to the main project ID. For example, the rse subgroup of the z19 project would have the ID z19-rse . If the main project has allocated storage quotas to the subproject the directories for this storage will be found at, for example: /home/z19/z19-rse/auser Your Linux home directory will generally not be changed when you are made a member of a subproject so you must change directories manually (or change the ownership of files) to make use of this different storage quota allocation.","title":"Subprojects"},{"location":"user-guide/data/#sharing-data-with-other-archer2-users","text":"How you share data with other ARCHER2 users depends on whether or not they belong to the same project as you. Each project has two shared folders that can be used for sharing data.","title":"Sharing data with other ARCHER2 users"},{"location":"user-guide/data/#sharing-data-with-archer2-users-in-your-project","text":"Each project has an inner shared folder. /work/[project code]/[project code]/shared This folder has read/write permissions for all project members. You can place any data you wish to share with other project members in this directory. For example, if your project code is x01 the inner shared folder would be located at /work/x01/x01/shared .","title":"Sharing data with ARCHER2 users in your project"},{"location":"user-guide/data/#sharing-data-with-all-archer2-users","text":"Each project also has an outer shared folder.: /work/[project code]/shared It is writable by all project members and readable by any user on the system. You can place any data you wish to share with other ARCHER2 users who are not members of your project in this directory. For example, if your project code is x01 the outer shared folder would be located at /work/x01/shared .","title":"Sharing data with all ARCHER2 users"},{"location":"user-guide/data/#permissions","text":"You should check the permissions of any files that you place in the shared area, especially if those files were created in your own ARCHER2 account. Files of the latter type are likely to be readable by you only. The chmod command below shows how to make sure that a file placed in the outer shared folder is also readable by all ARCHER2 users. chmod a+r /work/x01/shared/your-shared-file.txt Similarly, for the inner shared folder, chmod can be called such that read permission is granted to all users within the x01 project. chmod g+r /work/x01/x01/shared/your-shared-file.txt If you're sharing a set of files stored within a folder hierarchy the chmod is slightly more complicated. chmod -R a+Xr /work/x01/shared/my-shared-folder chmod -R g+Xr /work/x01/x01/shared/my-shared-folder The -R option ensures that the read permission is enabled recursively and the +X guarantees that the user(s) you're sharing the folder with can access the subdirectories below my-shared-folder .","title":"Permissions"},{"location":"user-guide/data/#sharing-data-between-projects-and-subprojects","text":"Every file has an owner group that specifies access permissions for users belonging to that group. It's usually the case that the group id is synonymous with the project code. Somewhat confusingly however, projects can contain groups of their own, called subprojects, which can be assigned disk space quotas distinct from the project. chown -R $USER:x01-subproject /work/x01/x01-subproject/$USER/my-folder The chown command above changes the owning group for all the files within my-folder to the x01-subproject group. This might be necessary if previously those files were owned by the x01 group and thereby using some of the x01 disk quota.","title":"Sharing data between projects and subprojects"},{"location":"user-guide/data/#archiving-and-data-transfer","text":"Data transfer speed may be limited by many different factors so the best data transfer mechanism to use depends on the type of data being transferred and where the data is going. Disk speed - The ARCHER2 /work file system is highly parallel, consisting of a very large number of high performance disk drives. This allows it to support a very high data bandwidth. Unless the remote system has a similar parallel file-system you may find your transfer speed limited by disk performance. Meta-data performance - Meta-data operations such as opening and closing files or listing the owner or size of a file are much less parallel than read/write operations. If your data consists of a very large number of small files you may find your transfer speed is limited by meta-data operations. Meta-data operations performed by other users of the system will interact strongly with those you perform so reducing the number of such operations you use, may reduce variability in your IO timings. Network speed - Data transfer performance can be limited by network speed. More importantly it is limited by the slowest section of the network between source and destination. Firewall speed - Most modern networks are protected by some form of firewall that filters out malicious traffic. This filtering has some overhead and can result in a reduction in data transfer performance. The needs of a general purpose network that hosts email/web-servers and desktop machines are quite different from a research network that needs to support high volume data transfers. If you are trying to transfer data to or from a host on a general purpose network you may find the firewall for that network will limit the transfer rate you can achieve. The method you use to transfer data to/from ARCHER2 will depend on how much you want to transfer and where to. The methods we cover in this guide are: scp/sftp/rsync - These are the simplest methods of transferring data and can be used up to moderate amounts of data. If you are transferring data to your workstation/laptop then this is the method you will use. GridFTP - It is sometimes more convenient to transfer large amounts of data (> 100 GBs) using GridFTP servers. Before discussing specific data transfer methods, we cover archiving which is an essential process for transferring data efficiently.","title":"Archiving and data transfer"},{"location":"user-guide/data/#archiving","text":"If you have related data that consists of a large number of small files it is strongly recommended to pack the files into a larger \"archive\" file for ease of transfer and manipulation. A single large file makes more efficient use of the file system and is easier to move and copy and transfer because significantly fewer meta-data operations are required. Archive files can be created using tools like tar and zip .","title":"Archiving"},{"location":"user-guide/data/#tar","text":"The tar command packs files into a \"tape archive\" format. The command has general form: tar [options] [file(s)] Common options include: -c create a new archive -v verbosely list files processed -W verify the archive after writing -l confirm all file hard links are included in the archive -f use an archive file (for historical reasons, tar writes its output to stdout by default rather than a file). Putting these together: tar -cvWlf mydata.tar mydata will create and verify an archive. To extract files from a tar file, the option -x is used. For example: tar -xf mydata.tar will recover the contents of mydata.tar to the current working directory. To verify an existing tar file against a set of data, the -d (diff) option can be used. By default, no output will be given if a verification succeeds and an example of a failed verification follows: $> tar -df mydata.tar mydata/* mydata/damaged_file: Mod time differs mydata/damaged_file: Size differs Note tar files do not store checksums with their data, requiring the original data to be present during verification. Tip Further information on using tar can be found in the tar manual (accessed via man tar or at man tar ).","title":"tar"},{"location":"user-guide/data/#zip","text":"The zip file format is widely used for archiving files and is supported by most major operating systems. The utility to create zip files can be run from the command line as: zip [options] mydata.zip [file(s)] Common options are: -r used to zip up a directory -# where \"#\" represents a digit ranging from 0 to 9 to specify compression level, 0 being the least and 9 the most. Default compression is -6 but we recommend using -0 to speed up the archiving process. Together: zip -0r mydata.zip mydata will create an archive. Note Unlike tar, zip files do not preserve hard links. File data will be copied on archive creation, e.g. an uncompressed zip archive of a 100MB file and a hard link to that file will be approximately 200MB in size. This makes zip an unsuitable format if you wish to precisely reproduce the file system layout. The corresponding unzip command is used to extract data from the archive. The simplest use case is: unzip mydata.zip which recovers the contents of the archive to the current working directory. Files in a zip archive are stored with a CRC checksum to help detect data loss. unzip provides options for verifying this checksum against the stored files. The relevant flag is -t and is used as follows: $> unzip -t mydata.zip Archive: mydata.zip testing: mydata/ OK testing: mydata/file OK No errors detected in compressed data of mydata.zip. Tip Further information on using zip can be found in the zip manual (accessed via man zip or at man zip ).","title":"zip"},{"location":"user-guide/data/#data-transfer-via-ssh","text":"The easiest way of transferring data to/from ARCHER2 is to use one of the standard programs based on the SSH protocol such as scp , sftp or rsync . These all use the same underlying mechanism (SSH) as you normally use to log-in to ARCHER2. So, once the the command has been executed via the command line, you will be prompted for your password for the specified account on the remote machine (ARCHER2 in this case). To avoid having to type in your password multiple times you can set up a SSH key pair and use an SSH agent as documented in the User Guide at connecting .","title":"Data transfer via SSH"},{"location":"user-guide/data/#ssh-data-transfer-performance-considerations","text":"The SSH protocol encrypts all traffic it sends. This means that file transfer using SSH consumes a relatively large amount of CPU time at both ends of the transfer (for encryption and decryption). The ARCHER2 login nodes have fairly fast processors that can sustain about 100 MB/s transfer. The encryption algorithm used is negotiated between the SSH client and the SSH server. There are command line flags that allow you to specify a preference for which encryption algorithm should be used. You may be able to improve transfer speeds by requesting a different algorithm than the default. The aes128-ctr or aes256-ctr algorithms are well supported and fast as they are implemented in hardware. These are not usually the default choice when using scp so you will need to manually specify them. A single SSH based transfer will usually not be able to saturate the available network bandwidth or the available disk bandwidth so you may see an overall improvement by running several data transfer operations in parallel. To reduce metadata interactions it is a good idea to overlap transfers of files from different directories. In addition, you should consider the following when transferring data: Only transfer those files that are required. Consider which data you really need to keep. Combine lots of small files into a single tar archive, to reduce the overheads associated in initiating many separate data transfers (over SSH, each file counts as an individual transfer). Compress data before transferring it, e.g. using gzip .","title":"SSH data transfer performance considerations"},{"location":"user-guide/data/#scp","text":"The scp command creates a copy of a file, or if given the -r flag, a directory either from a local machine onto a remote machine or from a remote machine onto a local machine. For example, to transfer files to ARCHER2 from a local machine: scp [options] source user@login.archer2.ac.uk:[destination] (Remember to replace user with your ARCHER2 username in the example above.) In the above example, the [destination] is optional, as when left out scp will copy the source into your home directory. Also, the source should be the absolute path of the file/directory being copied or the command should be executed in the directory containing the source file/directory. If you want to request a different encryption algorithm add the -c [algorithm-name] flag to the scp options. For example, to use the (usually faster) arcfour encryption algorithm you would use: scp [options] -c aes128-ctr source user@login.archer2.ac.uk:[destination] (Remember to replace user with your ARCHER2 username in the example above.)","title":"scp"},{"location":"user-guide/data/#rsync","text":"The rsync command can also transfer data between hosts using a ssh connection. It creates a copy of a file or, if given the -r flag, a directory at the given destination, similar to scp above. Given the -a option rsync can also make exact copies (including permissions), this is referred to as mirroring . In this case the rsync command is executed with ssh to create the copy on a remote machine. To transfer files to ARCHER2 using rsync with ssh the command has the form: rsync [options] -e ssh source user@login.archer2.ac.uk:[destination] (Remember to replace user with your ARCHER2 username in the example above.) In the above example, the [destination] is optional, as when left out rsync will copy the source into your home directory. Also the source should be the absolute path of the file/directory being copied or the command should be executed in the directory containing the source file/directory. Additional flags can be specified for the underlying ssh command by using a quoted string as the argument of the -e flag. e.g. rsync [options] -e \"ssh -c arcfour\" source user@login.archer2.ac.uk:[destination] (Remember to replace user with your ARCHER2 username in the example above.) Tip Further information on using rsync can be found in the rsync manual (accessed via man rsync or at man rsync ).","title":"rsync"},{"location":"user-guide/data/#data-transfer-via-gridftp","text":"ARCHER2 provides a module for grid computing, gct/6.2 , otherwise known as the Globus Grid Community Toolkit v6.2.20201212. This toolkit provides a command line interface for moving data to and from GridFTP servers. Data transfers are managed by the globus-url-copy command. Full details concerning this command's use can be found in the GCT 6.2 GridFTP User's Guide . Please note, the GCT module does not yet support parallel streams. We anticipate having this feature available soon. Please consult the module help ( module help gct/6.2 ) for confirmation of when this work has been completed.","title":"Data transfer via GridFTP"},{"location":"user-guide/data/#ssh-data-transfer-example-laptopworkstation-to-archer2","text":"Here we have a short example demonstrating transfer of data directly from a laptop/workstation to ARCHER2. Note This guide assumes you are using a command line interface to transfer data. This means the terminal on Linux or macOS, MobaXterm local terminal on Windows or Powershell. Before we can transfer of data to ARCHER2 we need to make sure we have an SSH key setup to access ARCHER2 from the system we are transferring data from. If you are using the same system that you use to log into ARCHER2 then you should be all set. If you want to use a different system you will need to generate a new SSH key there (or use SSH key forwarding) to allow you to connect to ARCHER2. Tip Remember that you will need to use both a key and your password to transfer data to ARCHER2. Once we know our keys are setup correctly, we are now ready to transfer data directly between the two machines. We begin by combining our important research data in to a single archive file using the following command: tar -czf all_my_files.tar.gz file1.txt file2.txt file3.txt We then initiate the data transfer from our system to ARCHER2, here using rsync to allow the transfer to be recommenced without needing to start again, in the event of a loss of connection or other failure. For example, using the SSH key in the file ~/.ssh/id_RSA_A2 on our local system: rsync -Pv -e\"ssh -c aes128-gcm@openssh.com -i $HOME/.ssh/id_RSA_A2\" ./all_my_files.tar.gz otbz19@login.archer2.ac.uk:/work/z19/z19/otbz19/ Note the use of the -P flag to allow partial transfer -- the same command could be used to restart the transfer after a loss of connection. The -e flag allows specification of the ssh command - we have used this to add the location of the identity file. The -c option specifies the cipher to be used as aes128-gcm which has been found to increase performance Unfortunately the ~ shortcut is not correctly expanded, so we have specified the full path. We move our research archive to our project work directory on ARCHER2. Note Remember to replace otbz19 with your username on ARCHER2. If we were unconcerned about being able to restart an interrupted transfer, we could instead use the scp command, scp -c aes128-gcm@openssh.com -i ~/.ssh/id_RSA_A2 all_my_files.tar.gz otbz19@login.archer2.ac.uk:/work/z19/z19/otbz19/ but rsync is recommended for larger transfers.","title":"SSH data transfer example: laptop/workstation to ARCHER2"},{"location":"user-guide/debug/","text":"Debugging The following debugging tools are available on ARCHER2: gdb4hpc is a command-line debugging tool provided by HPE Cray. It works similarly to gdb , but allows the user to debug multiple parallel processes without multiple windows. gdb4hpc can be used to investigate deadlocked code, segfaults, and other errors for C/C++ and Fortran code. Users can single-step code and focus on specific processes groups to help identify unexpected code behavior. (text from ALCF ). valgrind4hpc is a parallel memory debugging tool that aids in detection of memory leaks and errors in parallel applications. It aggregates like errors across processes and threads to simply debugging of parallel applications. STAT generate merged stack traces for parallel applications. Also has visualisation tools. ATP provides scalable core file and backtrace analysis when parallel programs crash. CCDB Cray Comparative Debugger. Compare two versions of code side-by-side to analyse differences. (Not currently described in this documentation.) gdb4hpc The GNU Debugger for HPC (gdb4hpc) is a GDB-based debugger used to debug applications compiled with CCE, PGI, GNU, and Intel Fortran, C and C++ compilers. It allows programmers to either launch an application within it or to attach to an already-running application. Attaching to an already-running and hanging application is a quick way of understanding why the application is hanging, whereas launching an application through gdb4hpc will allow you to see your application running step-by-step, output the values of variables, and check whether the application runs as expected. Tip For your executable to be compatible with gdb4hpc, it will need to be coded with MPI. You will also need to compile your code with the debugging flag -g (e.g. cc -g my_program.c -o my_exe ). Launching through gdb4hpc Launch gdb4hpc : module load gdb4hpc gdb4hpc You will get some information about this version of the program and, eventually, you will get a command prompt: gdb4hpc 4.5 - Cray Line Mode Parallel Debugger With Cray Comparative Debugging Technology. Copyright 2007-2019 Cray Inc. All Rights Reserved. Copyright 1996-2016 University of Queensland. All Rights Reserved. Type \"help\" for a list of commands. Type \"help <cmd>\" for detailed help about a command. dbg all> We will use launch to begin a multi-process application within gdb4hpc. Consider that we are wanting to test an application called my_exe , and that we want this to be launched across all 256 processes in two nodes. We would launch this in gdb4hpc by running: dbg all> launch --launcher-args=\"--account=[budget code] --partition=standard --qos=standard --nodes=2 --tasks-per-node=128 --cpus-per-task=1 --exclusive --export=ALL\" $my_prog{256} ./my_ex Make sure to replace the --account input to your budget code ( e.g. if you are using budget t01, that part should look like --account=t01 ). The default launcher is srun and the --launcher-args=\"...\" allows you to set launcher flags for srun . The variable $my_prog is a dummy name for the program being launched and you could use whatever name you want for it -- this will be the name of the srun job that will be run. The number in the brackets {256} is the number of processes over which the program will be executed, it's 256 here, but you could use any number. You should try to run this on as few processors as possible -- the more you use, the longer it will take for gdb4hpc to load the program. Once the program is launched, gdb4hpc will load up the program and begin to run it. You will get output to screen something that looks like: Starting application, please wait... Creating MRNet communication network... Waiting for debug servers to attach to MRNet communications network... Timeout in 400 seconds. Please wait for the attach to complete. Number of dbgsrvs connected: [0]; Timeout Counter: [1] Number of dbgsrvs connected: [0]; Timeout Counter: [2] Number of dbgsrvs connected: [0]; Timeout Counter: [3] Number of dbgsrvs connected: [1]; Timeout Counter: [0] Number of dbgsrvs connected: [1]; Timeout Counter: [1] Number of dbgsrvs connected: [2]; Timeout Counter: [0] Finalizing setup... Launch complete. my_prog{0..255}: Initial breakpoint, main at /PATH/TO/my_program.c:34 The line number at which the initial breakpoint is made (in the above example, line 34) corresponds to the line number at which MPI is initialised. You will not be able to see any parts of the code outside of the MPI region of a code with gdb4hpc. Once the code is loaded, you can use various commands to move through your code. The following lists and describes some of the most useful ones: help -- Lists all gdb4hpc commands. You can run help COMMAND_NAME to learn more about a specific command ( e.g. help launch will tell you about the launch command list -- Will show the current line of code and the 9 lines following. Repeated use of list will move you down the code in ten-line chunks. next -- Will jump to the next step in the program for each process and output which line of code each process is one. It will not enter subroutines. !!! note that there is no reverse-step in gdb4hpc. step -- Like next , but this will step into subroutines. up -- Go up one level in the program ( e.g. from a subroutine back to main). print var -- Prints the value of variable var at this point in the code. watch var -- Like print, but will print whenever a variable changes value. quit -- Exits gdb4hpc. Remember to exit the interactive session once you are done debugging. Attaching with gdb4hpc Attaching to a hanging job using gdb4hpc is a great way of seeing which state each processor is in. However, this does not produce the most visually appealing results. For a more easy-to-read program, please take a look at the STAT tool. In your interactive session, launch your executable as a background task (by adding an & at the end of the command). For example, if you are running an executable called my_exe using 256 processes, you would run: srun -n 256 --nodes=2 --tasks-per-node=128 --cpus-per-task=1 --time=01:00:00 --export=ALL \\ --account=[budget code] --partition=standard --qos=standard ./my_exe & Make sure to replace the --account input to your budget code ( e.g. if you are using budget t01, that part should look like --account=t01 ). You will need to get the full job ID of the job you have just launched. To do this, run: squeue -u $USER and find the job ID associated with this interactive session -- this will be the one with the jobname bash . In this example: JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 1050 workq my_mpi_j jsindt R 0:16 1 nid000001 1051 workq bash jsindt R 0:12 1 nid000002 the appropriate job id is 1051. Next, you will need to run sstat on this job id: sstat 1051 This will output a large amount of information about this specific job. We are looking for the first number of this output, which should look like JOB_ID.## -- the number after the job ID is the number of slurm tasks performed in this interactive session. For our example (where srun is the first slurm task performed), the number is 1051.0. Launch gdb4hpc : module load gdb4hpc gdb4hpc You will get some information about this version of the program and, eventually, you will get a command prompt: gdb4hpc 4.5 - Cray Line Mode Parallel Debugger With Cray Comparative Debugging Technology. Copyright 2007-2019 Cray Inc. All Rights Reserved. Copyright 1996-2016 University of Queensland. All Rights Reserved. Type \"help\" for a list of commands. Type \"help <cmd>\" for detailed help about a command. dbg all> We will be using the attach command to attach to our program that hangs. This is done by writing: dbg all> attach $my_prog JOB_ID.## where JOB_ID.## is the full job ID found using sstat (in our example, this would be 1051.0). The name $my_prog is a dummy-name -- it could be whatever name you like. As it is attaching, gdb4hpc will output text to screen that looks like: Attaching to application, please wait... Creating MRNet communication network... Waiting for debug servers to attach to MRNet communications network... Timeout in 400 seconds. Please wait for the attach to complete. Number of dbgsrvs connected: [0]; Timeout Counter: [1] ... Finalizing setup... Attach complete. Current rank location: After this, you will get an output that, among other things, tells you which line of your code each process is on, and what each process is doing. This can be helpful to see where the hang-up is. If you accidentally attached to the wrong job, you can detach by running: dbg all> release $my_prog and re-attach with the correct job ID. You will need to change your dummy name from $my_prog to something else. When you are finished using gbd4hpc , simply run: dbg all> quit Do not forget to exit your interactive session. valgrind4hpc valgrind4hpc is a Valgrind-based debugging tool to aid in the detection of memory leaks and errors in parallel applications. Valgrind4hpc aggregates any duplicate messages across ranks to help provide an understandable picture of program behavior. Valgrind4hpc manages starting and redirecting output from many copies of Valgrind, as well as recombining and filtering Valgrind messages. If your program can be debugged with Valgrind, it can be debugged with valgrind4hpc. The valgrind4hpc module enables the use of standard valgrind as well as the valgrind4hpc version more suitable to parallel programs. Using Valgrind with serial programs Launch valgrind4hpc : module load valgrind4hpc Next, run your executable through valgrind: valgrind --tool=memcheck --leak-check=yes my_executable The log outputs to screen. The ERROR SUMMARY will tell you whether, and how many, memory errors there are in your program. Furthermore, if you compile your code using the -g debugging flag ( e.g. gcc -g my_program.c -o my_executable.c ), the log will point out the code lines where the error occurs. Valgrind also includes a tool called Massif that can be used to give insight into the memory usage of your program. It takes regular snapshots and outputs this data into a single file, which can be visualised to show the total amount of memory used as a function of time. This shows when peaks and bottlenecks occur and allows you to identify which data structures in your code are responsible for the largest memory usage of your program. Documentation explaining how to use Massif is available at the official Massif manual . In short, you should run your executable as follows: valgrind --tool=massif my_executable The memory profiling data will be output into a file called massif.out.pid , where pid is the runtime process ID of your program. A custom filename can be chosen using the --massif-out-file option , as follows: valgrind --tool=massif --massif-out-file=optional_filename.out my_executable The output file contains raw profiling statistics. To view a summary including a graphical plot of memory usage over time, use the ms_print command as follows: ms_print massif.out.12345 or, to save to a file: ms_print massif.out.12345 > massif.analysis.12345 This will show total memory usage over time as well as a breakdown of the top data structures contributing to memory usage at each snapshot where there has been a significant allocation or deallocation of memory. Using Valgrind4hpc with parallel programs First, load valgrind4hpc : module load valgrind4hpc To run valgrind4hpc, first reserve the resources you will use with salloc . The following reservation request is for 2 nodes (256 physical cores) for 20 minutes on the short queue: auser@uan01:> salloc --nodes=2 --tasks-per-node=128 --cpus-per-task=1 \\ --time=00:20:00 --partition=standard --qos=short \\ --reservation=shortqos --hint=nomultithread \\ --distribution=block:block --account=[budget code] Once your allocation is ready, Use valgrind4hpc to run and profile your executable. To test an executable called my_executable that requires two arguments arg1 and arg2 on 2 nodes and 256 processes, run: valgrind4hpc --tool=memcheck --num-ranks=256 my_executable -- arg1 arg2 In particular, note the -- separating the executable from the arguments (this is not necessary if your executable takes no arguments). Valgrind4hpc only supports certain tools found in valgrind. These are: memcheck, helgrind, exp-sgcheck, or drd. The --valgrind-args=\"arguments\" allows users to use valgrind options not supported in valgrind4hpc ( e.g. --leak-check ) -- note, however, that some of these options might interfere with valgrind4hpc. More information on valgrind4hpc can be found in the manual ( man valgrind4hpc ). STAT The Stack Trace Analysis Tool (STAT) is a cross-platform debugging tool from the University of Wisconsin-Madison. ATP is based on the same technology as STAT, both are designed to gather and merge stack traces from a running application's parallel processes. The STAT tool can be useful when application seems to be deadlocked or stuck, i.e. they don't crash but they don't progress as expected, and it has been designed to scale to a very large number of processes. Full information on STAT, including use cases, is available at the STAT website . STAT will attach to a running program and query that program to find out where all the processes in that program currently are. It will then process that data and produce a graph displaying the unique process locations (i.e. where all the processes in the running program currently are). To make this easily understandable it collates together all processes that are in the same place providing only unique program locations for display. Using STAT on ARCHER2 On the login node, load the cray-stat module: module load cray-stat Then, launch your job using srun as a background task (by adding an & at the end of the command). For example, if you are running an executable called my_exe using 256 processes, you would run: srun -n 256 --nodes=2 --tasks-per-node=128 --cpus-per-task=1 --time=01:00:00 --export=ALL\\ --account=[budget code] --partition=standard --qos=standard./my_exe & Note This example has set the job time limit to 1 hour -- if you need longer, change the --time command. You will need the Program ID (PID) of the job you have just launched -- the PID is printed to screen upon launch, or you can get it by running: ps -u $USER This will present you with a set of text that looks like this: PID TTY TIME CMD 154296 ? 00:00:00 systemd 154297 ? 00:00:00 (sd-pam) 154302 ? 00:00:00 sshd 154303 pts/8 00:00:00 bash 157150 pts/8 00:00:00 salloc 157152 pts/8 00:00:00 bash 157183 pts/8 00:00:00 srun 157185 pts/8 00:00:00 srun 157191 pts/8 00:00:00 ps Once your application has reached the point where it hangs, issue the following command (replacing PID with the ID of the first srun task -- in the above example, I would replace PID with 157183): stat-cl -i PID You will get an output that looks like this: STAT started at 2020-07-22-13:31:35 Attaching to job launcher (null):157565 and launching tool daemons... Tool daemons launched and connected! Attaching to application... Attached! Application already paused... ignoring request to pause Sampling traces... Traces sampled! Resuming the application... Resumed! Pausing the application... Paused! ... Detaching from application... Detached! Results written to $PATH_TO_RUN_DIRECTORY/stat_results/my_exe.0000 Once STAT is finished, you can kill the srun job using scancel (replacing JID with the job ID of the job you just launched): scancel JID You can view the results that STAT has produced using the following command (note that \"my_exe\" will need to be replaced with the name of the executable you ran): stat-view stat_results/my_exe.0000/00_my_exe.0000.3D.dot This produces a graph displaying all the different places within the program that the parallel processes were when you queried them. Note To see the graph, you will need to have exported your X display when logging in. Larger jobs may spend significant time queueing, requiring submission as a batch job. In this case the above can be adapted for batch submission as follows: #!/bin/bash --login #SBATCH --job-name=test_job #SBATCH --nodes=1 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 #SBATCH --time=2:0:0 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Load additional modules module load cray-stat # Launch job in background srun ./my_exe & sleep 3600 # Wait a sufficient time for job to hang, e.g. 1 hour # Find PID and run STAT pid = $( ps -u ${ USER } | \\ # Get all USER PIDs grep srun | \\ # Find all the sruns head -n 1 | \\ # We want the first one awk '{print $1;}' ) # The PID is the first \"word\" in the line stat-cl -i $pid If the job is hanging it will continue to run until the wall clock exceeds the requested time. ATP To enable ATP you should load the atp module and set the ATP_ENABLED environment variable to 1 on the login node: module load atp export ATP_ENABLED=1 Then, launch your job using srun as a background task (by adding an & at the end of the command). For example, if you are running an executable called my_exe using 256 processes, you would run: srun -n=256 --nodes=2 --tasks-per-node=128 --cpus-per-task=1 --time=01:00:00 --export=ALL \\ --account=[budget code] --partition=standard --qos=standard ./my_exe & Note This example has set the job time limit to 1 hour -- if you need longer, change the --time command. Once the job has finished running, load the stat module to view the results: module load cray-stat and view the merged stack trace using: stat-view atpMergedBT.dot Note To see the graph, you will need to have exported your X display when logging in.","title":"Debugging"},{"location":"user-guide/debug/#debugging","text":"The following debugging tools are available on ARCHER2: gdb4hpc is a command-line debugging tool provided by HPE Cray. It works similarly to gdb , but allows the user to debug multiple parallel processes without multiple windows. gdb4hpc can be used to investigate deadlocked code, segfaults, and other errors for C/C++ and Fortran code. Users can single-step code and focus on specific processes groups to help identify unexpected code behavior. (text from ALCF ). valgrind4hpc is a parallel memory debugging tool that aids in detection of memory leaks and errors in parallel applications. It aggregates like errors across processes and threads to simply debugging of parallel applications. STAT generate merged stack traces for parallel applications. Also has visualisation tools. ATP provides scalable core file and backtrace analysis when parallel programs crash. CCDB Cray Comparative Debugger. Compare two versions of code side-by-side to analyse differences. (Not currently described in this documentation.)","title":"Debugging"},{"location":"user-guide/debug/#gdb4hpc","text":"The GNU Debugger for HPC (gdb4hpc) is a GDB-based debugger used to debug applications compiled with CCE, PGI, GNU, and Intel Fortran, C and C++ compilers. It allows programmers to either launch an application within it or to attach to an already-running application. Attaching to an already-running and hanging application is a quick way of understanding why the application is hanging, whereas launching an application through gdb4hpc will allow you to see your application running step-by-step, output the values of variables, and check whether the application runs as expected. Tip For your executable to be compatible with gdb4hpc, it will need to be coded with MPI. You will also need to compile your code with the debugging flag -g (e.g. cc -g my_program.c -o my_exe ).","title":"gdb4hpc"},{"location":"user-guide/debug/#launching-through-gdb4hpc","text":"Launch gdb4hpc : module load gdb4hpc gdb4hpc You will get some information about this version of the program and, eventually, you will get a command prompt: gdb4hpc 4.5 - Cray Line Mode Parallel Debugger With Cray Comparative Debugging Technology. Copyright 2007-2019 Cray Inc. All Rights Reserved. Copyright 1996-2016 University of Queensland. All Rights Reserved. Type \"help\" for a list of commands. Type \"help <cmd>\" for detailed help about a command. dbg all> We will use launch to begin a multi-process application within gdb4hpc. Consider that we are wanting to test an application called my_exe , and that we want this to be launched across all 256 processes in two nodes. We would launch this in gdb4hpc by running: dbg all> launch --launcher-args=\"--account=[budget code] --partition=standard --qos=standard --nodes=2 --tasks-per-node=128 --cpus-per-task=1 --exclusive --export=ALL\" $my_prog{256} ./my_ex Make sure to replace the --account input to your budget code ( e.g. if you are using budget t01, that part should look like --account=t01 ). The default launcher is srun and the --launcher-args=\"...\" allows you to set launcher flags for srun . The variable $my_prog is a dummy name for the program being launched and you could use whatever name you want for it -- this will be the name of the srun job that will be run. The number in the brackets {256} is the number of processes over which the program will be executed, it's 256 here, but you could use any number. You should try to run this on as few processors as possible -- the more you use, the longer it will take for gdb4hpc to load the program. Once the program is launched, gdb4hpc will load up the program and begin to run it. You will get output to screen something that looks like: Starting application, please wait... Creating MRNet communication network... Waiting for debug servers to attach to MRNet communications network... Timeout in 400 seconds. Please wait for the attach to complete. Number of dbgsrvs connected: [0]; Timeout Counter: [1] Number of dbgsrvs connected: [0]; Timeout Counter: [2] Number of dbgsrvs connected: [0]; Timeout Counter: [3] Number of dbgsrvs connected: [1]; Timeout Counter: [0] Number of dbgsrvs connected: [1]; Timeout Counter: [1] Number of dbgsrvs connected: [2]; Timeout Counter: [0] Finalizing setup... Launch complete. my_prog{0..255}: Initial breakpoint, main at /PATH/TO/my_program.c:34 The line number at which the initial breakpoint is made (in the above example, line 34) corresponds to the line number at which MPI is initialised. You will not be able to see any parts of the code outside of the MPI region of a code with gdb4hpc. Once the code is loaded, you can use various commands to move through your code. The following lists and describes some of the most useful ones: help -- Lists all gdb4hpc commands. You can run help COMMAND_NAME to learn more about a specific command ( e.g. help launch will tell you about the launch command list -- Will show the current line of code and the 9 lines following. Repeated use of list will move you down the code in ten-line chunks. next -- Will jump to the next step in the program for each process and output which line of code each process is one. It will not enter subroutines. !!! note that there is no reverse-step in gdb4hpc. step -- Like next , but this will step into subroutines. up -- Go up one level in the program ( e.g. from a subroutine back to main). print var -- Prints the value of variable var at this point in the code. watch var -- Like print, but will print whenever a variable changes value. quit -- Exits gdb4hpc. Remember to exit the interactive session once you are done debugging.","title":"Launching through gdb4hpc"},{"location":"user-guide/debug/#attaching-with-gdb4hpc","text":"Attaching to a hanging job using gdb4hpc is a great way of seeing which state each processor is in. However, this does not produce the most visually appealing results. For a more easy-to-read program, please take a look at the STAT tool. In your interactive session, launch your executable as a background task (by adding an & at the end of the command). For example, if you are running an executable called my_exe using 256 processes, you would run: srun -n 256 --nodes=2 --tasks-per-node=128 --cpus-per-task=1 --time=01:00:00 --export=ALL \\ --account=[budget code] --partition=standard --qos=standard ./my_exe & Make sure to replace the --account input to your budget code ( e.g. if you are using budget t01, that part should look like --account=t01 ). You will need to get the full job ID of the job you have just launched. To do this, run: squeue -u $USER and find the job ID associated with this interactive session -- this will be the one with the jobname bash . In this example: JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 1050 workq my_mpi_j jsindt R 0:16 1 nid000001 1051 workq bash jsindt R 0:12 1 nid000002 the appropriate job id is 1051. Next, you will need to run sstat on this job id: sstat 1051 This will output a large amount of information about this specific job. We are looking for the first number of this output, which should look like JOB_ID.## -- the number after the job ID is the number of slurm tasks performed in this interactive session. For our example (where srun is the first slurm task performed), the number is 1051.0. Launch gdb4hpc : module load gdb4hpc gdb4hpc You will get some information about this version of the program and, eventually, you will get a command prompt: gdb4hpc 4.5 - Cray Line Mode Parallel Debugger With Cray Comparative Debugging Technology. Copyright 2007-2019 Cray Inc. All Rights Reserved. Copyright 1996-2016 University of Queensland. All Rights Reserved. Type \"help\" for a list of commands. Type \"help <cmd>\" for detailed help about a command. dbg all> We will be using the attach command to attach to our program that hangs. This is done by writing: dbg all> attach $my_prog JOB_ID.## where JOB_ID.## is the full job ID found using sstat (in our example, this would be 1051.0). The name $my_prog is a dummy-name -- it could be whatever name you like. As it is attaching, gdb4hpc will output text to screen that looks like: Attaching to application, please wait... Creating MRNet communication network... Waiting for debug servers to attach to MRNet communications network... Timeout in 400 seconds. Please wait for the attach to complete. Number of dbgsrvs connected: [0]; Timeout Counter: [1] ... Finalizing setup... Attach complete. Current rank location: After this, you will get an output that, among other things, tells you which line of your code each process is on, and what each process is doing. This can be helpful to see where the hang-up is. If you accidentally attached to the wrong job, you can detach by running: dbg all> release $my_prog and re-attach with the correct job ID. You will need to change your dummy name from $my_prog to something else. When you are finished using gbd4hpc , simply run: dbg all> quit Do not forget to exit your interactive session.","title":"Attaching with gdb4hpc"},{"location":"user-guide/debug/#valgrind4hpc","text":"valgrind4hpc is a Valgrind-based debugging tool to aid in the detection of memory leaks and errors in parallel applications. Valgrind4hpc aggregates any duplicate messages across ranks to help provide an understandable picture of program behavior. Valgrind4hpc manages starting and redirecting output from many copies of Valgrind, as well as recombining and filtering Valgrind messages. If your program can be debugged with Valgrind, it can be debugged with valgrind4hpc. The valgrind4hpc module enables the use of standard valgrind as well as the valgrind4hpc version more suitable to parallel programs.","title":"valgrind4hpc"},{"location":"user-guide/debug/#using-valgrind-with-serial-programs","text":"Launch valgrind4hpc : module load valgrind4hpc Next, run your executable through valgrind: valgrind --tool=memcheck --leak-check=yes my_executable The log outputs to screen. The ERROR SUMMARY will tell you whether, and how many, memory errors there are in your program. Furthermore, if you compile your code using the -g debugging flag ( e.g. gcc -g my_program.c -o my_executable.c ), the log will point out the code lines where the error occurs. Valgrind also includes a tool called Massif that can be used to give insight into the memory usage of your program. It takes regular snapshots and outputs this data into a single file, which can be visualised to show the total amount of memory used as a function of time. This shows when peaks and bottlenecks occur and allows you to identify which data structures in your code are responsible for the largest memory usage of your program. Documentation explaining how to use Massif is available at the official Massif manual . In short, you should run your executable as follows: valgrind --tool=massif my_executable The memory profiling data will be output into a file called massif.out.pid , where pid is the runtime process ID of your program. A custom filename can be chosen using the --massif-out-file option , as follows: valgrind --tool=massif --massif-out-file=optional_filename.out my_executable The output file contains raw profiling statistics. To view a summary including a graphical plot of memory usage over time, use the ms_print command as follows: ms_print massif.out.12345 or, to save to a file: ms_print massif.out.12345 > massif.analysis.12345 This will show total memory usage over time as well as a breakdown of the top data structures contributing to memory usage at each snapshot where there has been a significant allocation or deallocation of memory.","title":"Using Valgrind with serial programs"},{"location":"user-guide/debug/#using-valgrind4hpc-with-parallel-programs","text":"First, load valgrind4hpc : module load valgrind4hpc To run valgrind4hpc, first reserve the resources you will use with salloc . The following reservation request is for 2 nodes (256 physical cores) for 20 minutes on the short queue: auser@uan01:> salloc --nodes=2 --tasks-per-node=128 --cpus-per-task=1 \\ --time=00:20:00 --partition=standard --qos=short \\ --reservation=shortqos --hint=nomultithread \\ --distribution=block:block --account=[budget code] Once your allocation is ready, Use valgrind4hpc to run and profile your executable. To test an executable called my_executable that requires two arguments arg1 and arg2 on 2 nodes and 256 processes, run: valgrind4hpc --tool=memcheck --num-ranks=256 my_executable -- arg1 arg2 In particular, note the -- separating the executable from the arguments (this is not necessary if your executable takes no arguments). Valgrind4hpc only supports certain tools found in valgrind. These are: memcheck, helgrind, exp-sgcheck, or drd. The --valgrind-args=\"arguments\" allows users to use valgrind options not supported in valgrind4hpc ( e.g. --leak-check ) -- note, however, that some of these options might interfere with valgrind4hpc. More information on valgrind4hpc can be found in the manual ( man valgrind4hpc ).","title":"Using Valgrind4hpc with parallel programs"},{"location":"user-guide/debug/#stat","text":"The Stack Trace Analysis Tool (STAT) is a cross-platform debugging tool from the University of Wisconsin-Madison. ATP is based on the same technology as STAT, both are designed to gather and merge stack traces from a running application's parallel processes. The STAT tool can be useful when application seems to be deadlocked or stuck, i.e. they don't crash but they don't progress as expected, and it has been designed to scale to a very large number of processes. Full information on STAT, including use cases, is available at the STAT website . STAT will attach to a running program and query that program to find out where all the processes in that program currently are. It will then process that data and produce a graph displaying the unique process locations (i.e. where all the processes in the running program currently are). To make this easily understandable it collates together all processes that are in the same place providing only unique program locations for display.","title":"STAT"},{"location":"user-guide/debug/#using-stat-on-archer2","text":"On the login node, load the cray-stat module: module load cray-stat Then, launch your job using srun as a background task (by adding an & at the end of the command). For example, if you are running an executable called my_exe using 256 processes, you would run: srun -n 256 --nodes=2 --tasks-per-node=128 --cpus-per-task=1 --time=01:00:00 --export=ALL\\ --account=[budget code] --partition=standard --qos=standard./my_exe & Note This example has set the job time limit to 1 hour -- if you need longer, change the --time command. You will need the Program ID (PID) of the job you have just launched -- the PID is printed to screen upon launch, or you can get it by running: ps -u $USER This will present you with a set of text that looks like this: PID TTY TIME CMD 154296 ? 00:00:00 systemd 154297 ? 00:00:00 (sd-pam) 154302 ? 00:00:00 sshd 154303 pts/8 00:00:00 bash 157150 pts/8 00:00:00 salloc 157152 pts/8 00:00:00 bash 157183 pts/8 00:00:00 srun 157185 pts/8 00:00:00 srun 157191 pts/8 00:00:00 ps Once your application has reached the point where it hangs, issue the following command (replacing PID with the ID of the first srun task -- in the above example, I would replace PID with 157183): stat-cl -i PID You will get an output that looks like this: STAT started at 2020-07-22-13:31:35 Attaching to job launcher (null):157565 and launching tool daemons... Tool daemons launched and connected! Attaching to application... Attached! Application already paused... ignoring request to pause Sampling traces... Traces sampled! Resuming the application... Resumed! Pausing the application... Paused! ... Detaching from application... Detached! Results written to $PATH_TO_RUN_DIRECTORY/stat_results/my_exe.0000 Once STAT is finished, you can kill the srun job using scancel (replacing JID with the job ID of the job you just launched): scancel JID You can view the results that STAT has produced using the following command (note that \"my_exe\" will need to be replaced with the name of the executable you ran): stat-view stat_results/my_exe.0000/00_my_exe.0000.3D.dot This produces a graph displaying all the different places within the program that the parallel processes were when you queried them. Note To see the graph, you will need to have exported your X display when logging in. Larger jobs may spend significant time queueing, requiring submission as a batch job. In this case the above can be adapted for batch submission as follows: #!/bin/bash --login #SBATCH --job-name=test_job #SBATCH --nodes=1 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 #SBATCH --time=2:0:0 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Load additional modules module load cray-stat # Launch job in background srun ./my_exe & sleep 3600 # Wait a sufficient time for job to hang, e.g. 1 hour # Find PID and run STAT pid = $( ps -u ${ USER } | \\ # Get all USER PIDs grep srun | \\ # Find all the sruns head -n 1 | \\ # We want the first one awk '{print $1;}' ) # The PID is the first \"word\" in the line stat-cl -i $pid If the job is hanging it will continue to run until the wall clock exceeds the requested time.","title":"Using STAT on ARCHER2"},{"location":"user-guide/debug/#atp","text":"To enable ATP you should load the atp module and set the ATP_ENABLED environment variable to 1 on the login node: module load atp export ATP_ENABLED=1 Then, launch your job using srun as a background task (by adding an & at the end of the command). For example, if you are running an executable called my_exe using 256 processes, you would run: srun -n=256 --nodes=2 --tasks-per-node=128 --cpus-per-task=1 --time=01:00:00 --export=ALL \\ --account=[budget code] --partition=standard --qos=standard ./my_exe & Note This example has set the job time limit to 1 hour -- if you need longer, change the --time command. Once the job has finished running, load the stat module to view the results: module load cray-stat and view the merged stack trace using: stat-view atpMergedBT.dot Note To see the graph, you will need to have exported your X display when logging in.","title":"ATP"},{"location":"user-guide/dev-environment-4cab/","text":"Application development environment: 4-cabinet system Important This section covers the application development environment on the initial, 4-cabinet ARCHER2 system. For docmentation on the application development environment on the full ARCHER2 system, please see Application development environment: full system . What's available ARCHER2 runs on the Cray Linux Environment (a version of SUSE Linux), and provides a development environment which includes: Software modules via a standard module framework Three different compiler environments (AMD, Cray, and GNU) MPI, OpenMP, and SHMEM Scientific and numerical libraries Parallel Python and R Parallel debugging and profiling Singularity containers Access to particular software, and particular versions, is managed by a standard TCL module framework. Most software is available via standard software modules and the different programming environments are available via module collections. You can see what programming environments are available with: auser@uan01:~> module savelist Named collection list: 1) PrgEnv-aocc 2) PrgEnv-cray 3) PrgEnv-gnu Other software modules can be listed with auser@uan01:~> module avail ------------------------------- /opt/cray/pe/perftools/20.09.0/modulefiles -------------------------------- perftools perftools-lite-events perftools-lite-hbm perftools-nwpc perftools-lite perftools-lite-gpu perftools-lite-loops perftools-preload ---------------------------------- /opt/cray/pe/craype/2.7.0/modulefiles ---------------------------------- craype-hugepages1G craype-hugepages8M craype-hugepages128M craype-network-ofi craype-hugepages2G craype-hugepages16M craype-hugepages256M craype-network-slingshot10 craype-hugepages2M craype-hugepages32M craype-hugepages512M craype-x86-rome craype-hugepages4M craype-hugepages64M craype-network-none ------------------------------------- /usr/local/Modules/modulefiles -------------------------------------- dot module-git module-info modules null use.own -------------------------------------- /opt/cray/pe/cpe-prgenv/7.0.0 -------------------------------------- cpe-aocc cpe-cray cpe-gnu -------------------------------------------- /opt/modulefiles --------------------------------------------- aocc/2.1.0.3(default) cray-R/4.0.2.0(default) gcc/8.1.0 gcc/9.3.0 gcc/10.1.0(default) ---------------------------------------- /opt/cray/pe/modulefiles ----------------------------------------- atp/3.7.4(default) cray-mpich-abi/8.0.15 craype-dl-plugin-py3/20.06.1(default) cce/10.0.3(default) cray-mpich-ucx/8.0.15 craype/2.7.0(default) cray-ccdb/4.7.1(default) cray-mpich/8.0.15(default) craypkg-gen/1.3.10(default) cray-cti/2.7.3(default) cray-netcdf-hdf5parallel/4.7.4.0 gdb4hpc/4.7.3(default) cray-dsmml/0.1.2(default) cray-netcdf/4.7.4.0 iobuf/2.0.10(default) cray-fftw/3.3.8.7(default) cray-openshmemx/11.1.1(default) papi/6.0.0.2(default) cray-ga/5.7.0.3 cray-parallel-netcdf/1.12.1.0 perftools-base/20.09.0(default) cray-hdf5-parallel/1.12.0.0 cray-pmi-lib/6.0.6(default) valgrind4hpc/2.7.2(default) cray-hdf5/1.12.0.0 cray-pmi/6.0.6(default) cray-libsci/20.08.1.2(default) cray-python/3.8.5.0(default) A full discussion of the module system is available in the Software environment section . A consistent set of modules is loaded on login to the machine (currently PrgEnv-cray , see below). Developing applications then means selecting and loading the appropriate set of modules before starting work. This section is aimed at code developers and will concentrate on the compilation environment and building libraries and executables, and specifically parallel executables. Other topics such as Python and Containers are covered in more detail in separate sections of the documentation. Managing development ARCHER2 supports common revision control software such as git . Standard GNU autoconf tools are available, along with make (which is GNU Make). Versions of cmake are available. Note Some of these tools are part of the system software, and typically reside in /usr/bin , while others are provided as part of the module system. Some tools may be available in different versions via both /usr/bin and via the module system. Compilation environment There are three different compiler environments available on ARCHER2: AMD (AOCC), Cray (CCE), and GNU (GCC). The current compiler suite is selected via the programming environment, while the specific compiler versions are determined by the relevant compiler module. A summary is: Suite name Module Programming environment collection CCE cce PrgEnv-cray GCC gcc PrgEnv-gnu AOCC aocc PrgEnv-aocc For example, at login, the default set of modules are: Currently Loaded Modulefiles: 1) cpe-cray 7) cray-dsmml/0.1.2(default) 2) cce/10.0.3(default) 8) perftools-base/20.09.0(default) 3) craype/2.7.0(default) 9) xpmem/2.2.35-7.0.1.0_1.3__gd50fabf.shasta(default) 4) craype-x86-rome 10) cray-mpich/8.0.15(default) 5) libfabric/1.11.0.0.233(default) 11) cray-libsci/20.08.1.2(default) 6) craype-network-ofi from which we see the default programming environment is Cray (indicated by cpe-cray (at 1 in the list above) and the default compiler module is cce/10.0.3 (at 2 in the list above). The programming environment will give access to a consistent set of compiler, MPI library via cray-mpich (at 10), and other libraries e.g., cray-libsci (at 11 in the list above) infrastructure. Within a given programming environment, it is possible to swap to a different compiler version by swapping the relevant compiler module. To ensure consistent behaviour, compilation of C, C++, and Fortran source code should then take place using the appropriate compiler wrapper: cc , CC , and ftn , respectively. The wrapper will automatically call the relevant underlying compiler and add the appropriate include directories and library locations to the invocation. This typically eliminates the need to specify this additional information explicitly in the configuration stage. To see the details of the exact compiler invocation use the -craype-verbose flag to the compiler wrapper. The default link time behaviour is also related to the current programming environment. See the section below on Linking and libraries . Users should not, in general, invoke specific compilers at compile/link stages. In particular, gcc , which may default to /usr/bin/gcc , should not be used. The compiler wrappers cc , CC , and ftn should be used via the appropriate module. Other common MPI compiler wrappers e.g., mpicc should also be replaced by the relevant wrapper cc ( mpicc etc are not available). Important Always use the compiler wrappers cc , CC , and/or ftn and not a specific compiler invocation. This will ensure consistent compile/link time behaviour. Compiler man pages and help Further information on both the compiler wrappers, and the individual compilers themselves are available via the command line, and via standard man pages. The man page for the compiler wrappers is common to all programming environments, while the man page for individual compilers depends on the currently loaded programming environment. The following table summarises options for obtaining information on the compiler and compile options: Compiler suite C C++ Fortran Cray man craycc man crayCC man crayftn GNU man gcc man g++ man gfortran Wrappers man cc man CC man ftn Tip You can also pass the --help option to any of the compilers or wrappers to get a summary of how to use them. The Cray Fortran compiler uses ftn --craype-help to access the help options. Tip There are no man pages for the AOCC compilers at the moment. Tip Cray C/C++ is based on Clang and therefore supports similar options to clang/gcc ( man clang is in fact equivalent to man craycc ). clang --help will produce a full summary of options with Cray-specific options marked \"Cray\". The craycc man page concentrates on these Cray extensions to the clang front end and does not provide an exhaustive description of all clang options. Cray Fortran is not based on Flang and so takes different options from flang/gfortran. Dynamic Linking Executables on ARCHER2 link dynamically, and the Cray Programming Environment does not currently support static linking. This is in contrast to ARCHER where the default was to build statically. If you attempt to link statically, you will see errors similar to: /usr/bin/ld: cannot find -lpmi /usr/bin/ld: cannot find -lpmi2 collect2: error: ld returned 1 exit status The compiler wrapper scripts on ARCHER link runtime libraries in using the runpath by default. This means that the paths to the runtime libraries are encoded into the executable so you do not need to load the compiler environment in your job submission scripts. Which compiler environment? If you are unsure which compiler you should choose, we suggest the starting point should be the GNU compiler collection (GCC, PrgEnv-gnu ); this is perhaps the most commonly used by code developers, particularly in the open source software domain. A portable, standard-conforming code should (in principle) compile in any of the three programming environments. For users requiring specific compiler features, such as co-array Fortran, the recommended starting point would be Cray. The following sections provide further details of the different programming environments. Warning Intel compilers are not available on ARCHER2. AMD Optimizing C/C++ Compiler (AOCC) The AMD Optimizing C/++ Compiler (AOCC) is a clang-based optimising compiler. AOCC (despite its name) includes a flang-based Fortran compiler. Switch the the AOCC programming environment via $ module restore PrgEnv-aocc Note Further details on AOCC will appear here as they become available. AOCC reference material AMD website https://developer.amd.com/amd-aocc/ Cray compiler environment (CCE) The Cray compiler environment (CCE) is the default compiler at the point of login. CCE supports C/C++ (along with unified parallel C UPC), and Fortran (including co-array Fortran). Support for OpenMP parallelism is available for both C/C++ and Fortran (currently OpenMP 4.5, with a number of exceptions). The Cray C/C++ compiler is based on a clang front end, and so compiler options are similar to those for gcc/clang. However, the Fortran compiler remains based around Cray-specific options. Be sure to separate C/C++ compiler options and Fortran compiler options (typically CFLAGS and FFLAGS ) if compiling mixed C/Fortran applications. Switch the the Cray programming environment via $ module restore PrgEnv-cray Useful CCE C/C++ options When using the compiler wrappers cc or CC , some of the following options may be useful: Language, warning, Debugging options: Option Comment -std=<standard> Default is -std=gnu11 ( gnu++14 for C++) [1] Performance options: Option Comment -Ofast Optimisation levels: -O0, -O1, -O2, -O3, -Ofast -ffp=level Floating point maths optimisations levels 0-4 [2] -flto Link time optimisation Miscellaneous options: Option Comment -fopenmp Compile OpenMP (default is off) -v Display verbose output from compiler stages Notes Option -std=gnu11 gives c11 plus GNU extensions (likewise c++14 plus GNU extensions). See https://gcc.gnu.org/onlinedocs/gcc-4.8.2/gcc/C-Extensions.html Option -ffp=3 is implied by -Ofast or -ffast-math Useful CCE Fortran options Language, Warning, Debugging options: Option Comment -m <level> Message level (default -m 3 errors and warnings) Performance options: Option Comment -O <level> Optimisation levels: -O0 to -O3 (default -O2) -h fp<level> Floating point maths optimisations levels 0-3 -h ipa Inter-procedural analysis Miscellaneous options: Option Comment -h omp Compile OpenMP (default is -hnoomp ) -v Display verbose output from compiler stages GNU compiler collection (GCC) The commonly used open source GNU compiler collection is available and provides C/C++ and Fortran compilers. The GNU compiler collection is loaded by switching to the GNU programming environment: $ module restore PrgEnv-gnu Bug The gcc/8.1.0 module is available on ARCHER2 but cannot be used as the supporting scientific and system libraries are not available. You should not use this version of GCC. Warning If you want to use GCC version 10 or greater to compile Fortran code, with the old MPI interfaces (i.e. use mpi or INCLUDE 'mpif.h' ) you must add the -fallow-argument-mismatch option (or equivalent) when compiling otherwise you will see compile errors associated with MPI functions. The reason for this is that past versions of gfortran have allowed mismatched arguments to external procedures (e.g., where an explicit interface is not available). This is often the case for MPI routines using the old MPI interfaces where arrays of different types are passed to, for example, MPI_Send() . This will now generate an error as not standard conforming. The -fallow-argument-mismatch option is used to reduce the error to a warning. The same effect may be achieved via -std=legacy . If you use the Fortran 2008 MPI interface (i.e. use mpi_f08 ) then you should not need to add this option. Fortran language MPI bindings are described in more detail at in the MPI Standard documentation . Useful Gnu Fortran options Option Comment -std=<standard> Default is gnu -fallow-argument-mismatch Allow mismatched procedure arguments. This argument is required for compiling MPI Fortran code with GCC version 10 or greater if you are using the older MPI interfaces (see warning above) -fbounds-check Use runtime checking of array indices -fopenmp Compile OpenMP (default is no OpenMP) -v Display verbose output from compiler stages Tip The standard in -std may be one of f95 f2003 , f2008 or f2018 . The default option -std=gnu is the latest Fortran standard plus gnu extensions. Warning Past versions of gfortran have allowed mismatched arguments to external procedures (e.g., where an explicit interface is not available). This is often the case for MPI routines where arrays of different types are passed to MPI_Send() and so on. This will now generate an error as not standard conforming. Use -fallow-argument-mismatch to reduce the error to a warning. The same effect may be achieved via -std=legacy . Reference material C/C++ documentation https://gcc.gnu.org/onlinedocs/gcc-9.3.0/gcc/ Fortran documentation https://gcc.gnu.org/onlinedocs/gcc-9.3.0/gfortran/ Message passing interface (MPI) HPE Cray MPICH HPE Cray provide, as standard, an MPICH implementation of the message passing interface which is specifically optimised for the ARCHER2 network. The current implementation supports MPI standard version 3.1. The HPE Cray MPICH implementation is linked into software by default when compiling using the standard wrapper scripts: cc , CC and ftn . MPI reference material MPI standard documents: https://www.mpi-forum.org/docs/ Linking and libraries Linking to libraries is performed dynamically on ARCHER2. One can use the -craype-verbose flag to the compiler wrapper to check exactly what linker arguments are invoked. The compiler wrapper scripts encode the paths to the programming environment system libraries using RUNPATH. This ensures that the executable can find the correct runtime libraries without the matching software modules loaded. The library RUNPATH associated with an executable can be inspected via, e.g., $ readelf -d ./a.out (swap a.out for the name of the executable you are querying). Commonly used libraries Modules with names prefixed by cray- are provided by HPE Cray, and are supported to be consistent with any of the programming environments and associated compilers. These modules should be the first choice for access to software libraries if available. Tip More information on the different software libraries on ARCHER2 can be found in the Software libraries section of the user guide. Switching to a different HPE Cray Programming Environment release Important See the section below on using non-default versions of HPE Cray libraries below as this process will generally need to be followed when using software from non-default PE installs. Access to non-default PE environments is controlled by the use of the cpe modules. These modules are typically loaded after you have restored a PrgEnv and loaded all the other modules you need and will set your compile environment to match that in the other PE release. This means: The compiler version will be switched to the one from the selected PE HPE Cray provided libraries (or modules) that are loaded before you switch to the new programming environment are switched to those from the programming environment that you select. For example, if you have a code that uses the Gnu programming environment, FFTW and NetCDF parallel libraries and you want to compile in the (non-default) 21.03 programming environment, you would do the following: First, restore the Gnu programming environment and load the required library modules (FFTW and NetCDF HDF5 parallel). The loaded module list shows they are the versions from the default (20.10) programming environment): auser@uan02:/work/t01/t01/auser> module restore -s PrgEnv-gnu auser@uan02:/work/t01/t01/auser> module load cray-fftw auser@uan02:/work/t01/t01/auser> module load cray-netcdf auser@uan02:/work/t01/t01/auser> module load cray-netcdf-hdf5parallel auser@uan02:/work/t01/t01/auser> module list Currently Loaded Modulefiles: 1) cpe-gnu 9) xpmem/2.2.35-7.0.1.0_1.9__gd50fabf.shasta(default) 2) gcc/10.1.0(default) 10) cray-mpich/8.0.16(default) 3) craype/2.7.2(default) 11) cray-libsci/20.10.1.2(default) 4) craype-x86-rome 12) bolt/0.7 5) libfabric/1.11.0.0.233(default) 13) /work/y07/shared/archer2-modules/modulefiles-cse/epcc-setup-env 6) craype-network-ofi 14) /usr/local/share/epcc-module/epcc-module-loader 7) cray-dsmml/0.1.2(default) 15) cray-fftw/3.3.8.8(default) 8) perftools-base/20.10.0(default) 16) cray-netcdf-hdf5parallel/4.7.4.2(default) Now, load the cpe/21.03 programming environment module to switch all the currently loaded HPE Cray modules from the default (20.10) programming environment version to the 21.03 programming environment versions: auser@uan02:/work/t01/t01/auser> module load cpe/21.03 Switching to cray-dsmml/0.1.3. Switching to cray-fftw/3.3.8.9. Switching to cray-libsci/21.03.1.1. Switching to cray-mpich/8.1.3. Switching to cray-netcdf-hdf5parallel/4.7.4.3. Switching to craype/2.7.5. Switching to gcc/9.3.0. Switching to perftools-base/21.02.0. Loading cpe/21.03 Unloading conflict: cray-dsmml/0.1.2 cray-fftw/3.3.8.8 cray-libsci/20.10.1.2 cray-mpich/8.0.16 cray-netcdf-hdf5parallel/4.7.4.2 craype/2.7.2 gcc/10.1.0 perftools-base/20.10.0 Loading requirement: cray-dsmml/0.1.3 cray-fftw/3.3.8.9 cray-libsci/21.03.1.1 cray-mpich/8.1.3 cray-netcdf-hdf5parallel/4.7.4.3 craype/2.7.5 gcc/9.3.0 perftools-base/21.02.0 auser@uan02:/work/t01/t01/auser> module list Currently Loaded Modulefiles: 1) cpe-gnu 9) cray-dsmml/0.1.3 17) cpe/21.03(default) 2) craype-x86-rome 10) cray-fftw/3.3.8.9 3) libfabric/1.11.0.0.233(default) 11) cray-libsci/21.03.1.1 4) craype-network-ofi 12) cray-mpich/8.1.3 5) xpmem/2.2.35-7.0.1.0_1.9__gd50fabf.shasta(default) 13) cray-netcdf-hdf5parallel/4.7.4.3 6) bolt/0.7 14) craype/2.7.5 7) /work/y07/shared/archer2-modules/modulefiles-cse/epcc-setup-env 15) gcc/9.3.0 8) /usr/local/share/epcc-module/epcc-module-loader 16) perftools-base/21.02.0 Finally (as noted above), you will need to modify the value of LD_LIBRARY_PATH before you compile your software to ensure it picks up the non-default versions of libraries: auser@uan02:/work/t01/t01/auser> export LD_LIBRARY_PATH=$CRAY_LD_LIBRARY_PATH:$LD_LIBRARY_PATH Now you can go ahead and compile your software with the new programming environment. Important The cpe modules only change the versions of software modules provided as part of the HPE Cray programming environments. Any modules provided by the ARCHER2 service will need to be loaded manually after you have completed the process described above. Note Unloading the cpe module does not restore the original programming environment release. To restore the default programming environment release you should log out and then log back in to ARCHER2. Bug The cpe/21.03 module has a known issue with PrgEnv-gnu where it loads an old version of GCC (9.3.0) rather than the correct, newer version (10.2.0). You can resolve this by using the sequence: module restore -s PrgEnv-gnu ...load any other modules you need... module load cpe/21.03 module unload cpe/21.03 module swap gcc gcc/10.2.0 Available HPE Cray Programming Environment releases on ARCHER2 ARCHER2 currently has the following HPE Cray Programming Environment releases available: 20.08: not available via cpe module 20.10: Current default 21.03: available via cpe/21.03 module Tip You can see which programming environment release you currently have loaded by using module list and looking at the version number of the cray-libsci module you have loaded. The first two numbers indicate the version of the PE you have loaded. For example, if you have cray-libsci/20.10.1.2 loaded then you are using the 20.10 PE release. Using non-default versions of HPE Cray libraries on ARCHER2 If you wish to make use of non-default versions of libraries provided by HPE Cray (usually because they are part of a non-default PE release: either old or new) then you need to make changes at both compile and runtime. In summary, you need to load the correct module and also make changes to the LD_LIBRARY_PATH environment variable. At compile time you need to load the version of the library module before you compile and set the LD_LIBRARY_PATH environment variable to include the contencts of $CRAY_LD_LIBRARY_PATH as the first entry. For example, to use the, non-default, 20.08.1.2 version of HPE Cray LibSci in the default programming environment (Cray Compiler Environment, CCE) you would first setup the environment to compile with: auser@uan01:~/test/libsci> module swap cray-libsci cray-libsci/20.08.1.2 auser@uan01:~/test/libsci> export LD_LIBRARY_PATH=$CRAY_LD_LIBRARY_PATH:$LD_LIBRARY_PATH The order is important here: every time you change a module, you will need to reset the value of LD_LIBRARY_PATH for the process to work (it will not be updated automatically). Now you can compile your code. You can check that the executable is using the correct version of LibSci with the ldd command and look for the line beginning libsci_cray.so.5 , you should see the version in the path to the library file: auser@uan01:~/test/libsci> ldd dgemv.x linux-vdso.so.1 (0x00007ffe4a7d2000) libsci_cray.so.5 => /opt/cray/pe/libsci/20.08.1.2/CRAY/9.0/x86_64/lib/libsci_cray.so.5 (0x00007fafd6a43000) libdl.so.2 => /lib64/libdl.so.2 (0x00007fafd683f000) libxpmem.so.0 => /opt/cray/xpmem/default/lib64/libxpmem.so.0 (0x00007fafd663c000) libquadmath.so.0 => /opt/cray/pe/cce/10.0.4/cce/x86_64/lib/libquadmath.so.0 (0x00007fafd63fc000) libmodules.so.1 => /opt/cray/pe/cce/10.0.4/cce/x86_64/lib/libmodules.so.1 (0x00007fafd61e0000) libfi.so.1 => /opt/cray/pe/cce/10.0.4/cce/x86_64/lib/libfi.so.1 (0x00007fafd5abe000) libcraymath.so.1 => /opt/cray/pe/cce/10.0.4/cce/x86_64/lib/libcraymath.so.1 (0x00007fafd57e2000) libf.so.1 => /opt/cray/pe/cce/10.0.4/cce/x86_64/lib/libf.so.1 (0x00007fafd554f000) libu.so.1 => /opt/cray/pe/cce/10.0.4/cce/x86_64/lib/libu.so.1 (0x00007fafd523b000) libcsup.so.1 => /opt/cray/pe/cce/10.0.4/cce/x86_64/lib/libcsup.so.1 (0x00007fafd5035000) libstdc++.so.6 => /opt/cray/pe/gcc-libs/libstdc++.so.6 (0x00007fafd4c62000) libpthread.so.0 => /lib64/libpthread.so.0 (0x00007fafd4a43000) libc.so.6 => /lib64/libc.so.6 (0x00007fafd4688000) libm.so.6 => /lib64/libm.so.6 (0x00007fafd4350000) /lib64/ld-linux-x86-64.so.2 (0x00007fafda988000) librt.so.1 => /lib64/librt.so.1 (0x00007fafd4148000) libgfortran.so.5 => /opt/cray/pe/gcc-libs/libgfortran.so.5 (0x00007fafd3c92000) libgcc_s.so.1 => /opt/cray/pe/gcc-libs/libgcc_s.so.1 (0x00007fafd3a7a000) Tip If any of the libraries point to versions in the /opt/cray/pe/lib64 directory then these are using the default versions of the libraries rather than the specific versions. This happens at compile time if you have forgotton to load the right module and set $LD_LIBRARY_PATH afterwards. At run time (typically in your job script) you need to repeat the environment setup steps (you can also use the ldd command in your job submission script to check the library is pointing to the correct version). For example, a job submission script to run our dgemv.x executable with the non-default version of LibSci could look like: #!/bin/bash #SBATCH --job-name=dgemv #SBATCH --time=0:20:0 #SBATCH --nodes=1 #SBATCH --tasks-per-node=1 #SBATCH --cpus-per-task=1 # Replace the account code, partition and QoS with those you wish to use #SBATCH --account=t01 #SBATCH --partition=standard #SBATCH --qos=short #SBATCH --reservation=shortqos # Load the standard environment module module load epcc-job-env # Setup up the environment to use the non-default version of LibSci # We use \"module swap\" as the \"cray-libsci\" is loaded by default. # This must be done after loading the \"epcc-job-env\" module module swap cray-libsci cray-libsci/20.08.1.2 export LD_LIBRARY_PATH=$CRAY_LD_LIBRARY_PATH:$LD_LIBRARY_PATH # Check which library versions the executable is pointing too ldd dgemv.x export OMP_NUM_THREADS=1 srun --hint=nomultithread --distribution=block:block dgemv.x Tip As when compiling, the order of commands matters. Setting the value of LD_LIBRARY_PATH must happen after you have finished all your module commands for it to have the correct effect. Important You must setup the environment at both compile and run time otherwise you will end up using the default version of the library. Compiling in compute nodes Sometimes you may wish to compile in a batch job. For example, the compile process may take a long time or the compile process is part of the research workflow and can be coupled to the production job. Unlike login nodes, the /home file system is not available. An example job submission script for a compile job using make (assuming the Makefile is in the same directory as the job submission script) would be: #!/bin/bash #SBATCH --job-name=compile #SBATCH --time=00:20:00 #SBATCH --nodes=1 #SBATCH --tasks-per-node=1 #SBATCH --cpus-per-task=1 # Replace the account code, partition and QoS with those you wish to use #SBATCH --account=t01 #SBATCH --partition=standard #SBATCH --qos=standard # Load the compilation environment (cray, gnu or aocc) module restore /etc/cray-pe.d/PrgEnv-cray make clean make Warning Do not forget to include the full path when the compilation environment is restored. For instance: module restore /etc/cray-pe.d/PrgEnv-cray You can also use a compute node in an interactive way using salloc . Please see Section Using salloc to reserve resources for further details. Once your interactive session is ready, you can load the compilation environment and compile the code. Build instructions for software on ARCHER2 The ARCHER2 CSE team at EPCC and other contributors provide build configurations ando instructions for a range of research software, software libraries and tools on a variety of HPC systems (including ARCHER2) in a public Github repository. See: Build instructions repository The repository always welcomes contributions from the ARCHER2 user community. Support for building software on ARCHER2 If you run into issues building software on ARCHER2 or the software you require is not available then please contact the ARCHER2 Service Desk with any questions you have.","title":"Application development environment: 4-cabinet system"},{"location":"user-guide/dev-environment-4cab/#application-development-environment-4-cabinet-system","text":"Important This section covers the application development environment on the initial, 4-cabinet ARCHER2 system. For docmentation on the application development environment on the full ARCHER2 system, please see Application development environment: full system .","title":"Application development environment: 4-cabinet system"},{"location":"user-guide/dev-environment-4cab/#whats-available","text":"ARCHER2 runs on the Cray Linux Environment (a version of SUSE Linux), and provides a development environment which includes: Software modules via a standard module framework Three different compiler environments (AMD, Cray, and GNU) MPI, OpenMP, and SHMEM Scientific and numerical libraries Parallel Python and R Parallel debugging and profiling Singularity containers Access to particular software, and particular versions, is managed by a standard TCL module framework. Most software is available via standard software modules and the different programming environments are available via module collections. You can see what programming environments are available with: auser@uan01:~> module savelist Named collection list: 1) PrgEnv-aocc 2) PrgEnv-cray 3) PrgEnv-gnu Other software modules can be listed with auser@uan01:~> module avail ------------------------------- /opt/cray/pe/perftools/20.09.0/modulefiles -------------------------------- perftools perftools-lite-events perftools-lite-hbm perftools-nwpc perftools-lite perftools-lite-gpu perftools-lite-loops perftools-preload ---------------------------------- /opt/cray/pe/craype/2.7.0/modulefiles ---------------------------------- craype-hugepages1G craype-hugepages8M craype-hugepages128M craype-network-ofi craype-hugepages2G craype-hugepages16M craype-hugepages256M craype-network-slingshot10 craype-hugepages2M craype-hugepages32M craype-hugepages512M craype-x86-rome craype-hugepages4M craype-hugepages64M craype-network-none ------------------------------------- /usr/local/Modules/modulefiles -------------------------------------- dot module-git module-info modules null use.own -------------------------------------- /opt/cray/pe/cpe-prgenv/7.0.0 -------------------------------------- cpe-aocc cpe-cray cpe-gnu -------------------------------------------- /opt/modulefiles --------------------------------------------- aocc/2.1.0.3(default) cray-R/4.0.2.0(default) gcc/8.1.0 gcc/9.3.0 gcc/10.1.0(default) ---------------------------------------- /opt/cray/pe/modulefiles ----------------------------------------- atp/3.7.4(default) cray-mpich-abi/8.0.15 craype-dl-plugin-py3/20.06.1(default) cce/10.0.3(default) cray-mpich-ucx/8.0.15 craype/2.7.0(default) cray-ccdb/4.7.1(default) cray-mpich/8.0.15(default) craypkg-gen/1.3.10(default) cray-cti/2.7.3(default) cray-netcdf-hdf5parallel/4.7.4.0 gdb4hpc/4.7.3(default) cray-dsmml/0.1.2(default) cray-netcdf/4.7.4.0 iobuf/2.0.10(default) cray-fftw/3.3.8.7(default) cray-openshmemx/11.1.1(default) papi/6.0.0.2(default) cray-ga/5.7.0.3 cray-parallel-netcdf/1.12.1.0 perftools-base/20.09.0(default) cray-hdf5-parallel/1.12.0.0 cray-pmi-lib/6.0.6(default) valgrind4hpc/2.7.2(default) cray-hdf5/1.12.0.0 cray-pmi/6.0.6(default) cray-libsci/20.08.1.2(default) cray-python/3.8.5.0(default) A full discussion of the module system is available in the Software environment section . A consistent set of modules is loaded on login to the machine (currently PrgEnv-cray , see below). Developing applications then means selecting and loading the appropriate set of modules before starting work. This section is aimed at code developers and will concentrate on the compilation environment and building libraries and executables, and specifically parallel executables. Other topics such as Python and Containers are covered in more detail in separate sections of the documentation.","title":"What's available"},{"location":"user-guide/dev-environment-4cab/#managing-development","text":"ARCHER2 supports common revision control software such as git . Standard GNU autoconf tools are available, along with make (which is GNU Make). Versions of cmake are available. Note Some of these tools are part of the system software, and typically reside in /usr/bin , while others are provided as part of the module system. Some tools may be available in different versions via both /usr/bin and via the module system.","title":"Managing development"},{"location":"user-guide/dev-environment-4cab/#compilation-environment","text":"There are three different compiler environments available on ARCHER2: AMD (AOCC), Cray (CCE), and GNU (GCC). The current compiler suite is selected via the programming environment, while the specific compiler versions are determined by the relevant compiler module. A summary is: Suite name Module Programming environment collection CCE cce PrgEnv-cray GCC gcc PrgEnv-gnu AOCC aocc PrgEnv-aocc For example, at login, the default set of modules are: Currently Loaded Modulefiles: 1) cpe-cray 7) cray-dsmml/0.1.2(default) 2) cce/10.0.3(default) 8) perftools-base/20.09.0(default) 3) craype/2.7.0(default) 9) xpmem/2.2.35-7.0.1.0_1.3__gd50fabf.shasta(default) 4) craype-x86-rome 10) cray-mpich/8.0.15(default) 5) libfabric/1.11.0.0.233(default) 11) cray-libsci/20.08.1.2(default) 6) craype-network-ofi from which we see the default programming environment is Cray (indicated by cpe-cray (at 1 in the list above) and the default compiler module is cce/10.0.3 (at 2 in the list above). The programming environment will give access to a consistent set of compiler, MPI library via cray-mpich (at 10), and other libraries e.g., cray-libsci (at 11 in the list above) infrastructure. Within a given programming environment, it is possible to swap to a different compiler version by swapping the relevant compiler module. To ensure consistent behaviour, compilation of C, C++, and Fortran source code should then take place using the appropriate compiler wrapper: cc , CC , and ftn , respectively. The wrapper will automatically call the relevant underlying compiler and add the appropriate include directories and library locations to the invocation. This typically eliminates the need to specify this additional information explicitly in the configuration stage. To see the details of the exact compiler invocation use the -craype-verbose flag to the compiler wrapper. The default link time behaviour is also related to the current programming environment. See the section below on Linking and libraries . Users should not, in general, invoke specific compilers at compile/link stages. In particular, gcc , which may default to /usr/bin/gcc , should not be used. The compiler wrappers cc , CC , and ftn should be used via the appropriate module. Other common MPI compiler wrappers e.g., mpicc should also be replaced by the relevant wrapper cc ( mpicc etc are not available). Important Always use the compiler wrappers cc , CC , and/or ftn and not a specific compiler invocation. This will ensure consistent compile/link time behaviour.","title":"Compilation environment"},{"location":"user-guide/dev-environment-4cab/#compiler-man-pages-and-help","text":"Further information on both the compiler wrappers, and the individual compilers themselves are available via the command line, and via standard man pages. The man page for the compiler wrappers is common to all programming environments, while the man page for individual compilers depends on the currently loaded programming environment. The following table summarises options for obtaining information on the compiler and compile options: Compiler suite C C++ Fortran Cray man craycc man crayCC man crayftn GNU man gcc man g++ man gfortran Wrappers man cc man CC man ftn Tip You can also pass the --help option to any of the compilers or wrappers to get a summary of how to use them. The Cray Fortran compiler uses ftn --craype-help to access the help options. Tip There are no man pages for the AOCC compilers at the moment. Tip Cray C/C++ is based on Clang and therefore supports similar options to clang/gcc ( man clang is in fact equivalent to man craycc ). clang --help will produce a full summary of options with Cray-specific options marked \"Cray\". The craycc man page concentrates on these Cray extensions to the clang front end and does not provide an exhaustive description of all clang options. Cray Fortran is not based on Flang and so takes different options from flang/gfortran.","title":"Compiler man pages and help"},{"location":"user-guide/dev-environment-4cab/#dynamic-linking","text":"Executables on ARCHER2 link dynamically, and the Cray Programming Environment does not currently support static linking. This is in contrast to ARCHER where the default was to build statically. If you attempt to link statically, you will see errors similar to: /usr/bin/ld: cannot find -lpmi /usr/bin/ld: cannot find -lpmi2 collect2: error: ld returned 1 exit status The compiler wrapper scripts on ARCHER link runtime libraries in using the runpath by default. This means that the paths to the runtime libraries are encoded into the executable so you do not need to load the compiler environment in your job submission scripts.","title":"Dynamic Linking"},{"location":"user-guide/dev-environment-4cab/#which-compiler-environment","text":"If you are unsure which compiler you should choose, we suggest the starting point should be the GNU compiler collection (GCC, PrgEnv-gnu ); this is perhaps the most commonly used by code developers, particularly in the open source software domain. A portable, standard-conforming code should (in principle) compile in any of the three programming environments. For users requiring specific compiler features, such as co-array Fortran, the recommended starting point would be Cray. The following sections provide further details of the different programming environments. Warning Intel compilers are not available on ARCHER2.","title":"Which compiler environment?"},{"location":"user-guide/dev-environment-4cab/#amd-optimizing-cc-compiler-aocc","text":"The AMD Optimizing C/++ Compiler (AOCC) is a clang-based optimising compiler. AOCC (despite its name) includes a flang-based Fortran compiler. Switch the the AOCC programming environment via $ module restore PrgEnv-aocc Note Further details on AOCC will appear here as they become available.","title":"AMD Optimizing C/C++ Compiler (AOCC)"},{"location":"user-guide/dev-environment-4cab/#aocc-reference-material","text":"AMD website https://developer.amd.com/amd-aocc/","title":"AOCC reference material"},{"location":"user-guide/dev-environment-4cab/#cray-compiler-environment-cce","text":"The Cray compiler environment (CCE) is the default compiler at the point of login. CCE supports C/C++ (along with unified parallel C UPC), and Fortran (including co-array Fortran). Support for OpenMP parallelism is available for both C/C++ and Fortran (currently OpenMP 4.5, with a number of exceptions). The Cray C/C++ compiler is based on a clang front end, and so compiler options are similar to those for gcc/clang. However, the Fortran compiler remains based around Cray-specific options. Be sure to separate C/C++ compiler options and Fortran compiler options (typically CFLAGS and FFLAGS ) if compiling mixed C/Fortran applications. Switch the the Cray programming environment via $ module restore PrgEnv-cray","title":"Cray compiler environment (CCE)"},{"location":"user-guide/dev-environment-4cab/#useful-cce-cc-options","text":"When using the compiler wrappers cc or CC , some of the following options may be useful: Language, warning, Debugging options: Option Comment -std=<standard> Default is -std=gnu11 ( gnu++14 for C++) [1] Performance options: Option Comment -Ofast Optimisation levels: -O0, -O1, -O2, -O3, -Ofast -ffp=level Floating point maths optimisations levels 0-4 [2] -flto Link time optimisation Miscellaneous options: Option Comment -fopenmp Compile OpenMP (default is off) -v Display verbose output from compiler stages Notes Option -std=gnu11 gives c11 plus GNU extensions (likewise c++14 plus GNU extensions). See https://gcc.gnu.org/onlinedocs/gcc-4.8.2/gcc/C-Extensions.html Option -ffp=3 is implied by -Ofast or -ffast-math","title":"Useful CCE C/C++ options"},{"location":"user-guide/dev-environment-4cab/#useful-cce-fortran-options","text":"Language, Warning, Debugging options: Option Comment -m <level> Message level (default -m 3 errors and warnings) Performance options: Option Comment -O <level> Optimisation levels: -O0 to -O3 (default -O2) -h fp<level> Floating point maths optimisations levels 0-3 -h ipa Inter-procedural analysis Miscellaneous options: Option Comment -h omp Compile OpenMP (default is -hnoomp ) -v Display verbose output from compiler stages","title":"Useful CCE Fortran options"},{"location":"user-guide/dev-environment-4cab/#gnu-compiler-collection-gcc","text":"The commonly used open source GNU compiler collection is available and provides C/C++ and Fortran compilers. The GNU compiler collection is loaded by switching to the GNU programming environment: $ module restore PrgEnv-gnu Bug The gcc/8.1.0 module is available on ARCHER2 but cannot be used as the supporting scientific and system libraries are not available. You should not use this version of GCC. Warning If you want to use GCC version 10 or greater to compile Fortran code, with the old MPI interfaces (i.e. use mpi or INCLUDE 'mpif.h' ) you must add the -fallow-argument-mismatch option (or equivalent) when compiling otherwise you will see compile errors associated with MPI functions. The reason for this is that past versions of gfortran have allowed mismatched arguments to external procedures (e.g., where an explicit interface is not available). This is often the case for MPI routines using the old MPI interfaces where arrays of different types are passed to, for example, MPI_Send() . This will now generate an error as not standard conforming. The -fallow-argument-mismatch option is used to reduce the error to a warning. The same effect may be achieved via -std=legacy . If you use the Fortran 2008 MPI interface (i.e. use mpi_f08 ) then you should not need to add this option. Fortran language MPI bindings are described in more detail at in the MPI Standard documentation .","title":"GNU compiler collection (GCC)"},{"location":"user-guide/dev-environment-4cab/#useful-gnu-fortran-options","text":"Option Comment -std=<standard> Default is gnu -fallow-argument-mismatch Allow mismatched procedure arguments. This argument is required for compiling MPI Fortran code with GCC version 10 or greater if you are using the older MPI interfaces (see warning above) -fbounds-check Use runtime checking of array indices -fopenmp Compile OpenMP (default is no OpenMP) -v Display verbose output from compiler stages Tip The standard in -std may be one of f95 f2003 , f2008 or f2018 . The default option -std=gnu is the latest Fortran standard plus gnu extensions. Warning Past versions of gfortran have allowed mismatched arguments to external procedures (e.g., where an explicit interface is not available). This is often the case for MPI routines where arrays of different types are passed to MPI_Send() and so on. This will now generate an error as not standard conforming. Use -fallow-argument-mismatch to reduce the error to a warning. The same effect may be achieved via -std=legacy .","title":"Useful Gnu Fortran options"},{"location":"user-guide/dev-environment-4cab/#reference-material","text":"C/C++ documentation https://gcc.gnu.org/onlinedocs/gcc-9.3.0/gcc/ Fortran documentation https://gcc.gnu.org/onlinedocs/gcc-9.3.0/gfortran/","title":"Reference material"},{"location":"user-guide/dev-environment-4cab/#message-passing-interface-mpi","text":"","title":"Message passing interface (MPI)"},{"location":"user-guide/dev-environment-4cab/#hpe-cray-mpich","text":"HPE Cray provide, as standard, an MPICH implementation of the message passing interface which is specifically optimised for the ARCHER2 network. The current implementation supports MPI standard version 3.1. The HPE Cray MPICH implementation is linked into software by default when compiling using the standard wrapper scripts: cc , CC and ftn .","title":"HPE Cray MPICH"},{"location":"user-guide/dev-environment-4cab/#mpi-reference-material","text":"MPI standard documents: https://www.mpi-forum.org/docs/","title":"MPI reference material"},{"location":"user-guide/dev-environment-4cab/#linking-and-libraries","text":"Linking to libraries is performed dynamically on ARCHER2. One can use the -craype-verbose flag to the compiler wrapper to check exactly what linker arguments are invoked. The compiler wrapper scripts encode the paths to the programming environment system libraries using RUNPATH. This ensures that the executable can find the correct runtime libraries without the matching software modules loaded. The library RUNPATH associated with an executable can be inspected via, e.g., $ readelf -d ./a.out (swap a.out for the name of the executable you are querying).","title":"Linking and libraries"},{"location":"user-guide/dev-environment-4cab/#commonly-used-libraries","text":"Modules with names prefixed by cray- are provided by HPE Cray, and are supported to be consistent with any of the programming environments and associated compilers. These modules should be the first choice for access to software libraries if available. Tip More information on the different software libraries on ARCHER2 can be found in the Software libraries section of the user guide.","title":"Commonly used libraries"},{"location":"user-guide/dev-environment-4cab/#switching-to-a-different-hpe-cray-programming-environment-release","text":"Important See the section below on using non-default versions of HPE Cray libraries below as this process will generally need to be followed when using software from non-default PE installs. Access to non-default PE environments is controlled by the use of the cpe modules. These modules are typically loaded after you have restored a PrgEnv and loaded all the other modules you need and will set your compile environment to match that in the other PE release. This means: The compiler version will be switched to the one from the selected PE HPE Cray provided libraries (or modules) that are loaded before you switch to the new programming environment are switched to those from the programming environment that you select. For example, if you have a code that uses the Gnu programming environment, FFTW and NetCDF parallel libraries and you want to compile in the (non-default) 21.03 programming environment, you would do the following: First, restore the Gnu programming environment and load the required library modules (FFTW and NetCDF HDF5 parallel). The loaded module list shows they are the versions from the default (20.10) programming environment): auser@uan02:/work/t01/t01/auser> module restore -s PrgEnv-gnu auser@uan02:/work/t01/t01/auser> module load cray-fftw auser@uan02:/work/t01/t01/auser> module load cray-netcdf auser@uan02:/work/t01/t01/auser> module load cray-netcdf-hdf5parallel auser@uan02:/work/t01/t01/auser> module list Currently Loaded Modulefiles: 1) cpe-gnu 9) xpmem/2.2.35-7.0.1.0_1.9__gd50fabf.shasta(default) 2) gcc/10.1.0(default) 10) cray-mpich/8.0.16(default) 3) craype/2.7.2(default) 11) cray-libsci/20.10.1.2(default) 4) craype-x86-rome 12) bolt/0.7 5) libfabric/1.11.0.0.233(default) 13) /work/y07/shared/archer2-modules/modulefiles-cse/epcc-setup-env 6) craype-network-ofi 14) /usr/local/share/epcc-module/epcc-module-loader 7) cray-dsmml/0.1.2(default) 15) cray-fftw/3.3.8.8(default) 8) perftools-base/20.10.0(default) 16) cray-netcdf-hdf5parallel/4.7.4.2(default) Now, load the cpe/21.03 programming environment module to switch all the currently loaded HPE Cray modules from the default (20.10) programming environment version to the 21.03 programming environment versions: auser@uan02:/work/t01/t01/auser> module load cpe/21.03 Switching to cray-dsmml/0.1.3. Switching to cray-fftw/3.3.8.9. Switching to cray-libsci/21.03.1.1. Switching to cray-mpich/8.1.3. Switching to cray-netcdf-hdf5parallel/4.7.4.3. Switching to craype/2.7.5. Switching to gcc/9.3.0. Switching to perftools-base/21.02.0. Loading cpe/21.03 Unloading conflict: cray-dsmml/0.1.2 cray-fftw/3.3.8.8 cray-libsci/20.10.1.2 cray-mpich/8.0.16 cray-netcdf-hdf5parallel/4.7.4.2 craype/2.7.2 gcc/10.1.0 perftools-base/20.10.0 Loading requirement: cray-dsmml/0.1.3 cray-fftw/3.3.8.9 cray-libsci/21.03.1.1 cray-mpich/8.1.3 cray-netcdf-hdf5parallel/4.7.4.3 craype/2.7.5 gcc/9.3.0 perftools-base/21.02.0 auser@uan02:/work/t01/t01/auser> module list Currently Loaded Modulefiles: 1) cpe-gnu 9) cray-dsmml/0.1.3 17) cpe/21.03(default) 2) craype-x86-rome 10) cray-fftw/3.3.8.9 3) libfabric/1.11.0.0.233(default) 11) cray-libsci/21.03.1.1 4) craype-network-ofi 12) cray-mpich/8.1.3 5) xpmem/2.2.35-7.0.1.0_1.9__gd50fabf.shasta(default) 13) cray-netcdf-hdf5parallel/4.7.4.3 6) bolt/0.7 14) craype/2.7.5 7) /work/y07/shared/archer2-modules/modulefiles-cse/epcc-setup-env 15) gcc/9.3.0 8) /usr/local/share/epcc-module/epcc-module-loader 16) perftools-base/21.02.0 Finally (as noted above), you will need to modify the value of LD_LIBRARY_PATH before you compile your software to ensure it picks up the non-default versions of libraries: auser@uan02:/work/t01/t01/auser> export LD_LIBRARY_PATH=$CRAY_LD_LIBRARY_PATH:$LD_LIBRARY_PATH Now you can go ahead and compile your software with the new programming environment. Important The cpe modules only change the versions of software modules provided as part of the HPE Cray programming environments. Any modules provided by the ARCHER2 service will need to be loaded manually after you have completed the process described above. Note Unloading the cpe module does not restore the original programming environment release. To restore the default programming environment release you should log out and then log back in to ARCHER2. Bug The cpe/21.03 module has a known issue with PrgEnv-gnu where it loads an old version of GCC (9.3.0) rather than the correct, newer version (10.2.0). You can resolve this by using the sequence: module restore -s PrgEnv-gnu ...load any other modules you need... module load cpe/21.03 module unload cpe/21.03 module swap gcc gcc/10.2.0","title":"Switching to a different HPE Cray Programming Environment release"},{"location":"user-guide/dev-environment-4cab/#available-hpe-cray-programming-environment-releases-on-archer2","text":"ARCHER2 currently has the following HPE Cray Programming Environment releases available: 20.08: not available via cpe module 20.10: Current default 21.03: available via cpe/21.03 module Tip You can see which programming environment release you currently have loaded by using module list and looking at the version number of the cray-libsci module you have loaded. The first two numbers indicate the version of the PE you have loaded. For example, if you have cray-libsci/20.10.1.2 loaded then you are using the 20.10 PE release.","title":"Available HPE Cray Programming Environment releases on ARCHER2"},{"location":"user-guide/dev-environment-4cab/#using-non-default-versions-of-hpe-cray-libraries-on-archer2","text":"If you wish to make use of non-default versions of libraries provided by HPE Cray (usually because they are part of a non-default PE release: either old or new) then you need to make changes at both compile and runtime. In summary, you need to load the correct module and also make changes to the LD_LIBRARY_PATH environment variable. At compile time you need to load the version of the library module before you compile and set the LD_LIBRARY_PATH environment variable to include the contencts of $CRAY_LD_LIBRARY_PATH as the first entry. For example, to use the, non-default, 20.08.1.2 version of HPE Cray LibSci in the default programming environment (Cray Compiler Environment, CCE) you would first setup the environment to compile with: auser@uan01:~/test/libsci> module swap cray-libsci cray-libsci/20.08.1.2 auser@uan01:~/test/libsci> export LD_LIBRARY_PATH=$CRAY_LD_LIBRARY_PATH:$LD_LIBRARY_PATH The order is important here: every time you change a module, you will need to reset the value of LD_LIBRARY_PATH for the process to work (it will not be updated automatically). Now you can compile your code. You can check that the executable is using the correct version of LibSci with the ldd command and look for the line beginning libsci_cray.so.5 , you should see the version in the path to the library file: auser@uan01:~/test/libsci> ldd dgemv.x linux-vdso.so.1 (0x00007ffe4a7d2000) libsci_cray.so.5 => /opt/cray/pe/libsci/20.08.1.2/CRAY/9.0/x86_64/lib/libsci_cray.so.5 (0x00007fafd6a43000) libdl.so.2 => /lib64/libdl.so.2 (0x00007fafd683f000) libxpmem.so.0 => /opt/cray/xpmem/default/lib64/libxpmem.so.0 (0x00007fafd663c000) libquadmath.so.0 => /opt/cray/pe/cce/10.0.4/cce/x86_64/lib/libquadmath.so.0 (0x00007fafd63fc000) libmodules.so.1 => /opt/cray/pe/cce/10.0.4/cce/x86_64/lib/libmodules.so.1 (0x00007fafd61e0000) libfi.so.1 => /opt/cray/pe/cce/10.0.4/cce/x86_64/lib/libfi.so.1 (0x00007fafd5abe000) libcraymath.so.1 => /opt/cray/pe/cce/10.0.4/cce/x86_64/lib/libcraymath.so.1 (0x00007fafd57e2000) libf.so.1 => /opt/cray/pe/cce/10.0.4/cce/x86_64/lib/libf.so.1 (0x00007fafd554f000) libu.so.1 => /opt/cray/pe/cce/10.0.4/cce/x86_64/lib/libu.so.1 (0x00007fafd523b000) libcsup.so.1 => /opt/cray/pe/cce/10.0.4/cce/x86_64/lib/libcsup.so.1 (0x00007fafd5035000) libstdc++.so.6 => /opt/cray/pe/gcc-libs/libstdc++.so.6 (0x00007fafd4c62000) libpthread.so.0 => /lib64/libpthread.so.0 (0x00007fafd4a43000) libc.so.6 => /lib64/libc.so.6 (0x00007fafd4688000) libm.so.6 => /lib64/libm.so.6 (0x00007fafd4350000) /lib64/ld-linux-x86-64.so.2 (0x00007fafda988000) librt.so.1 => /lib64/librt.so.1 (0x00007fafd4148000) libgfortran.so.5 => /opt/cray/pe/gcc-libs/libgfortran.so.5 (0x00007fafd3c92000) libgcc_s.so.1 => /opt/cray/pe/gcc-libs/libgcc_s.so.1 (0x00007fafd3a7a000) Tip If any of the libraries point to versions in the /opt/cray/pe/lib64 directory then these are using the default versions of the libraries rather than the specific versions. This happens at compile time if you have forgotton to load the right module and set $LD_LIBRARY_PATH afterwards. At run time (typically in your job script) you need to repeat the environment setup steps (you can also use the ldd command in your job submission script to check the library is pointing to the correct version). For example, a job submission script to run our dgemv.x executable with the non-default version of LibSci could look like: #!/bin/bash #SBATCH --job-name=dgemv #SBATCH --time=0:20:0 #SBATCH --nodes=1 #SBATCH --tasks-per-node=1 #SBATCH --cpus-per-task=1 # Replace the account code, partition and QoS with those you wish to use #SBATCH --account=t01 #SBATCH --partition=standard #SBATCH --qos=short #SBATCH --reservation=shortqos # Load the standard environment module module load epcc-job-env # Setup up the environment to use the non-default version of LibSci # We use \"module swap\" as the \"cray-libsci\" is loaded by default. # This must be done after loading the \"epcc-job-env\" module module swap cray-libsci cray-libsci/20.08.1.2 export LD_LIBRARY_PATH=$CRAY_LD_LIBRARY_PATH:$LD_LIBRARY_PATH # Check which library versions the executable is pointing too ldd dgemv.x export OMP_NUM_THREADS=1 srun --hint=nomultithread --distribution=block:block dgemv.x Tip As when compiling, the order of commands matters. Setting the value of LD_LIBRARY_PATH must happen after you have finished all your module commands for it to have the correct effect. Important You must setup the environment at both compile and run time otherwise you will end up using the default version of the library.","title":"Using non-default versions of HPE Cray libraries on ARCHER2"},{"location":"user-guide/dev-environment-4cab/#compiling-in-compute-nodes","text":"Sometimes you may wish to compile in a batch job. For example, the compile process may take a long time or the compile process is part of the research workflow and can be coupled to the production job. Unlike login nodes, the /home file system is not available. An example job submission script for a compile job using make (assuming the Makefile is in the same directory as the job submission script) would be: #!/bin/bash #SBATCH --job-name=compile #SBATCH --time=00:20:00 #SBATCH --nodes=1 #SBATCH --tasks-per-node=1 #SBATCH --cpus-per-task=1 # Replace the account code, partition and QoS with those you wish to use #SBATCH --account=t01 #SBATCH --partition=standard #SBATCH --qos=standard # Load the compilation environment (cray, gnu or aocc) module restore /etc/cray-pe.d/PrgEnv-cray make clean make Warning Do not forget to include the full path when the compilation environment is restored. For instance: module restore /etc/cray-pe.d/PrgEnv-cray You can also use a compute node in an interactive way using salloc . Please see Section Using salloc to reserve resources for further details. Once your interactive session is ready, you can load the compilation environment and compile the code.","title":"Compiling in compute nodes"},{"location":"user-guide/dev-environment-4cab/#build-instructions-for-software-on-archer2","text":"The ARCHER2 CSE team at EPCC and other contributors provide build configurations ando instructions for a range of research software, software libraries and tools on a variety of HPC systems (including ARCHER2) in a public Github repository. See: Build instructions repository The repository always welcomes contributions from the ARCHER2 user community.","title":"Build instructions for software on ARCHER2"},{"location":"user-guide/dev-environment-4cab/#support-for-building-software-on-archer2","text":"If you run into issues building software on ARCHER2 or the software you require is not available then please contact the ARCHER2 Service Desk with any questions you have.","title":"Support for building software on ARCHER2"},{"location":"user-guide/dev-environment/","text":"Application development environment What's available ARCHER2 runs the HPE Cray Linux Environment (a version of SUSE Linux), and provides a development environment which includes: Software modules via a standard module framework Three different compiler environments (AMD, Cray, and GNU) MPI, OpenMP, and SHMEM Scientific and numerical libraries Parallel Python and R Parallel debugging and profiling Singularity containers Access to particular software, and particular versions, is managed by an Lmod module framework. Most software is available by loading modules, including the different compiler environments You can see what compiler environments are available with: auser@uan01:~> module avail PrgEnv ------------------------------------------ /opt/cray/pe/lmod/modulefiles/core ------------------------------------------ PrgEnv-aocc/8.0.0 (D) PrgEnv-cray/8.0.0 (L,D) PrgEnv-gnu/8.0.0 (D) PrgEnv-aocc/8.1.0 PrgEnv-cray/8.1.0 PrgEnv-gnu/8.1.0 Where: L: Module is loaded D: Default Module Use \"module spider\" to find all possible modules and extensions. Use \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\". Other software modules can be searched using the module spider command: auser@uan01:~> module spider -------------------------------------------------------------------------------------------------------------------- The following is a list of the modules and extensions currently available: -------------------------------------------------------------------------------------------------------------------- PrgEnv-aocc: PrgEnv-aocc/8.0.0, PrgEnv-aocc/8.1.0 PrgEnv-cray: PrgEnv-cray/8.0.0, PrgEnv-cray/8.1.0 PrgEnv-gnu: PrgEnv-gnu/8.0.0, PrgEnv-gnu/8.1.0 aocc: aocc/2.2.0.1, aocc/3.0.0 atp: atp/3.13.1, atp/3.14.5 cce: cce/11.0.4, cce/12.0.3 cpe: cpe/21.04, cpe/21.09 ...output trimmed... perftools-lite-hbm: perftools-lite-hbm perftools-lite-loops: perftools-lite-loops perftools-preload: perftools-preload settarg: settarg valgrind4hpc: valgrind4hpc/2.11.1, valgrind4hpc/2.12.4 xpmem: xpmem/2.2.40-7.0.1.0_2.7__g1d7a24d.shasta -------------------------------------------------------------------------------------------------------------------- To learn more about a package execute: $ module spider Foo where \"Foo\" is the name of a module. To find detailed information about a particular package you must specify the version if there is more than one version: $ module spider Foo/11.1 -------------------------------------------------------------------------------------------------------------------- A full discussion of the module system is available in the Software environment section . A consistent set of modules is loaded on login to the machine (currently PrgEnv-cray , see below). Developing applications then means selecting and loading the appropriate set of modules before starting work. This section is aimed at code developers and will concentrate on the compilation environment, building libraries and executables, specifically parallel executables. Other topics such as Python and Containers are covered in more detail in separate sections of the documentation. Tip If you want to get back to the login module state without having to logout and back in again, you can just use: module restore This is also handy for build scripts to ensure you are starting from a known state. Compiler environments There are three different compiler environments available on ARCHER2: AMD Compiler Collection (AOCC) GNU Compiler Collection (GCC) HPE Cray Compiler Collection (CCE) (current default compiler environment) The current compiler suite is selected via the PrgEnv module , while the specific compiler versions are determined by the relevant compiler module. A summary is: Suite name Compiler Environment Module Compiler Version Module CCE PrgEnv-cray cce GCC PrgEnv-gnu gcc AOCC PrgEnv-aocc aocc For example, at login, the default set of modules are: auser@ln03:~> module list Currently Loaded Modules: 1) cce/11.0.4 6) perftools-base/21.02.0 2) craype/2.7.6 7) xpmem/2.2.40-7.0.1.0_2.7__g1d7a24d.shasta 3) craype-x86-rome 8) cray-mpich/8.1.4 4) libfabric/1.11.0.4.71 9) cray-libsci/21.04.1.1 5) craype-network-ofi 10) PrgEnv-cray/8.0.0 from which we see the default compiler environment is Cray (indicated by PrgEnv-cray (at 10 in the list above) and the default compiler module is cce/11.0.4 (at 1 in the list above). The compiler environment will give access to a consistent set of compiler, MPI library via cray-mpich (at 8), and other libraries e.g., cray-libsci (at 9 in the list above) infrastructure. Switching between compiler environments Switching between different compiler environments is achieved using the module swap command. For example, to switch from the default HPE Cray (CCE) compiler environment to the GCC environment, you would use: auser@ln03:~> module swap PrgEnv-cray PrgEnv-gnu Due to MODULEPATH changes, the following have been reloaded: 1) cray-mpich/8.1.4 If you then use the module list command, you will see that your environment has been changed to the GCC environment: auser@ln03:~> module list Currently Loaded Modules: 1) gcc/10.2.0 6) perftools-base/21.02.0 2) craype/2.7.6 7) xpmem/2.2.40-7.0.1.0_2.7__g1d7a24d.shasta 3) craype-x86-rome 8) cray-mpich/8.1.4 4) libfabric/1.11.0.4.71 9) cray-libsci/21.04.1.1 5) craype-network-ofi 10) PrgEnv-gnu/8.0.0 Switching between compiler versions Within a given compiler environment, it is possible to swap to a different compiler version by swapping the relevant compiler module. To switch to the GNU compiler environment from the default HPE Cray compiler environment and than swap the version of GCC from the 11.2.0 default to the older 10.2.0 version, you would use auser@ln03:~> module swap PrgEnv-cray PrgEnv-gnu Due to MODULEPATH changes, the following have been reloaded: 1) cray-mpich/8.1.4 auser@ln03:~> module swap gcc gcc/11.2.0 The following have been reloaded with a version change: 1) gcc/10.2.0 => gcc/11.2.0 The first swap command moves to the GNU compiler environment and the second swap command moves to the older version of GCC. As before, module list will show that your environment has been changed: auser@ln03:~> module list Currently Loaded Modules: 1) craype/2.7.6 5) perftools-base/21.02.0 9) gcc/11.2.0 2) craype-x86-rome 6) xpmem/2.2.40-7.0.1.0_2.7__g1d7a24d.shasta 10) cray-mpich/8.1.4 3) libfabric/1.11.0.4.71 7) cray-libsci/21.04.1.1 4) craype-network-ofi 8) PrgEnv-gnu/8.0.0 Compiler wrapper scripts: cc , CC , ftn To ensure consistent behaviour, compilation of C, C++, and Fortran source code should then take place using the appropriate compiler wrapper: cc , CC , and ftn , respectively. The wrapper will automatically call the relevant underlying compiler and add the appropriate include directories and library locations to the invocation. This typically eliminates the need to specify this additional information explicitly in the configuration stage. To see the details of the exact compiler invocation use the -craype-verbose flag to the compiler wrapper. The default link time behaviour is also related to the current programming environment. See the section below on Linking and libraries . Users should not, in general, invoke specific compilers at compile/link stages. In particular, gcc , which may default to /usr/bin/gcc , should not be used. The compiler wrappers cc , CC , and ftn should be used (with the underlying compiler type and version set by the module system). Other common MPI compiler wrappers e.g., mpicc , should also be replaced by the relevant wrapper, e.g. cc (commands such as mpicc are not available on ARCHER2). Important Always use the compiler wrappers cc , CC , and/or ftn and not a specific compiler invocation. This will ensure consistent compile/link time behaviour. Tip If you are using a build system such as Make or CMake then you will need to replace all occurrences of mpicc with cc , mpicxx / mpic++ with CC and mpif90 with ftn . Compiler man pages and help Further information on both the compiler wrappers, and the individual compilers themselves are available via the command line, and via standard man pages. The man page for the compiler wrappers is common to all programming environments, while the man page for individual compilers depends on the currently loaded programming environment. The following table summarises options for obtaining information on the compiler and compile options: Compiler suite C C++ Fortran Cray man clang man clang++ man crayftn GNU man gcc man g++ man gfortran Wrappers man cc man CC man ftn Tip You can also pass the --help option to any of the compilers or wrappers to get a summary of how to use them. The Cray Fortran compiler uses ftn --craype-help to access the help options. Tip There are no man pages for the AOCC compilers at the moment. Tip Cray C/C++ is based on Clang and therefore supports similar options to clang/gcc. clang --help will produce a full summary of options with Cray-specific options marked \"Cray\". The clang man page on ARCHER2 concentrates on these Cray extensions to the clang front end and does not provide an exhaustive description of all clang options. Cray Fortran is not based on Flang and so takes different options from flang/gfortran. Which compiler environment? If you are unsure which compiler you should choose, we suggest the starting point should be the GNU compiler collection (GCC, PrgEnv-gnu ); this is perhaps the most commonly used by code developers, particularly in the open source software domain. A portable, standard-conforming code should (in principle) compile in any of the three compiler environments. For users requiring specific compiler features, such as co-array Fortran, the recommended starting point would be Cray. The following sections provide further details of the different compiler environments. Warning Intel compilers are not currently available on ARCHER2. GNU compiler collection (GCC) The commonly used open source GNU compiler collection is available and provides C/C++ and Fortran compilers. Switch the the GCC compiler environment from the default CCE (cray) compiler environment via: auser@ln03:~> module swap PrgEnv-cray PrgEnv-gnu Due to MODULEPATH changes, the following have been reloaded: 1) cray-mpich/8.1.4 Bug The gcc/8.1.0 module is available on ARCHER2 but cannot be used as the supporting scientific and system libraries are not available. You should not use this version of GCC. Warning If you want to use GCC version 10 or greater to compile Fortran code, with the old MPI interfaces (i.e. use mpi or INCLUDE 'mpif.h' ) you must add the -fallow-argument-mismatch option (or equivalent) when compiling otherwise you will see compile errors associated with MPI functions. The reason for this is that past versions of gfortran have allowed mismatched arguments to external procedures (e.g., where an explicit interface is not available). This is often the case for MPI routines using the old MPI interfaces where arrays of different types are passed to, for example, MPI_Send() . This will now generate an error as not standard conforming. The -fallow-argument-mismatch option is used to reduce the error to a warning. The same effect may be achieved via -std=legacy . If you use the Fortran 2008 MPI interface (i.e. use mpi_f08 ) then you should not need to add this option. Fortran language MPI bindings are described in more detail at in the MPI Standard documentation . Useful Gnu Fortran options Option Comment -std=<standard> Default is gnu -fallow-argument-mismatch Allow mismatched procedure arguments. This argument is required for compiling MPI Fortran code with GCC version 10 or greater if you are using the older MPI interfaces (see warning above) -fbounds-check Use runtime checking of array indices -fopenmp Compile OpenMP (default is no OpenMP) -v Display verbose output from compiler stages Tip The standard in -std may be one of f95 f2003 , f2008 or f2018 . The default option -std=gnu is the latest Fortran standard plus gnu extensions. Warning Past versions of gfortran have allowed mismatched arguments to external procedures (e.g., where an explicit interface is not available). This is often the case for MPI routines where arrays of different types are passed to MPI_Send() and so on. This will now generate an error as not standard conforming. Use -fallow-argument-mismatch to reduce the error to a warning. The same effect may be achieved via -std=legacy . Reference material C/C++ documentation https://gcc.gnu.org/onlinedocs/gcc-9.3.0/gcc/ Fortran documentation https://gcc.gnu.org/onlinedocs/gcc-9.3.0/gfortran/ Cray Compiling Environment (CCE) The Cray Compiling Environment (CCE) is the default compiler at the point of login. CCE supports C/C++ (along with unified parallel C UPC), and Fortran (including co-array Fortran). Support for OpenMP parallelism is available for both C/C++ and Fortran (currently OpenMP 4.5, with a number of exceptions). The Cray C/C++ compiler is based on a clang front end, and so compiler options are similar to those for gcc/clang. However, the Fortran compiler remains based around Cray-specific options. Be sure to separate C/C++ compiler options and Fortran compiler options (typically CFLAGS and FFLAGS ) if compiling mixed C/Fortran applications. As CCE is the default compiler environment on ARCHER2, you do not usually need to issue any commands to enable CCE. Note The CCE Clang compiler uses a GCC 8 toolchain so only C++ standard library features available in GCC 8 will be available in CCE Clang. You can add the compile option --gcc-toolchain=/opt/gcc/10.2.0/snos to use a more recent version of the C++ standard library if you wish. Useful CCE C/C++ options When using the compiler wrappers cc or CC , some of the following options may be useful: Language, warning, Debugging options: Option Comment -std=<standard> Default is -std=gnu11 ( gnu++14 for C++) [1] --gcc-toolchain=/opt/gcc/10.2.0/snos Use the GCC 10.2.0 toolchain instead of the default 8.1.0 version packaged with CCE Performance options: Option Comment -Ofast Optimisation levels: -O0, -O1, -O2, -O3, -Ofast -ffp=level Floating point maths optimisations levels 0-4 [2] -flto Link time optimisation Miscellaneous options: Option Comment -fopenmp Compile OpenMP (default is off) -v Display verbose output from compiler stages Notes Option -std=gnu11 gives c11 plus GNU extensions (likewise c++14 plus GNU extensions). See https://gcc.gnu.org/onlinedocs/gcc-4.8.2/gcc/C-Extensions.html Option -ffp=3 is implied by -Ofast or -ffast-math Useful CCE Fortran options Language, Warning, Debugging options: Option Comment -m <level> Message level (default -m 3 errors and warnings) Performance options: Option Comment -O <level> Optimisation levels: -O0 to -O3 (default -O2) -h fp<level> Floating point maths optimisations levels 0-3 -h ipa Inter-procedural analysis Miscellaneous options: Option Comment -h omp Compile OpenMP (default is -hnoomp ) -v Display verbose output from compiler stages CCE Reference Documentation Clang/Clang++ documentation , CCE-specific details are available via man clang once the CCE compiler environment is loaded. Cray Fortran documentation AMD Optimizing Compiler Collection (AOCC) The AMD Optimizing Compiler Collection (AOCC) is a clang-based optimising compiler. AOCC also includes a flang-based Fortran compiler. Switch the the AOCC compiler environment from the default CCE (cray) compiler environment via: auser@ln03:~> module swap PrgEnv-cray PrgEnv-aocc Due to MODULEPATH changes, the following have been reloaded: 1) cray-mpich/8.1.4 Note Further details on AOCC will appear here as they become available. AOCC reference material AMD website https://developer.amd.com/amd-aocc/ Message passing interface (MPI) HPE Cray MPICH HPE Cray provide, as standard, an MPICH implementation of the message passing interface which is specifically optimised for the ARCHER2 interconnect. The current implementation supports MPI standard version 3.1. The HPE Cray MPICH implementation is linked into software by default when compiling using the standard wrapper scripts: cc , CC and ftn . You do not need to do anything to make HPE Cray MPICH available when you log into ARCHER2, it is available by default to all users. Switching to alternative UCX MPI implementation HPE Cray MPICH can use two different low-level protocols to transfer data across the network. The default is the Open Fabrics Interface (OFI), but you can switch to the UCX protocol from Mellanox. Which performs better will be application-dependent, but our experience is that UCX is often faster for programs that send a lot of data collectively between many processes, e.g. all-to-all communications patterns such as occur in parallel FFTs. Note You do not need to recompile your program - you simply load different modules in your Slurm script. module swap craype-network-ofi craype-network-ucx module swap cray-mpich cray-mpich-ucx The performance benefits will also vary depending on the number of processes, so it is important to benchmark your application at the scale used in full production runs. MPI reference material MPI standard documents: https://www.mpi-forum.org/docs/ Linking and libraries Linking to libraries is performed dynamically on ARCHER2. Important Static linking is not supported on ARCHER2. If you attempt to link statically, you will see errors similar to: /usr/bin/ld: cannot find -lpmi /usr/bin/ld: cannot find -lpmi2 collect2: error: ld returned 1 exit status One can use the -craype-verbose flag to the compiler wrapper to check exactly what linker arguments are invoked. The compiler wrapper scripts encode the paths to the programming environment system libraries using RUNPATH. This ensures that the executable can find the correct runtime libraries without the matching software modules loaded. Tip The RUNPATH setting in the executable only works for default versions of libraries. If you want to use non-default versions then you need to add some additional commands at compile time and in your job submission scripts. See the Using non-default versions of HPE Cray libraries on ARCHER2 . The library RUNPATH associated with an executable can be inspected via, e.g., $ readelf -d ./a.out (swap a.out for the name of the executable you are querying). Commonly used libraries Modules with names prefixed by cray- are provided by HPE Cray, and work with any of the compiler environments and. These modules should be the first choice for access to software libraries if available. Tip More information on the different software libraries on ARCHER2 can be found in the Software libraries section of the user guide. Switching to a different HPE Cray Programming Environment release Important See the section below on using non-default versions of HPE Cray libraries as this process will generally need to be followed when using software from non-default PE installs. Access to non-default PE environments is controlled by the use of the cpe modules. Loading a cpe module will do the following: The compiler version will be switched to the one from the selected PE All HPE Cray PE modules will be updated so their default version is the one from the PE you have selected For example, if you have a code that uses the Gnu compiler environment, FFTW and NetCDF parallel libraries and you want to compile in the (non-default) 21.09 programming environment, you would do the following: First, load the cpe/21.09 module to switch all the defaults to the versions from the 21.09 PE. Then, swap to the Gnu compiler environment and load the required library modules (FFTW, hdf5-parallel and NetCDF HDF5 parallel). The loaded module list shows they are the versions from the 21.09 PE: auser@uan02:/work/t01/t01/auser> module load cpe/21.09 The following have been reloaded with a version change: 1) PrgEnv-cray/8.0.0 => PrgEnv-cray/8.1.0 2) cce/11.0.4 => cce/12.0.3 3) cray-libsci/21.04.1.1 => cray-libsci/21.08.1.2 4) cray-mpich/8.1.4 => cray-mpich/8.1.9 5) craype/2.7.6 => craype/2.7.10 auser@uan02:/work/t01/t01/auser> module swap PrgEnv-cray PrgEnv-gnu Due to MODULEPATH changes, the following have been reloaded: 1) cray-mpich/8.1.9 auser@uan02:/work/t01/t01/auser> module load cray-fftw auser@uan02:/work/t01/t01/auser> module load cray-hdf5-parallel auser@uan02:/work/t01/t01/auser> module load cray-netcdf-hdf5parallel auser@uan02:/work/t01/t01/auser> module list Currently Loaded Modules: 1) cpe/21.09 5) libfabric/1.11.0.4.71 9) cray-libsci/21.08.1.2 13) PrgEnv-gnu/8.1.0 2) gcc/11.2.0 6) craype-network-ofi 10) bolt/0.7 14) cray-fftw/3.3.8.11 3) craype/2.7.10 7) cray-dsmml/0.2.1 11) epcc-setup-env 15) cray-hdf5-parallel/1.12.0.7 4) craype-x86-rome 8) cray-mpich/8.1.9 12) load-epcc-module 16) cray-netcdf-hdf5parallel/4.7.4.7 Finally, you will need to modify the value of LD_LIBRARY_PATH before you compile your software to ensure it picks up the non-default versions of libraries: auser@uan02:/work/t01/t01/auser> export LD_LIBRARY_PATH=$CRAY_LD_LIBRARY_PATH:$LD_LIBRARY_PATH Now you can go ahead and compile your software with the new programming environment. Important The cpe modules only change the versions of software modules provided as part of the HPE Cray programming environments. Any modules provided by the ARCHER2 service will need to be loaded manually after you have completed the process described above. Note Unloading the cpe module does not restore the original programming environment release. To restore the default programming environment release you should log out and then log back in to ARCHER2. Accessing performance analysis tools in non-default Programming Environment The performance analysis tools (such as CrayPAT and CrayPAT-lite) behave slightly differently to other HPE Cray modules when you change to a non-default version of the programming environment. Specifically, an additional step is required to make them available. Once you have loaded the cpe module, you will also need to load the perftools-base module to be able to load and use the performance tools modules. Available HPE Cray Programming Environment releases on ARCHER2 ARCHER2 currently has the following HPE Cray Programming Environment releases available: 21.04: Current default 21.09: available via cpe/21.09 module You can find information, notes, and lists of changes for current and upcoming ARCHER2 HPE Cray programming environments in the HPE Cray Programming Environment GitHub repository . Using non-default versions of HPE Cray libraries on ARCHER2 If you wish to make use of non-default versions of libraries provided by HPE Cray (usually because they are part of a non-default PE release: either old or new) then you need to make changes at both compile and runtime. In summary, you need to load the correct module and also make changes to the LD_LIBRARY_PATH environment variable. At compile time you need to load the version of the library module before you compile and set the LD_LIBRARY_PATH environment variable to include the contencts of $CRAY_LD_LIBRARY_PATH as the first entry. For example, to use the, non-default, 21.09.1.2 version of HPE Cray LibSci in the default programming environment (Cray Compiler Environment, CCE) you would first setup the environment to compile with: auser@uan01:~/test/libsci> module swap cray-libsci cray-libsci/21.08.1.2 auser@uan01:~/test/libsci> export LD_LIBRARY_PATH=$CRAY_LD_LIBRARY_PATH:$LD_LIBRARY_PATH The order is important here: every time you change a module, you will need to reset the value of LD_LIBRARY_PATH for the process to work (it will not be updated automatically). Now you can compile your code. You can check that the executable is using the correct version of LibSci with the ldd command and look for the line beginning libsci_cray.so.5 , you should see the version in the path to the library file: auser@uan01:~/test/libsci> ldd dgemv.x linux-vdso.so.1 (0x00007fffd33dd000) libm.so.6 => /lib64/libm.so.6 (0x00007fbd6e7ed000) libsci_cray.so.5 => /opt/cray/pe/libsci/21.08.1.2/CRAY/9.0/x86_64/lib/libsci_cray.so.5 (0x00007fbd6a8a7000) libdl.so.2 => /lib64/libdl.so.2 (0x00007fbd6a6a3000) libxpmem.so.0 => /opt/cray/xpmem/default/lib64/libxpmem.so.0 (0x00007fbd6a4a0000) libquadmath.so.0 => /opt/cray/pe/cce/11.0.4/cce/x86_64/lib/libquadmath.so.0 (0x00007fbd6a260000) libmodules.so.1 => /opt/cray/pe/cce/11.0.4/cce/x86_64/lib/libmodules.so.1 (0x00007fbd6a044000) libfi.so.1 => /opt/cray/pe/cce/11.0.4/cce/x86_64/lib/libfi.so.1 (0x00007fbd69921000) libcraymath.so.1 => /opt/cray/pe/cce/11.0.4/cce/x86_64/lib/libcraymath.so.1 (0x00007fbd69640000) libf.so.1 => /opt/cray/pe/cce/11.0.4/cce/x86_64/lib/libf.so.1 (0x00007fbd693ac000) libu.so.1 => /opt/cray/pe/cce/11.0.4/cce/x86_64/lib/libu.so.1 (0x00007fbd69098000) libcsup.so.1 => /opt/cray/pe/cce/11.0.4/cce/x86_64/lib/libcsup.so.1 (0x00007fbd68e92000) libc.so.6 => /lib64/libc.so.6 (0x00007fbd68ad7000) /lib64/ld-linux-x86-64.so.2 (0x00007fbd6eb25000) libpthread.so.0 => /lib64/libpthread.so.0 (0x00007fbd688b8000) librt.so.1 => /lib64/librt.so.1 (0x00007fbd686b0000) libgfortran.so.5 => /opt/cray/pe/gcc-libs/libgfortran.so.5 (0x00007fbd681f9000) libstdc++.so.6 => /opt/cray/pe/gcc-libs/libstdc++.so.6 (0x00007fbd67e26000) libgcc_s.so.1 => /opt/cray/pe/gcc-libs/libgcc_s.so.1 (0x00007fbd67c0e000 Tip If any of the libraries point to versions in the /opt/cray/pe/lib64 directory then these are using the default versions of the libraries rather than the specific versions. This happens at compile time if you have forgotton to load the right module and set $LD_LIBRARY_PATH afterwards. At run time (typically in your job script) you need to repeat the environment setup steps (you can also use the ldd command in your job submission script to check the library is pointing to the correct version). For example, a job submission script to run our dgemv.x executable with the non-default version of LibSci could look like: #!/bin/bash #SBATCH --job-name=dgemv #SBATCH --time=0:20:0 #SBATCH --nodes=1 #SBATCH --tasks-per-node=1 #SBATCH --cpus-per-task=1 # Replace the account code, partition and QoS with those you wish to use #SBATCH --account=t01 #SBATCH --partition=standard #SBATCH --qos=short #SBATCH --reservation=shortqos # Setup up the environment to use the non-default version of LibSci # We use \"module swap\" as the \"cray-libsci\" is loaded by default. # This must be done after loading the \"epcc-job-env\" module module swap cray-libsci cray-libsci/20.08.1.2 export LD_LIBRARY_PATH=$CRAY_LD_LIBRARY_PATH:$LD_LIBRARY_PATH # Check which library versions the executable is pointing too ldd dgemv.x export OMP_NUM_THREADS=1 srun --hint=nomultithread --distribution=block:block dgemv.x Tip As when compiling, the order of commands matters. Setting the value of LD_LIBRARY_PATH must happen after you have finished all your module commands for it to have the correct effect. Important You must setup the environment at both compile and run time otherwise you will end up using the default version of the library. Compiling on compute nodes Sometimes you may wish to compile in a batch job. For example, the compile process may take a long time or the compile process is part of the research workflow and can be coupled to the production job. Unlike login nodes, the /home file system is not available. An example job submission script for a compile job using make (assuming the Makefile is in the same directory as the job submission script) would be: #!/bin/bash #SBATCH --job-name=compile #SBATCH --time=00:20:00 #SBATCH --nodes=1 #SBATCH --tasks-per-node=1 #SBATCH --cpus-per-task=1 # Replace the account code, partition and QoS with those you wish to use #SBATCH --account=t01 #SBATCH --partition=standard #SBATCH --qos=standard make clean make Note If you want to use a compiler environment other than the default then you will need to add the module swap command before the make command. e.g. to use the Gnu compiler environemnt: module swap PrgEnv-cray PrgEnv-gnu You can also use a compute node in an interactive way using salloc . Please see Section Using salloc to reserve resources for further details. Once your interactive session is ready, you can load the compilation environment and compile the code. Managing development ARCHER2 supports common revision control software such as git . Standard GNU autoconf tools are available, along with make (which is GNU Make). Versions of cmake are available. Tip Some of these tools are part of the system software, and typically reside in /usr/bin , while others are provided as part of the module system. Some tools may be available in different versions via both /usr/bin and via the module system. If you find the default version is too old, then look in the module system for a more recent version. Build instructions for software on ARCHER2 The ARCHER2 CSE team at EPCC and other contributors provide build configurations ando instructions for a range of research software, software libraries and tools on a variety of HPC systems (including ARCHER2) in a public Github repository. See: Build instructions repository The repository always welcomes contributions from the ARCHER2 user community. Support for building software on ARCHER2 If you run into issues building software on ARCHER2 or the software you require is not available then please contact the ARCHER2 Service Desk with any questions you have.","title":"Application development environment"},{"location":"user-guide/dev-environment/#application-development-environment","text":"","title":"Application development environment"},{"location":"user-guide/dev-environment/#whats-available","text":"ARCHER2 runs the HPE Cray Linux Environment (a version of SUSE Linux), and provides a development environment which includes: Software modules via a standard module framework Three different compiler environments (AMD, Cray, and GNU) MPI, OpenMP, and SHMEM Scientific and numerical libraries Parallel Python and R Parallel debugging and profiling Singularity containers Access to particular software, and particular versions, is managed by an Lmod module framework. Most software is available by loading modules, including the different compiler environments You can see what compiler environments are available with: auser@uan01:~> module avail PrgEnv ------------------------------------------ /opt/cray/pe/lmod/modulefiles/core ------------------------------------------ PrgEnv-aocc/8.0.0 (D) PrgEnv-cray/8.0.0 (L,D) PrgEnv-gnu/8.0.0 (D) PrgEnv-aocc/8.1.0 PrgEnv-cray/8.1.0 PrgEnv-gnu/8.1.0 Where: L: Module is loaded D: Default Module Use \"module spider\" to find all possible modules and extensions. Use \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\". Other software modules can be searched using the module spider command: auser@uan01:~> module spider -------------------------------------------------------------------------------------------------------------------- The following is a list of the modules and extensions currently available: -------------------------------------------------------------------------------------------------------------------- PrgEnv-aocc: PrgEnv-aocc/8.0.0, PrgEnv-aocc/8.1.0 PrgEnv-cray: PrgEnv-cray/8.0.0, PrgEnv-cray/8.1.0 PrgEnv-gnu: PrgEnv-gnu/8.0.0, PrgEnv-gnu/8.1.0 aocc: aocc/2.2.0.1, aocc/3.0.0 atp: atp/3.13.1, atp/3.14.5 cce: cce/11.0.4, cce/12.0.3 cpe: cpe/21.04, cpe/21.09 ...output trimmed... perftools-lite-hbm: perftools-lite-hbm perftools-lite-loops: perftools-lite-loops perftools-preload: perftools-preload settarg: settarg valgrind4hpc: valgrind4hpc/2.11.1, valgrind4hpc/2.12.4 xpmem: xpmem/2.2.40-7.0.1.0_2.7__g1d7a24d.shasta -------------------------------------------------------------------------------------------------------------------- To learn more about a package execute: $ module spider Foo where \"Foo\" is the name of a module. To find detailed information about a particular package you must specify the version if there is more than one version: $ module spider Foo/11.1 -------------------------------------------------------------------------------------------------------------------- A full discussion of the module system is available in the Software environment section . A consistent set of modules is loaded on login to the machine (currently PrgEnv-cray , see below). Developing applications then means selecting and loading the appropriate set of modules before starting work. This section is aimed at code developers and will concentrate on the compilation environment, building libraries and executables, specifically parallel executables. Other topics such as Python and Containers are covered in more detail in separate sections of the documentation. Tip If you want to get back to the login module state without having to logout and back in again, you can just use: module restore This is also handy for build scripts to ensure you are starting from a known state.","title":"What's available"},{"location":"user-guide/dev-environment/#compiler-environments","text":"There are three different compiler environments available on ARCHER2: AMD Compiler Collection (AOCC) GNU Compiler Collection (GCC) HPE Cray Compiler Collection (CCE) (current default compiler environment) The current compiler suite is selected via the PrgEnv module , while the specific compiler versions are determined by the relevant compiler module. A summary is: Suite name Compiler Environment Module Compiler Version Module CCE PrgEnv-cray cce GCC PrgEnv-gnu gcc AOCC PrgEnv-aocc aocc For example, at login, the default set of modules are: auser@ln03:~> module list Currently Loaded Modules: 1) cce/11.0.4 6) perftools-base/21.02.0 2) craype/2.7.6 7) xpmem/2.2.40-7.0.1.0_2.7__g1d7a24d.shasta 3) craype-x86-rome 8) cray-mpich/8.1.4 4) libfabric/1.11.0.4.71 9) cray-libsci/21.04.1.1 5) craype-network-ofi 10) PrgEnv-cray/8.0.0 from which we see the default compiler environment is Cray (indicated by PrgEnv-cray (at 10 in the list above) and the default compiler module is cce/11.0.4 (at 1 in the list above). The compiler environment will give access to a consistent set of compiler, MPI library via cray-mpich (at 8), and other libraries e.g., cray-libsci (at 9 in the list above) infrastructure.","title":"Compiler environments"},{"location":"user-guide/dev-environment/#switching-between-compiler-environments","text":"Switching between different compiler environments is achieved using the module swap command. For example, to switch from the default HPE Cray (CCE) compiler environment to the GCC environment, you would use: auser@ln03:~> module swap PrgEnv-cray PrgEnv-gnu Due to MODULEPATH changes, the following have been reloaded: 1) cray-mpich/8.1.4 If you then use the module list command, you will see that your environment has been changed to the GCC environment: auser@ln03:~> module list Currently Loaded Modules: 1) gcc/10.2.0 6) perftools-base/21.02.0 2) craype/2.7.6 7) xpmem/2.2.40-7.0.1.0_2.7__g1d7a24d.shasta 3) craype-x86-rome 8) cray-mpich/8.1.4 4) libfabric/1.11.0.4.71 9) cray-libsci/21.04.1.1 5) craype-network-ofi 10) PrgEnv-gnu/8.0.0","title":"Switching between compiler environments"},{"location":"user-guide/dev-environment/#switching-between-compiler-versions","text":"Within a given compiler environment, it is possible to swap to a different compiler version by swapping the relevant compiler module. To switch to the GNU compiler environment from the default HPE Cray compiler environment and than swap the version of GCC from the 11.2.0 default to the older 10.2.0 version, you would use auser@ln03:~> module swap PrgEnv-cray PrgEnv-gnu Due to MODULEPATH changes, the following have been reloaded: 1) cray-mpich/8.1.4 auser@ln03:~> module swap gcc gcc/11.2.0 The following have been reloaded with a version change: 1) gcc/10.2.0 => gcc/11.2.0 The first swap command moves to the GNU compiler environment and the second swap command moves to the older version of GCC. As before, module list will show that your environment has been changed: auser@ln03:~> module list Currently Loaded Modules: 1) craype/2.7.6 5) perftools-base/21.02.0 9) gcc/11.2.0 2) craype-x86-rome 6) xpmem/2.2.40-7.0.1.0_2.7__g1d7a24d.shasta 10) cray-mpich/8.1.4 3) libfabric/1.11.0.4.71 7) cray-libsci/21.04.1.1 4) craype-network-ofi 8) PrgEnv-gnu/8.0.0","title":"Switching between compiler versions"},{"location":"user-guide/dev-environment/#compiler-wrapper-scripts-cc-cc-ftn","text":"To ensure consistent behaviour, compilation of C, C++, and Fortran source code should then take place using the appropriate compiler wrapper: cc , CC , and ftn , respectively. The wrapper will automatically call the relevant underlying compiler and add the appropriate include directories and library locations to the invocation. This typically eliminates the need to specify this additional information explicitly in the configuration stage. To see the details of the exact compiler invocation use the -craype-verbose flag to the compiler wrapper. The default link time behaviour is also related to the current programming environment. See the section below on Linking and libraries . Users should not, in general, invoke specific compilers at compile/link stages. In particular, gcc , which may default to /usr/bin/gcc , should not be used. The compiler wrappers cc , CC , and ftn should be used (with the underlying compiler type and version set by the module system). Other common MPI compiler wrappers e.g., mpicc , should also be replaced by the relevant wrapper, e.g. cc (commands such as mpicc are not available on ARCHER2). Important Always use the compiler wrappers cc , CC , and/or ftn and not a specific compiler invocation. This will ensure consistent compile/link time behaviour. Tip If you are using a build system such as Make or CMake then you will need to replace all occurrences of mpicc with cc , mpicxx / mpic++ with CC and mpif90 with ftn .","title":"Compiler wrapper scripts: cc, CC, ftn"},{"location":"user-guide/dev-environment/#compiler-man-pages-and-help","text":"Further information on both the compiler wrappers, and the individual compilers themselves are available via the command line, and via standard man pages. The man page for the compiler wrappers is common to all programming environments, while the man page for individual compilers depends on the currently loaded programming environment. The following table summarises options for obtaining information on the compiler and compile options: Compiler suite C C++ Fortran Cray man clang man clang++ man crayftn GNU man gcc man g++ man gfortran Wrappers man cc man CC man ftn Tip You can also pass the --help option to any of the compilers or wrappers to get a summary of how to use them. The Cray Fortran compiler uses ftn --craype-help to access the help options. Tip There are no man pages for the AOCC compilers at the moment. Tip Cray C/C++ is based on Clang and therefore supports similar options to clang/gcc. clang --help will produce a full summary of options with Cray-specific options marked \"Cray\". The clang man page on ARCHER2 concentrates on these Cray extensions to the clang front end and does not provide an exhaustive description of all clang options. Cray Fortran is not based on Flang and so takes different options from flang/gfortran.","title":"Compiler man pages and help"},{"location":"user-guide/dev-environment/#which-compiler-environment","text":"If you are unsure which compiler you should choose, we suggest the starting point should be the GNU compiler collection (GCC, PrgEnv-gnu ); this is perhaps the most commonly used by code developers, particularly in the open source software domain. A portable, standard-conforming code should (in principle) compile in any of the three compiler environments. For users requiring specific compiler features, such as co-array Fortran, the recommended starting point would be Cray. The following sections provide further details of the different compiler environments. Warning Intel compilers are not currently available on ARCHER2.","title":"Which compiler environment?"},{"location":"user-guide/dev-environment/#gnu-compiler-collection-gcc","text":"The commonly used open source GNU compiler collection is available and provides C/C++ and Fortran compilers. Switch the the GCC compiler environment from the default CCE (cray) compiler environment via: auser@ln03:~> module swap PrgEnv-cray PrgEnv-gnu Due to MODULEPATH changes, the following have been reloaded: 1) cray-mpich/8.1.4 Bug The gcc/8.1.0 module is available on ARCHER2 but cannot be used as the supporting scientific and system libraries are not available. You should not use this version of GCC. Warning If you want to use GCC version 10 or greater to compile Fortran code, with the old MPI interfaces (i.e. use mpi or INCLUDE 'mpif.h' ) you must add the -fallow-argument-mismatch option (or equivalent) when compiling otherwise you will see compile errors associated with MPI functions. The reason for this is that past versions of gfortran have allowed mismatched arguments to external procedures (e.g., where an explicit interface is not available). This is often the case for MPI routines using the old MPI interfaces where arrays of different types are passed to, for example, MPI_Send() . This will now generate an error as not standard conforming. The -fallow-argument-mismatch option is used to reduce the error to a warning. The same effect may be achieved via -std=legacy . If you use the Fortran 2008 MPI interface (i.e. use mpi_f08 ) then you should not need to add this option. Fortran language MPI bindings are described in more detail at in the MPI Standard documentation .","title":"GNU compiler collection (GCC)"},{"location":"user-guide/dev-environment/#useful-gnu-fortran-options","text":"Option Comment -std=<standard> Default is gnu -fallow-argument-mismatch Allow mismatched procedure arguments. This argument is required for compiling MPI Fortran code with GCC version 10 or greater if you are using the older MPI interfaces (see warning above) -fbounds-check Use runtime checking of array indices -fopenmp Compile OpenMP (default is no OpenMP) -v Display verbose output from compiler stages Tip The standard in -std may be one of f95 f2003 , f2008 or f2018 . The default option -std=gnu is the latest Fortran standard plus gnu extensions. Warning Past versions of gfortran have allowed mismatched arguments to external procedures (e.g., where an explicit interface is not available). This is often the case for MPI routines where arrays of different types are passed to MPI_Send() and so on. This will now generate an error as not standard conforming. Use -fallow-argument-mismatch to reduce the error to a warning. The same effect may be achieved via -std=legacy .","title":"Useful Gnu Fortran options"},{"location":"user-guide/dev-environment/#reference-material","text":"C/C++ documentation https://gcc.gnu.org/onlinedocs/gcc-9.3.0/gcc/ Fortran documentation https://gcc.gnu.org/onlinedocs/gcc-9.3.0/gfortran/","title":"Reference material"},{"location":"user-guide/dev-environment/#cray-compiling-environment-cce","text":"The Cray Compiling Environment (CCE) is the default compiler at the point of login. CCE supports C/C++ (along with unified parallel C UPC), and Fortran (including co-array Fortran). Support for OpenMP parallelism is available for both C/C++ and Fortran (currently OpenMP 4.5, with a number of exceptions). The Cray C/C++ compiler is based on a clang front end, and so compiler options are similar to those for gcc/clang. However, the Fortran compiler remains based around Cray-specific options. Be sure to separate C/C++ compiler options and Fortran compiler options (typically CFLAGS and FFLAGS ) if compiling mixed C/Fortran applications. As CCE is the default compiler environment on ARCHER2, you do not usually need to issue any commands to enable CCE. Note The CCE Clang compiler uses a GCC 8 toolchain so only C++ standard library features available in GCC 8 will be available in CCE Clang. You can add the compile option --gcc-toolchain=/opt/gcc/10.2.0/snos to use a more recent version of the C++ standard library if you wish.","title":"Cray Compiling Environment (CCE)"},{"location":"user-guide/dev-environment/#useful-cce-cc-options","text":"When using the compiler wrappers cc or CC , some of the following options may be useful: Language, warning, Debugging options: Option Comment -std=<standard> Default is -std=gnu11 ( gnu++14 for C++) [1] --gcc-toolchain=/opt/gcc/10.2.0/snos Use the GCC 10.2.0 toolchain instead of the default 8.1.0 version packaged with CCE Performance options: Option Comment -Ofast Optimisation levels: -O0, -O1, -O2, -O3, -Ofast -ffp=level Floating point maths optimisations levels 0-4 [2] -flto Link time optimisation Miscellaneous options: Option Comment -fopenmp Compile OpenMP (default is off) -v Display verbose output from compiler stages Notes Option -std=gnu11 gives c11 plus GNU extensions (likewise c++14 plus GNU extensions). See https://gcc.gnu.org/onlinedocs/gcc-4.8.2/gcc/C-Extensions.html Option -ffp=3 is implied by -Ofast or -ffast-math","title":"Useful CCE C/C++ options"},{"location":"user-guide/dev-environment/#useful-cce-fortran-options","text":"Language, Warning, Debugging options: Option Comment -m <level> Message level (default -m 3 errors and warnings) Performance options: Option Comment -O <level> Optimisation levels: -O0 to -O3 (default -O2) -h fp<level> Floating point maths optimisations levels 0-3 -h ipa Inter-procedural analysis Miscellaneous options: Option Comment -h omp Compile OpenMP (default is -hnoomp ) -v Display verbose output from compiler stages","title":"Useful CCE Fortran options"},{"location":"user-guide/dev-environment/#cce-reference-documentation","text":"Clang/Clang++ documentation , CCE-specific details are available via man clang once the CCE compiler environment is loaded. Cray Fortran documentation","title":"CCE Reference Documentation"},{"location":"user-guide/dev-environment/#amd-optimizing-compiler-collection-aocc","text":"The AMD Optimizing Compiler Collection (AOCC) is a clang-based optimising compiler. AOCC also includes a flang-based Fortran compiler. Switch the the AOCC compiler environment from the default CCE (cray) compiler environment via: auser@ln03:~> module swap PrgEnv-cray PrgEnv-aocc Due to MODULEPATH changes, the following have been reloaded: 1) cray-mpich/8.1.4 Note Further details on AOCC will appear here as they become available.","title":"AMD Optimizing Compiler Collection (AOCC)"},{"location":"user-guide/dev-environment/#aocc-reference-material","text":"AMD website https://developer.amd.com/amd-aocc/","title":"AOCC reference material"},{"location":"user-guide/dev-environment/#message-passing-interface-mpi","text":"","title":"Message passing interface (MPI)"},{"location":"user-guide/dev-environment/#hpe-cray-mpich","text":"HPE Cray provide, as standard, an MPICH implementation of the message passing interface which is specifically optimised for the ARCHER2 interconnect. The current implementation supports MPI standard version 3.1. The HPE Cray MPICH implementation is linked into software by default when compiling using the standard wrapper scripts: cc , CC and ftn . You do not need to do anything to make HPE Cray MPICH available when you log into ARCHER2, it is available by default to all users.","title":"HPE Cray MPICH"},{"location":"user-guide/dev-environment/#switching-to-alternative-ucx-mpi-implementation","text":"HPE Cray MPICH can use two different low-level protocols to transfer data across the network. The default is the Open Fabrics Interface (OFI), but you can switch to the UCX protocol from Mellanox. Which performs better will be application-dependent, but our experience is that UCX is often faster for programs that send a lot of data collectively between many processes, e.g. all-to-all communications patterns such as occur in parallel FFTs. Note You do not need to recompile your program - you simply load different modules in your Slurm script. module swap craype-network-ofi craype-network-ucx module swap cray-mpich cray-mpich-ucx The performance benefits will also vary depending on the number of processes, so it is important to benchmark your application at the scale used in full production runs.","title":"Switching to alternative UCX MPI implementation"},{"location":"user-guide/dev-environment/#mpi-reference-material","text":"MPI standard documents: https://www.mpi-forum.org/docs/","title":"MPI reference material"},{"location":"user-guide/dev-environment/#linking-and-libraries","text":"Linking to libraries is performed dynamically on ARCHER2. Important Static linking is not supported on ARCHER2. If you attempt to link statically, you will see errors similar to: /usr/bin/ld: cannot find -lpmi /usr/bin/ld: cannot find -lpmi2 collect2: error: ld returned 1 exit status One can use the -craype-verbose flag to the compiler wrapper to check exactly what linker arguments are invoked. The compiler wrapper scripts encode the paths to the programming environment system libraries using RUNPATH. This ensures that the executable can find the correct runtime libraries without the matching software modules loaded. Tip The RUNPATH setting in the executable only works for default versions of libraries. If you want to use non-default versions then you need to add some additional commands at compile time and in your job submission scripts. See the Using non-default versions of HPE Cray libraries on ARCHER2 . The library RUNPATH associated with an executable can be inspected via, e.g., $ readelf -d ./a.out (swap a.out for the name of the executable you are querying).","title":"Linking and libraries"},{"location":"user-guide/dev-environment/#commonly-used-libraries","text":"Modules with names prefixed by cray- are provided by HPE Cray, and work with any of the compiler environments and. These modules should be the first choice for access to software libraries if available. Tip More information on the different software libraries on ARCHER2 can be found in the Software libraries section of the user guide.","title":"Commonly used libraries"},{"location":"user-guide/dev-environment/#switching-to-a-different-hpe-cray-programming-environment-release","text":"Important See the section below on using non-default versions of HPE Cray libraries as this process will generally need to be followed when using software from non-default PE installs. Access to non-default PE environments is controlled by the use of the cpe modules. Loading a cpe module will do the following: The compiler version will be switched to the one from the selected PE All HPE Cray PE modules will be updated so their default version is the one from the PE you have selected For example, if you have a code that uses the Gnu compiler environment, FFTW and NetCDF parallel libraries and you want to compile in the (non-default) 21.09 programming environment, you would do the following: First, load the cpe/21.09 module to switch all the defaults to the versions from the 21.09 PE. Then, swap to the Gnu compiler environment and load the required library modules (FFTW, hdf5-parallel and NetCDF HDF5 parallel). The loaded module list shows they are the versions from the 21.09 PE: auser@uan02:/work/t01/t01/auser> module load cpe/21.09 The following have been reloaded with a version change: 1) PrgEnv-cray/8.0.0 => PrgEnv-cray/8.1.0 2) cce/11.0.4 => cce/12.0.3 3) cray-libsci/21.04.1.1 => cray-libsci/21.08.1.2 4) cray-mpich/8.1.4 => cray-mpich/8.1.9 5) craype/2.7.6 => craype/2.7.10 auser@uan02:/work/t01/t01/auser> module swap PrgEnv-cray PrgEnv-gnu Due to MODULEPATH changes, the following have been reloaded: 1) cray-mpich/8.1.9 auser@uan02:/work/t01/t01/auser> module load cray-fftw auser@uan02:/work/t01/t01/auser> module load cray-hdf5-parallel auser@uan02:/work/t01/t01/auser> module load cray-netcdf-hdf5parallel auser@uan02:/work/t01/t01/auser> module list Currently Loaded Modules: 1) cpe/21.09 5) libfabric/1.11.0.4.71 9) cray-libsci/21.08.1.2 13) PrgEnv-gnu/8.1.0 2) gcc/11.2.0 6) craype-network-ofi 10) bolt/0.7 14) cray-fftw/3.3.8.11 3) craype/2.7.10 7) cray-dsmml/0.2.1 11) epcc-setup-env 15) cray-hdf5-parallel/1.12.0.7 4) craype-x86-rome 8) cray-mpich/8.1.9 12) load-epcc-module 16) cray-netcdf-hdf5parallel/4.7.4.7 Finally, you will need to modify the value of LD_LIBRARY_PATH before you compile your software to ensure it picks up the non-default versions of libraries: auser@uan02:/work/t01/t01/auser> export LD_LIBRARY_PATH=$CRAY_LD_LIBRARY_PATH:$LD_LIBRARY_PATH Now you can go ahead and compile your software with the new programming environment. Important The cpe modules only change the versions of software modules provided as part of the HPE Cray programming environments. Any modules provided by the ARCHER2 service will need to be loaded manually after you have completed the process described above. Note Unloading the cpe module does not restore the original programming environment release. To restore the default programming environment release you should log out and then log back in to ARCHER2.","title":"Switching to a different HPE Cray Programming Environment release"},{"location":"user-guide/dev-environment/#accessing-performance-analysis-tools-in-non-default-programming-environment","text":"The performance analysis tools (such as CrayPAT and CrayPAT-lite) behave slightly differently to other HPE Cray modules when you change to a non-default version of the programming environment. Specifically, an additional step is required to make them available. Once you have loaded the cpe module, you will also need to load the perftools-base module to be able to load and use the performance tools modules.","title":"Accessing performance analysis tools in non-default Programming Environment"},{"location":"user-guide/dev-environment/#available-hpe-cray-programming-environment-releases-on-archer2","text":"ARCHER2 currently has the following HPE Cray Programming Environment releases available: 21.04: Current default 21.09: available via cpe/21.09 module You can find information, notes, and lists of changes for current and upcoming ARCHER2 HPE Cray programming environments in the HPE Cray Programming Environment GitHub repository .","title":"Available HPE Cray Programming Environment releases on ARCHER2"},{"location":"user-guide/dev-environment/#using-non-default-versions-of-hpe-cray-libraries-on-archer2","text":"If you wish to make use of non-default versions of libraries provided by HPE Cray (usually because they are part of a non-default PE release: either old or new) then you need to make changes at both compile and runtime. In summary, you need to load the correct module and also make changes to the LD_LIBRARY_PATH environment variable. At compile time you need to load the version of the library module before you compile and set the LD_LIBRARY_PATH environment variable to include the contencts of $CRAY_LD_LIBRARY_PATH as the first entry. For example, to use the, non-default, 21.09.1.2 version of HPE Cray LibSci in the default programming environment (Cray Compiler Environment, CCE) you would first setup the environment to compile with: auser@uan01:~/test/libsci> module swap cray-libsci cray-libsci/21.08.1.2 auser@uan01:~/test/libsci> export LD_LIBRARY_PATH=$CRAY_LD_LIBRARY_PATH:$LD_LIBRARY_PATH The order is important here: every time you change a module, you will need to reset the value of LD_LIBRARY_PATH for the process to work (it will not be updated automatically). Now you can compile your code. You can check that the executable is using the correct version of LibSci with the ldd command and look for the line beginning libsci_cray.so.5 , you should see the version in the path to the library file: auser@uan01:~/test/libsci> ldd dgemv.x linux-vdso.so.1 (0x00007fffd33dd000) libm.so.6 => /lib64/libm.so.6 (0x00007fbd6e7ed000) libsci_cray.so.5 => /opt/cray/pe/libsci/21.08.1.2/CRAY/9.0/x86_64/lib/libsci_cray.so.5 (0x00007fbd6a8a7000) libdl.so.2 => /lib64/libdl.so.2 (0x00007fbd6a6a3000) libxpmem.so.0 => /opt/cray/xpmem/default/lib64/libxpmem.so.0 (0x00007fbd6a4a0000) libquadmath.so.0 => /opt/cray/pe/cce/11.0.4/cce/x86_64/lib/libquadmath.so.0 (0x00007fbd6a260000) libmodules.so.1 => /opt/cray/pe/cce/11.0.4/cce/x86_64/lib/libmodules.so.1 (0x00007fbd6a044000) libfi.so.1 => /opt/cray/pe/cce/11.0.4/cce/x86_64/lib/libfi.so.1 (0x00007fbd69921000) libcraymath.so.1 => /opt/cray/pe/cce/11.0.4/cce/x86_64/lib/libcraymath.so.1 (0x00007fbd69640000) libf.so.1 => /opt/cray/pe/cce/11.0.4/cce/x86_64/lib/libf.so.1 (0x00007fbd693ac000) libu.so.1 => /opt/cray/pe/cce/11.0.4/cce/x86_64/lib/libu.so.1 (0x00007fbd69098000) libcsup.so.1 => /opt/cray/pe/cce/11.0.4/cce/x86_64/lib/libcsup.so.1 (0x00007fbd68e92000) libc.so.6 => /lib64/libc.so.6 (0x00007fbd68ad7000) /lib64/ld-linux-x86-64.so.2 (0x00007fbd6eb25000) libpthread.so.0 => /lib64/libpthread.so.0 (0x00007fbd688b8000) librt.so.1 => /lib64/librt.so.1 (0x00007fbd686b0000) libgfortran.so.5 => /opt/cray/pe/gcc-libs/libgfortran.so.5 (0x00007fbd681f9000) libstdc++.so.6 => /opt/cray/pe/gcc-libs/libstdc++.so.6 (0x00007fbd67e26000) libgcc_s.so.1 => /opt/cray/pe/gcc-libs/libgcc_s.so.1 (0x00007fbd67c0e000 Tip If any of the libraries point to versions in the /opt/cray/pe/lib64 directory then these are using the default versions of the libraries rather than the specific versions. This happens at compile time if you have forgotton to load the right module and set $LD_LIBRARY_PATH afterwards. At run time (typically in your job script) you need to repeat the environment setup steps (you can also use the ldd command in your job submission script to check the library is pointing to the correct version). For example, a job submission script to run our dgemv.x executable with the non-default version of LibSci could look like: #!/bin/bash #SBATCH --job-name=dgemv #SBATCH --time=0:20:0 #SBATCH --nodes=1 #SBATCH --tasks-per-node=1 #SBATCH --cpus-per-task=1 # Replace the account code, partition and QoS with those you wish to use #SBATCH --account=t01 #SBATCH --partition=standard #SBATCH --qos=short #SBATCH --reservation=shortqos # Setup up the environment to use the non-default version of LibSci # We use \"module swap\" as the \"cray-libsci\" is loaded by default. # This must be done after loading the \"epcc-job-env\" module module swap cray-libsci cray-libsci/20.08.1.2 export LD_LIBRARY_PATH=$CRAY_LD_LIBRARY_PATH:$LD_LIBRARY_PATH # Check which library versions the executable is pointing too ldd dgemv.x export OMP_NUM_THREADS=1 srun --hint=nomultithread --distribution=block:block dgemv.x Tip As when compiling, the order of commands matters. Setting the value of LD_LIBRARY_PATH must happen after you have finished all your module commands for it to have the correct effect. Important You must setup the environment at both compile and run time otherwise you will end up using the default version of the library.","title":"Using non-default versions of HPE Cray libraries on ARCHER2"},{"location":"user-guide/dev-environment/#compiling-on-compute-nodes","text":"Sometimes you may wish to compile in a batch job. For example, the compile process may take a long time or the compile process is part of the research workflow and can be coupled to the production job. Unlike login nodes, the /home file system is not available. An example job submission script for a compile job using make (assuming the Makefile is in the same directory as the job submission script) would be: #!/bin/bash #SBATCH --job-name=compile #SBATCH --time=00:20:00 #SBATCH --nodes=1 #SBATCH --tasks-per-node=1 #SBATCH --cpus-per-task=1 # Replace the account code, partition and QoS with those you wish to use #SBATCH --account=t01 #SBATCH --partition=standard #SBATCH --qos=standard make clean make Note If you want to use a compiler environment other than the default then you will need to add the module swap command before the make command. e.g. to use the Gnu compiler environemnt: module swap PrgEnv-cray PrgEnv-gnu You can also use a compute node in an interactive way using salloc . Please see Section Using salloc to reserve resources for further details. Once your interactive session is ready, you can load the compilation environment and compile the code.","title":"Compiling on compute nodes"},{"location":"user-guide/dev-environment/#managing-development","text":"ARCHER2 supports common revision control software such as git . Standard GNU autoconf tools are available, along with make (which is GNU Make). Versions of cmake are available. Tip Some of these tools are part of the system software, and typically reside in /usr/bin , while others are provided as part of the module system. Some tools may be available in different versions via both /usr/bin and via the module system. If you find the default version is too old, then look in the module system for a more recent version.","title":"Managing development"},{"location":"user-guide/dev-environment/#build-instructions-for-software-on-archer2","text":"The ARCHER2 CSE team at EPCC and other contributors provide build configurations ando instructions for a range of research software, software libraries and tools on a variety of HPC systems (including ARCHER2) in a public Github repository. See: Build instructions repository The repository always welcomes contributions from the ARCHER2 user community.","title":"Build instructions for software on ARCHER2"},{"location":"user-guide/dev-environment/#support-for-building-software-on-archer2","text":"If you run into issues building software on ARCHER2 or the software you require is not available then please contact the ARCHER2 Service Desk with any questions you have.","title":"Support for building software on ARCHER2"},{"location":"user-guide/energy-monitoring/","text":"Energy monitoring on ARCHER2 Using sacct to get energy usage for individual jobs Energy usage for a particular job may be obtained using the sacct command. For instance sacct -j 00001 --format=JobID,Elapsed,ConsumedEnergy will provide the elapsed time and consumed energy in joules for the job(s) specified with -j . In this case we would get something like JobID Elapsed ConsumedEnergy ------------ ---------- -------------- 00001 00:01:50 59.53K 00001.bat+ 00:01:50 59.16K 00001.ext+ 00:01:50 59.53K 00001.0 00:01:44 58.80K In this case we can see that the job consumed 59.53kJ for a run lasting 1 minute and 50 seconds. To convert to kWh we can multiply the energy in joules by 2.78e-7, in this case resulting in 0.0165kWh. Data for completed jobs is stored for up to 24 hours after the job completes, so usage statistics must be gathered soon after the job finishes. In addition to energy statistics sacct provides a number of other statistics that can be specified to the --format option, the full list of which can be viewed with sacct --helpformat or using the man pages.","title":"Energy monitoring"},{"location":"user-guide/energy-monitoring/#energy-monitoring-on-archer2","text":"","title":"Energy monitoring on ARCHER2"},{"location":"user-guide/energy-monitoring/#using-sacct-to-get-energy-usage-for-individual-jobs","text":"Energy usage for a particular job may be obtained using the sacct command. For instance sacct -j 00001 --format=JobID,Elapsed,ConsumedEnergy will provide the elapsed time and consumed energy in joules for the job(s) specified with -j . In this case we would get something like JobID Elapsed ConsumedEnergy ------------ ---------- -------------- 00001 00:01:50 59.53K 00001.bat+ 00:01:50 59.16K 00001.ext+ 00:01:50 59.53K 00001.0 00:01:44 58.80K In this case we can see that the job consumed 59.53kJ for a run lasting 1 minute and 50 seconds. To convert to kWh we can multiply the energy in joules by 2.78e-7, in this case resulting in 0.0165kWh. Data for completed jobs is stored for up to 24 hours after the job completes, so usage statistics must be gathered soon after the job finishes. In addition to energy statistics sacct provides a number of other statistics that can be specified to the --format option, the full list of which can be viewed with sacct --helpformat or using the man pages.","title":"Using sacct to get energy usage for individual jobs"},{"location":"user-guide/hardware/","text":"ARCHER2 hardware System overview ARCHER2 is a HPE Cray EX supercomputing system which has a total of 5,860 compute nodes. Each compute node has 128 cores (dual AMD EPYC 7742 64-core 2.25GHz processors) giving a total of 750,080 cores. Compute nodes are connected together by a HPE Slingshot interconnect. There are additional User Access Nodes (UAN, also called login nodes), which provide access to the system, and data-analysis nodes, which are well-suited for preparation of job inputs and analysis of job outputs. Compute nodes are only accessible via the Slurm job scheduling system. There are two storage types: home and work. Home is available on login nodes and data-analysis nodes. Work is available on login, data-analysis nodes and compute nodes (see I/O and file systems ). This is shown in the ARCHER2 architecture diagram: The home file system is provided by dual NetApp FAS8200A systems (one primary and one disaster recovery) with a capacity of 1 PB each. The work file system consists of four separate HPE Cray L300 storage systems, each with a capacity of 3.6 PB. The interconnect uses a dragonfly topology, and has a bandwidth of 100 Gbps. The system also includes 1.1 PB burst buffer NVMe storage, provided by an HPE Cray E1000F. Note The NVMe storage is currently in preparation, and is planned to be made available to users in Spring 2022. Compute node details The compute nodes each have 128 cores. They are dual socket nodes with two 64 core AMD EPYC 7742 processors. Note Note due to Simultaneous Multi-Threading (SMT) each core has 2 threads, therefore a node has 128 cores / 256 threads. Most users will not want to use SMT, see Launching parallel jobs . Component Details Processor 2x AMD Zen2 (Romes) EPYC 7742, 64-core, 2.25 Ghz Cores per node 128 NUMA structure 8 NUMA regions per node (16 cores per NUMA region) Memory per node 256 GB (standard), 512 GB (high memory) Memory per core 2 GB (standard), 4 GB (high memory) L1 cache 32 kB/core L2 cache 512 kB/core L3 cache 16 MB/4-cores Vector support AVX2 Network connection 2x 100 Gb/s injection ports per node Memory details The 5,276 standard nodes each have 256 GB and the 584 high memory nodes each have 512 GB. All memory is 8-channel, DDR4 3200MHz with 204.8 GB/s peak bandwidth. Interconnect details ARCHER2 has a HPE Slingshot interconnect with 200 Gb/s signalling. It uses a dragonfly topology: Nodes are organized into groups. 128 Nodes in a group. Electrical links between Network Interface Card (NIC) and switch. 16 switches per group. 2x NIC per node. All-to-all connection amongst switches in a group using electrical links. All-to-all connection between groups using optical links. 2 groups per ARCHER2 Cabinet.","title":"Hardware"},{"location":"user-guide/hardware/#archer2-hardware","text":"","title":"ARCHER2 hardware"},{"location":"user-guide/hardware/#system-overview","text":"ARCHER2 is a HPE Cray EX supercomputing system which has a total of 5,860 compute nodes. Each compute node has 128 cores (dual AMD EPYC 7742 64-core 2.25GHz processors) giving a total of 750,080 cores. Compute nodes are connected together by a HPE Slingshot interconnect. There are additional User Access Nodes (UAN, also called login nodes), which provide access to the system, and data-analysis nodes, which are well-suited for preparation of job inputs and analysis of job outputs. Compute nodes are only accessible via the Slurm job scheduling system. There are two storage types: home and work. Home is available on login nodes and data-analysis nodes. Work is available on login, data-analysis nodes and compute nodes (see I/O and file systems ). This is shown in the ARCHER2 architecture diagram: The home file system is provided by dual NetApp FAS8200A systems (one primary and one disaster recovery) with a capacity of 1 PB each. The work file system consists of four separate HPE Cray L300 storage systems, each with a capacity of 3.6 PB. The interconnect uses a dragonfly topology, and has a bandwidth of 100 Gbps. The system also includes 1.1 PB burst buffer NVMe storage, provided by an HPE Cray E1000F. Note The NVMe storage is currently in preparation, and is planned to be made available to users in Spring 2022.","title":"System overview"},{"location":"user-guide/hardware/#compute-node-details","text":"The compute nodes each have 128 cores. They are dual socket nodes with two 64 core AMD EPYC 7742 processors. Note Note due to Simultaneous Multi-Threading (SMT) each core has 2 threads, therefore a node has 128 cores / 256 threads. Most users will not want to use SMT, see Launching parallel jobs . Component Details Processor 2x AMD Zen2 (Romes) EPYC 7742, 64-core, 2.25 Ghz Cores per node 128 NUMA structure 8 NUMA regions per node (16 cores per NUMA region) Memory per node 256 GB (standard), 512 GB (high memory) Memory per core 2 GB (standard), 4 GB (high memory) L1 cache 32 kB/core L2 cache 512 kB/core L3 cache 16 MB/4-cores Vector support AVX2 Network connection 2x 100 Gb/s injection ports per node","title":"Compute node details"},{"location":"user-guide/hardware/#memory-details","text":"The 5,276 standard nodes each have 256 GB and the 584 high memory nodes each have 512 GB. All memory is 8-channel, DDR4 3200MHz with 204.8 GB/s peak bandwidth.","title":"Memory details"},{"location":"user-guide/hardware/#interconnect-details","text":"ARCHER2 has a HPE Slingshot interconnect with 200 Gb/s signalling. It uses a dragonfly topology: Nodes are organized into groups. 128 Nodes in a group. Electrical links between Network Interface Card (NIC) and switch. 16 switches per group. 2x NIC per node. All-to-all connection amongst switches in a group using electrical links. All-to-all connection between groups using optical links. 2 groups per ARCHER2 Cabinet.","title":"Interconnect details"},{"location":"user-guide/io/","text":"I/O performance and tuning This section describes common IO patterns and how to get good performance on the ARCHER2 storage. Information on the file systems, directory layouts, quotas, archiving and transferring data can be found in the Data management and transfer section . The advice here is targetted at use of the parallel file systems available on the compute nodes on ARCHER2 (i.e. Not the home and RDFaaS file systems). Common I/O patterns There are number of I/O patterns that are frequently used in parallel applications: Single file, single writer (Serial I/O) A common approach is to funnel all the I/O through one controller process (e.g. rank 0 in an MPI program). Although this has the advantage of producing a single file, the fact that only one client is doing all the I/O means that it gains little benefit from the parallel file system. In practice this severely limits the I/O rates, e.g. when writing large files the speed is not likely to significantly exceed 1 GB/s. File-per-process (FPP) One of the first parallel strategies people use for I/O is for each parallel process to write to its own file. This is a simple scheme to implement and understand and can achieve high bandwidth as, with many I/O clients active at once, it benefits from the parallel Lustre filesystem. However, it has the distinct disadvantage that the data is spread across many different files and may therefore be very difficult to use for further analysis without a data reconstruction stage to recombine potentially thousands of small files. In addition, having thousands of files open at once can overload the filesystem and lead to poor performance. File-per-node (FPN) A simple way to reduce the sheer number of files is to write a file per node rather than a file per process; as ARCHER2 has 128 CPU-cores per node, this can reduce the number of files by more than a factor of 100 and should not significantly affect the I/O rates. However, it still produces multiple files which can be hard to work with in practice. Single file, multiple writers without collective operations All aspects of data management are simpler if your parallel program produces a single file in the same format as a serial code, e.g. analysis or program restart are much more straightforward. There are a number of ways to achieve this. For example, many processes can open the same file but access different parts by skipping some initial offset, although this is problematic when writing as locking may be needed to ensure consistency. Parallel I/O libraries such as MPI-IO, HDF5 and NetCDF allow for this form of access and will implement locking automatically. The problem is that, with many clients all individually accessing the same file, there can be a lot of contention for file system resources, leading to poor I/O rates. When writing, file locking can effectively serialise the access and there is no benefit from the parallel filesystem. Single Shared File with collective writes (SSF) The problem with having many clients performing I/O at the same time is that the I/O library may have to restrict access to one client at a time by locking. However if I/O is done collectively, where the library knows that all clients are doing I/O at the same time, then reads and writes can be explicitly coordinated to avoid clashes and no locking is required. It is only through collective I/O that the full bandwidth of the file system can be realised while accessing a single file. Whatever I/O library you are using, it is essential to use collective forms of the read and write calls to achieve good performance. Achieving efficient I/O This section provides information on getting the best performance out of the parallel /work file systems on ARCHER2 when writing data, particularly using parallel I/O patterns. Lustre The ARCHER2 /work file systems use Lustre as a parallel file system technology. It has many disk units (called Object Storage Targets or OSTs), all under the control of a single Meta Data Server (MDS) so that it appears to the user as a single file system. The Lustre file system provides POSIX semantics (changes on one node are immediately visible on other nodes) and can support very high data rates for appropriate I/O patterns. Striping One of the main factors leading to the high performance of Lustre file systems is the ability to store data on multiple OSTs. For many small files, this is achieved by storing different files on different OSTs; large files must be striped across multiple OSTs to benefit from the parallel nature of Lustre. When a file is striped it is split into chunks and stored across multiple OSTs in a round-robin fashion. Striping can improve the I/O performance because it increases the available bandwidth: multiple processes can read and write the same file simultaneously by accessing different OSTs. However striping can also increase the overhead. Choosing the right striping configuration is key to obtain high performance results. Users have control of a number of striping settings on Lustre file systems. Although these parameters can be set on a per-file basis they are usually set on the directory where your output files will be written so that all output files inherit the same settings. Default configuration The /work file systems on ARCHER2 have the same default stripe settings: A default stripe count of 1 A default stripe size of 1 MiB (2 20 = 1048576 bytes) These settings have been chosen to provide a good compromise for the wide variety of I/O patterns that are seen on the system but are unlikely to be optimal for any one particular scenario. The Lustre command to query the stripe settings for a directory (or file) is lfs getstripe . For example, to query the stripe settings of an already created directory resdir : auser@ln03:~> lfs getstripe resdir/ resdir stripe_count: 1 stripe_size: 1048576 stripe_offset: -1 Setting Custom Striping Configurations Users can set stripe settings for a directory (or file) using the lfs setstripe command. The options for lfs setstripe are: [--stripe-count|-c] to set the stripe count; 0 means use the system default (usually 1) and -1 means stripe over all available OSTs. [--stripe-size|-S] to set the stripe size; 0 means use the system default (usually 1 MB) otherwise use k, m or g for KB, MB or GB respectively [--stripe-index|-i] to set the OST index (starting at 0) on which to start striping for this file. An index of -1 allows the MDS to choose the starting index and it is strongly recommended, as this allows space and load balancing to be done by the MDS as needed. For example, to set a stripe size of 4 MiB for the existing directory resdir , along with maximum striping count you would use: auser@ln03:~> lfs setstripe -S 4m -c -1 resdir/ Recommended ARCHER2 I/O settings As mentioned above, it is very important to use collective calls when doing parallel I/O to a single shared file. However, with the default settings, parallel I/O on multiple nodes can currently give poor performance. We recommend always setting the following environment variable in your SLURM batch script: export FI_OFI_RXM_SAR_LIMIT=64K Although I/O requirements vary significantly between different applications, the following settings should be good in most cases: If each process or node is writing to its own individual file then the default settings (unstriped files) should give good performance. If processes are writing to a single shared file (e.g. using MPI-IO, HDF5 or NetCDF), set the appropriate directories to be fully striped: lfs setstripe -c -1 resdir . On ARCHER2 this will use all of the 12 OSTs. Alternative MPI library Setting the environment variable FI_OFI_RXM_SAR_LIMIT can improve the performance of MPI collectives when handling large amounts of data, which in turn can improve collective file I/O. An alternative is to use the non-default UCX implementation of the MPI library as an alternative to the default OFI version. To switch library version see the Application Development Environment section of the User Guide . Note This will affect all your MPI calls, not just those related to I/O, so you should check the overall performance of your program before and after the switch. It is possible that other functions may run slower even if the I/O performance improves. I/O Profiling If you are concerned about your I/O performance, you should quantify your transfer rates in terms of GB/s of data read or written to disk. Small files can achieve very high I/O rates due to data caching in Lustre. However, for large files you should be able to achieve a maximum of around 1 GB/s for an unstriped file, or up to 10 GB/s for a fully striped file (across all 12 OSTs). Warning You share /work with all other users so I/O rates can be very variable, especially if the machine is heavily loaded. If your I/O rates are poor then you can get useful summary information about how the parallel libraries are performing by setting this variable in your Slurm script export MPICH_MPIIO_STATS=1 Amongst other things, this will give you information on how many independent and collective I/O operations were issued. If you see a large number of independent operations compared to collectives, this indicates that you have inefficient I/O patterns and you should check that you are calling your parallel I/O library correctly. Although this information comes from the MPI library, it is still useful for users of higher-level libraries such as HDF5 as they all call MPI-IO at the lowest level. Note We will add additional advice on I/O profiling soon.","title":"I/O and file systems"},{"location":"user-guide/io/#io-performance-and-tuning","text":"This section describes common IO patterns and how to get good performance on the ARCHER2 storage. Information on the file systems, directory layouts, quotas, archiving and transferring data can be found in the Data management and transfer section . The advice here is targetted at use of the parallel file systems available on the compute nodes on ARCHER2 (i.e. Not the home and RDFaaS file systems).","title":"I/O performance and tuning"},{"location":"user-guide/io/#common-io-patterns","text":"There are number of I/O patterns that are frequently used in parallel applications:","title":"Common I/O patterns"},{"location":"user-guide/io/#single-file-single-writer-serial-io","text":"A common approach is to funnel all the I/O through one controller process (e.g. rank 0 in an MPI program). Although this has the advantage of producing a single file, the fact that only one client is doing all the I/O means that it gains little benefit from the parallel file system. In practice this severely limits the I/O rates, e.g. when writing large files the speed is not likely to significantly exceed 1 GB/s.","title":"Single file, single writer (Serial I/O)"},{"location":"user-guide/io/#file-per-process-fpp","text":"One of the first parallel strategies people use for I/O is for each parallel process to write to its own file. This is a simple scheme to implement and understand and can achieve high bandwidth as, with many I/O clients active at once, it benefits from the parallel Lustre filesystem. However, it has the distinct disadvantage that the data is spread across many different files and may therefore be very difficult to use for further analysis without a data reconstruction stage to recombine potentially thousands of small files. In addition, having thousands of files open at once can overload the filesystem and lead to poor performance.","title":"File-per-process (FPP)"},{"location":"user-guide/io/#file-per-node-fpn","text":"A simple way to reduce the sheer number of files is to write a file per node rather than a file per process; as ARCHER2 has 128 CPU-cores per node, this can reduce the number of files by more than a factor of 100 and should not significantly affect the I/O rates. However, it still produces multiple files which can be hard to work with in practice.","title":"File-per-node (FPN)"},{"location":"user-guide/io/#single-file-multiple-writers-without-collective-operations","text":"All aspects of data management are simpler if your parallel program produces a single file in the same format as a serial code, e.g. analysis or program restart are much more straightforward. There are a number of ways to achieve this. For example, many processes can open the same file but access different parts by skipping some initial offset, although this is problematic when writing as locking may be needed to ensure consistency. Parallel I/O libraries such as MPI-IO, HDF5 and NetCDF allow for this form of access and will implement locking automatically. The problem is that, with many clients all individually accessing the same file, there can be a lot of contention for file system resources, leading to poor I/O rates. When writing, file locking can effectively serialise the access and there is no benefit from the parallel filesystem.","title":"Single file, multiple writers without collective operations"},{"location":"user-guide/io/#single-shared-file-with-collective-writes-ssf","text":"The problem with having many clients performing I/O at the same time is that the I/O library may have to restrict access to one client at a time by locking. However if I/O is done collectively, where the library knows that all clients are doing I/O at the same time, then reads and writes can be explicitly coordinated to avoid clashes and no locking is required. It is only through collective I/O that the full bandwidth of the file system can be realised while accessing a single file. Whatever I/O library you are using, it is essential to use collective forms of the read and write calls to achieve good performance.","title":"Single Shared File with collective writes (SSF)"},{"location":"user-guide/io/#achieving-efficient-io","text":"This section provides information on getting the best performance out of the parallel /work file systems on ARCHER2 when writing data, particularly using parallel I/O patterns.","title":"Achieving efficient I/O"},{"location":"user-guide/io/#lustre","text":"The ARCHER2 /work file systems use Lustre as a parallel file system technology. It has many disk units (called Object Storage Targets or OSTs), all under the control of a single Meta Data Server (MDS) so that it appears to the user as a single file system. The Lustre file system provides POSIX semantics (changes on one node are immediately visible on other nodes) and can support very high data rates for appropriate I/O patterns.","title":"Lustre"},{"location":"user-guide/io/#striping","text":"One of the main factors leading to the high performance of Lustre file systems is the ability to store data on multiple OSTs. For many small files, this is achieved by storing different files on different OSTs; large files must be striped across multiple OSTs to benefit from the parallel nature of Lustre. When a file is striped it is split into chunks and stored across multiple OSTs in a round-robin fashion. Striping can improve the I/O performance because it increases the available bandwidth: multiple processes can read and write the same file simultaneously by accessing different OSTs. However striping can also increase the overhead. Choosing the right striping configuration is key to obtain high performance results. Users have control of a number of striping settings on Lustre file systems. Although these parameters can be set on a per-file basis they are usually set on the directory where your output files will be written so that all output files inherit the same settings.","title":"Striping"},{"location":"user-guide/io/#default-configuration","text":"The /work file systems on ARCHER2 have the same default stripe settings: A default stripe count of 1 A default stripe size of 1 MiB (2 20 = 1048576 bytes) These settings have been chosen to provide a good compromise for the wide variety of I/O patterns that are seen on the system but are unlikely to be optimal for any one particular scenario. The Lustre command to query the stripe settings for a directory (or file) is lfs getstripe . For example, to query the stripe settings of an already created directory resdir : auser@ln03:~> lfs getstripe resdir/ resdir stripe_count: 1 stripe_size: 1048576 stripe_offset: -1","title":"Default configuration"},{"location":"user-guide/io/#setting-custom-striping-configurations","text":"Users can set stripe settings for a directory (or file) using the lfs setstripe command. The options for lfs setstripe are: [--stripe-count|-c] to set the stripe count; 0 means use the system default (usually 1) and -1 means stripe over all available OSTs. [--stripe-size|-S] to set the stripe size; 0 means use the system default (usually 1 MB) otherwise use k, m or g for KB, MB or GB respectively [--stripe-index|-i] to set the OST index (starting at 0) on which to start striping for this file. An index of -1 allows the MDS to choose the starting index and it is strongly recommended, as this allows space and load balancing to be done by the MDS as needed. For example, to set a stripe size of 4 MiB for the existing directory resdir , along with maximum striping count you would use: auser@ln03:~> lfs setstripe -S 4m -c -1 resdir/","title":"Setting Custom Striping Configurations"},{"location":"user-guide/io/#recommended-archer2-io-settings","text":"As mentioned above, it is very important to use collective calls when doing parallel I/O to a single shared file. However, with the default settings, parallel I/O on multiple nodes can currently give poor performance. We recommend always setting the following environment variable in your SLURM batch script: export FI_OFI_RXM_SAR_LIMIT=64K Although I/O requirements vary significantly between different applications, the following settings should be good in most cases: If each process or node is writing to its own individual file then the default settings (unstriped files) should give good performance. If processes are writing to a single shared file (e.g. using MPI-IO, HDF5 or NetCDF), set the appropriate directories to be fully striped: lfs setstripe -c -1 resdir . On ARCHER2 this will use all of the 12 OSTs.","title":"Recommended ARCHER2 I/O settings"},{"location":"user-guide/io/#alternative-mpi-library","text":"Setting the environment variable FI_OFI_RXM_SAR_LIMIT can improve the performance of MPI collectives when handling large amounts of data, which in turn can improve collective file I/O. An alternative is to use the non-default UCX implementation of the MPI library as an alternative to the default OFI version. To switch library version see the Application Development Environment section of the User Guide . Note This will affect all your MPI calls, not just those related to I/O, so you should check the overall performance of your program before and after the switch. It is possible that other functions may run slower even if the I/O performance improves.","title":"Alternative MPI library"},{"location":"user-guide/io/#io-profiling","text":"If you are concerned about your I/O performance, you should quantify your transfer rates in terms of GB/s of data read or written to disk. Small files can achieve very high I/O rates due to data caching in Lustre. However, for large files you should be able to achieve a maximum of around 1 GB/s for an unstriped file, or up to 10 GB/s for a fully striped file (across all 12 OSTs). Warning You share /work with all other users so I/O rates can be very variable, especially if the machine is heavily loaded. If your I/O rates are poor then you can get useful summary information about how the parallel libraries are performing by setting this variable in your Slurm script export MPICH_MPIIO_STATS=1 Amongst other things, this will give you information on how many independent and collective I/O operations were issued. If you see a large number of independent operations compared to collectives, this indicates that you have inefficient I/O patterns and you should check that you are calling your parallel I/O library correctly. Although this information comes from the MPI library, it is still useful for users of higher-level libraries such as HDF5 as they all call MPI-IO at the lowest level. Note We will add additional advice on I/O profiling soon.","title":"I/O Profiling"},{"location":"user-guide/profile/","text":"Profiling There are a number of different ways to access profiling data on ARCHER2. In this section, we discuss the HPE Cray profiling tools, CrayPAT-lite and CrayPAT. We also show how to get usage data on currently running jobs from Slurm batch system. CrayPat-lite CrayPat-lite is a simplified and easy-to-use version of the Cray Performance Measurement and Analysis Tool (CrayPat). CrayPat-lite provides basic performance analysis information automatically, with a minimum of user interaction, and yet offers information useful to users wishing to explore a program's behaviour further using the full CrayPat suite. How to use CrayPat-lite Ensure the perftools-base module is loaded. module list Load the perftools-lite module. module load perftools-lite Compile your application normally. An informational message from CrayPat-lite will appear indicating that the executable has been instrumented. cc -h std=c99 -o myapplication.x myapplication.c INFO: creating the CrayPat-instrumented executable 'myapplication.x' (lite-samples) ...OK Run the generated executable normally by submitting a job. #!/bin/bash #SBATCH --job-name=craypat_test #SBATCH --nodes=4 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 #SBATCH --time=00:20:00 #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Launch the parallel program srun --hint=nomultithread --distribution=block:block mpi_test.x Analyse the data. After the job finishes executing, CrayPat-lite output should be printed to stdout (i.e. at the end of the job's output file). A new directory will also be created containing .rpt and .ap2 files. The .rpt files are text files that contain the same information printed in the job's output file and the .ap2 files can be used to obtain more detailed information, which can be visualized using the Cray Apprentice2 tool . Further help CrayPat-lite User Guide CrayPat The Cray Performance Analysis Tool (CrayPAT) is a powerful framework for analysing a parallel application\u2019s performance on Cray supercomputers. It can provide very detailed information about the timing and performance of individual application procedures. CrayPat can perform two types of performance analysis, sampling experiments and tracing experiments. A sampling experiment probes the code at a predefined interval and produces a report based on the data collected. A tracing experiment explicitly monitors the code performance within named routines. Typically, the overhead associated with a tracing experiment is higher than that associated with a sampling experiment but provides much more detailed information. The key to getting useful data out of a sampling experiment is to run your profiling for a representative length of time. Sampling analysis Ensure the perftools-base module is loaded. module list Load perftools module. module load perftools Compile your code in the standard way always using the Cray compiler wrappers (ftn, cc and CC). Object files need to be made available to CrayPat to correctly build an instrumented executable for profiling or tracing, this means that the compile and link stage should be separated by using the -c compile flag. auser@ln01:/work/t01/t01/auser> cc -h std=c99 -c jacobi.c auser@ln01:/work/t01/t01/auser> cc jacobi.o -o jacobi To instrument the binary, run the pat_build command. This will generate a new binary with +pat appended to the end (e.g. jacobi+pat ). auser@ln:/work/t01/t01/auser> pat_build jacobi Run the new executable with +pat appended as you would with the regular executable. Each run will produce its own 'experiment directory' containing the performance data as .xf files inside a subdirectory called xf-files (e.g. running the jacobi+pat instrumented executable might produce jacobi+pat+12265-1573s/xf-files ). Generate report data with pat_report . The .xf files contain the raw sampling data from the run and need to be post-processed to produce useful results. This is done using the pat_report tool which converts all the raw data into a summarised and readable form. You should provide the name of the experiment directory as the argument to pat_report . auser@ln:/work/t01/t01/auser> pat_report jacobi+pat+12265-1573s Table 1: Profile by Function (limited entries shown) Samp% | Samp | Imb. | Imb. | Group | | Samp | Samp% | Function | | | | PE=HIDE 100.0% | 849.5 | -- | -- | Total |-------------------------------------------------- | 56.7% | 481.4 | -- | -- | MPI ||------------------------------------------------- || 48.7% | 414.1 | 50.9 | 11.0% | MPI_Allreduce || 4.4% | 37.5 | 118.5 | 76.6% | MPI_Waitall || 3.0% | 25.2 | 44.8 | 64.5% | MPI_Isend ||================================================= | 29.9% | 253.9 | 55.1 | 18.0% | USER ||------------------------------------------------- || 29.9% | 253.9 | 55.1 | 18.0% | main ||================================================= | 13.4% | 114.1 | -- | -- | ETC ||------------------------------------------------- || 13.4% | 113.9 | 26.1 | 18.8% | __cray_memcpy_SNB |================================================== This report will generate more files with the extension .ap2 in the experiment directory. These hold the same data as the .xf files but in the post-processed form. Another file produced has an .apa extension and is a text file with a suggested configuration for generating a traced experiment. The .ap2 files generated are used to view performance data graphically with the Cray Apprentice2 tool . The pat_report command is able to produce many different profile reports from the profiling data. You can select a predefined report with the -O flag to pat_report . A selection of the most generally useful predefined report types are:= listed below. ca+src - Show the callers (bottom-up view) leading to the routines that have a high use in the report and include source code line numbers for the calls and time-consuming statements. load_balance - Show load-balance statistics for the high-use routines in the program. Parallel processes with minimum, maximum and median times for routines will be displayed. Only available with tracing experiments. mpi_callers - Show MPI message statistics. Only available with tracing experiments. Example output: auser@ln01:/work/t01/t01/auser> pat_report -O ca+src,load_balance jacobi+pat+12265-1573s Table 1: Profile by Function and Callers, with Line Numbers (limited entries shown) Samp% | Samp | Imb. | Imb. | Group | | Samp | Samp% | Function | | | | PE=HIDE 100.0% | 849.5 | -- | -- | Total |-------------------------------------------------- |-------------------------------------- | 56.7% | 481.4 | MPI ||------------------------------------- || 48.7% | 414.1 | MPI_Allreduce 3| | | main:jacobi.c:line.80 || 4.4% | 37.5 | MPI_Waitall 3| | | main:jacobi.c:line.73 || 3.0% | 25.2 | MPI_Isend |||------------------------------------ 3|| 1.6% | 13.2 | main:jacobi.c:line.65 3|| 1.4% | 12.0 | main:jacobi.c:line.69 ||===================================== | 29.9% | 253.9 | USER ||------------------------------------- || 29.9% | 253.9 | main |||------------------------------------ 3|| 18.7% | 159.0 | main:jacobi.c:line.76 3|| 9.1% | 76.9 | main:jacobi.c:line.84 |||==================================== ||===================================== | 13.4% | 114.1 | ETC ||------------------------------------- || 13.4% | 113.9 | __cray_memcpy_SNB 3| | | __cray_memcpy_SNB |====================================== Tracing analysis Automatic Program Analysis (APA) We can produce a focused tracing experiment based on the results from the sampling experiment using pat_build with the .apa file produced during the sampling. auser@ln01:/work/t01/t01/auser> pat_build -O jacobi+pat+12265-1573s/build-options.apa This will produce a third binary with extension +apa . This binary should once again be run on the compute nodes and the name of the executable changed to jacobi+apa . As with the sampling analysis, a report can be produced using pat_report . For example: auser@ln01:/work/t01/t01/auser> pat_report jacobi+apa+13955-1573t Table 1: Profile by Function Group and Function (limited entries shown) Time% | Time | Imb. | Imb. | Calls | Group | | Time | Time% | | Function | | | | | PE=HIDE 100.0% | 12.987762 | -- | -- | 1,387,544.9 | Total |------------------------------------------------------------------------- | 44.9% | 5.831320 | -- | -- | 2.0 | USER ||------------------------------------------------------------------------ || 44.9% | 5.831229 | 0.398671 | 6.4% | 1.0 | main ||======================================================================== | 29.2% | 3.789904 | -- | -- | 199,111.0 | MPI_SYNC ||------------------------------------------------------------------------ || 29.2% | 3.789115 | 1.792050 | 47.3% | 199,109.0 | MPI_Allreduce(sync) ||======================================================================== | 25.9% | 3.366537 | -- | -- | 1,188,431.9 | MPI ||------------------------------------------------------------------------ || 18.0% | 2.334765 | 0.164646 | 6.6% | 199,109.0 | MPI_Allreduce || 3.7% | 0.486714 | 0.882654 | 65.0% | 199,108.0 | MPI_Waitall || 3.3% | 0.428731 | 0.557342 | 57.0% | 395,104.9 | MPI_Isend |========================================================================= Manual Program Analysis CrayPat allows you to manually choose your profiling preference. This is particularly useful if the APA mode does not meet your tracing analysis requirements. The entire program can be traced as a whole using -w : auser@ln01:/work/t01/t01/auser> pat_build -w jacobi Using -g , a program can be instrumented to trace all function entry point references belonging to the trace function group (mpi, libsci, lapack, scalapack, heap, etc): auser@ln01:/work/t01/t01/auser> pat_build -w -g mpi jacobi Dynamically-linked binaries CrayPat allows you to profile un-instrumented, dynamically linked binaries with the pat_run utility. pat_run delivers profiling information for codes that cannot easily be rebuilt. To use pat_run : Load the perftools-base module if it is not already loaded. module load perftools-base Run your application normally including the pat_run command right after your srun options. srun [srun-options] pat_run [pat_run-options] program [program-options] Use pat_report to examine any data collected during the execution of your application. auser@ln01:/work/t01/t01/auser> pat_report jacobi+pat+12265-1573s Some useful pat_run options are as follows. -w Collect data by tracing. -g Trace functions belonging to group names. See the -g option in pat_build(1) for a list of valid tracegroup values. -r Generate a text report upon successful execution. Further help CrayPat User Guide Cray Apprentice2 Cray Apprentice2 is an optional GUI tool that is used to visualize and manipulate the performance analysis data captured during program execution. Cray Apprentice2 can display a wide variety of reports and graphs, depending on the type of program being analyzed, the way in which the program was instrumented for data capture, and the data that was collected during program execution. You will need to use CrayPat to first instrument your program and capture performance analysis data, and then pat_report to generate the .ap2 files from the results. You may then use Cray Apprentice2 to visualize and explore those files. The number and appearance of the reports that can be generated using Cray Apprentice2 is determined by the kind and quantity of data captured during program execution, which in turn is determined by the way in which the program was instrumented and the environment variables in effect at the time of program execution. For example, changing the PAT_RT_SUMMARY environment variable to 0 before executing the instrumented program nearly doubles the number of reports available when analyzing the resulting data in Cray Apprentice2. export PAT_RT_SUMMARY=0 To use Cray Apprentice2 ( app2 ), load perftools-base module if it is not already loaded. module load perftools-base Next, open the experiment directory generated during the instrumentation phase with Apprentice2. auser@ln01:/work/t01/t01/auser> app2 jacobi+pat+12265-1573s Hardware Performance Counters Note Information on hardware counters will be added soon. Performance and profiling data in Slurm Slurm commands on the login nodes can be used to quickly and simply retrieve information about memory usage for currently running and completed jobs. There are three commands you can use on ARCHER2 to query job data from Slurm, two are standard Slurm commands and one is a script that provides information on running jobs: The sstat command is used to display status information of a running job or job step The sacct command is used to display accounting data for all finished jobs and job steps within the Slurm job database. The archer2jobload command is used to show CPU and memory usage information for running jobs. (This script is based on one originally written for the COSMA HPC facility at the University of Durham.) We provide examples of the use of these three commands below. For the sacct and sstat command, the memory properties we print out below are: AveRSS - The mean memory use per node over the length of the job MaxRSS - The maximum memory use per node measured during the job MaxRSSTask - The maximum memory use from any process in the job Example 1: sstat for running jobs To display the current memory use of a running job with the ID 123456: sstat --format=JobID,AveCPU,AveRSS,MaxRSS,MaxRSSTask -j 123456 Example 2: sacct for finished jobs To display the memory use of a completed job with the ID 123456: sacct --format=JobID,JobName,AveRSS,MaxRSS,MaxRSSTask -j 123456 Another usage of sacct is to display when a job was submitted, started running and ended for a particular user: sacct --format=JobID,Submit,Start,End -u auser Example 3: archer2jobload for running jobs Using the archer2jobload command on its own with no options will show the current CPU and memory use across compute nodes for all running jobs. More usefully, you can provide a job ID to archer2jobload and it will show a summary of the CPU and memory use for a specific job. For example, to get the usage data for job 123456, you would use: auser@ln01:~> archer2jobload 123456 # JOB: 123456 CPU_LOAD MEMORY ALLOCMEM FREE_MEM TMP_DISK NODELIST 127.35-127.86 256000 239872 169686-208172 0 nid[001481,001638-00 This shows the minimum CPU load on a compute node is 126.04 (close to the limit of 128 cores) with the maximum load 127.41 (indicating all the nodes are being used evenly). The minimum free memory is 171893 MB and the maximum free memory is 177224 MB. If you add the -l option, you will see a breakdown per node: auser@ln01:~> archer2jobload -l 276236 # JOB: 123456 NODELIST CPU_LOAD MEMORY ALLOCMEM FREE_MEM TMP_DISK nid001481 127.86 256000 239872 169686 0 nid001638 127.60 256000 239872 171060 0 nid001639 127.64 256000 239872 171253 0 nid001677 127.85 256000 239872 173820 0 nid001678 127.75 256000 239872 173170 0 nid001891 127.63 256000 239872 173316 0 nid001921 127.65 256000 239872 207562 0 nid001922 127.35 256000 239872 208172 0 Further help with Slurm The definitions of any variables discussed here and more usage information can be found in the man pages of sstat and sacct .","title":"Profiling"},{"location":"user-guide/profile/#profiling","text":"There are a number of different ways to access profiling data on ARCHER2. In this section, we discuss the HPE Cray profiling tools, CrayPAT-lite and CrayPAT. We also show how to get usage data on currently running jobs from Slurm batch system.","title":"Profiling"},{"location":"user-guide/profile/#craypat-lite","text":"CrayPat-lite is a simplified and easy-to-use version of the Cray Performance Measurement and Analysis Tool (CrayPat). CrayPat-lite provides basic performance analysis information automatically, with a minimum of user interaction, and yet offers information useful to users wishing to explore a program's behaviour further using the full CrayPat suite.","title":"CrayPat-lite"},{"location":"user-guide/profile/#how-to-use-craypat-lite","text":"Ensure the perftools-base module is loaded. module list Load the perftools-lite module. module load perftools-lite Compile your application normally. An informational message from CrayPat-lite will appear indicating that the executable has been instrumented. cc -h std=c99 -o myapplication.x myapplication.c INFO: creating the CrayPat-instrumented executable 'myapplication.x' (lite-samples) ...OK Run the generated executable normally by submitting a job. #!/bin/bash #SBATCH --job-name=craypat_test #SBATCH --nodes=4 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 #SBATCH --time=00:20:00 #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Launch the parallel program srun --hint=nomultithread --distribution=block:block mpi_test.x Analyse the data. After the job finishes executing, CrayPat-lite output should be printed to stdout (i.e. at the end of the job's output file). A new directory will also be created containing .rpt and .ap2 files. The .rpt files are text files that contain the same information printed in the job's output file and the .ap2 files can be used to obtain more detailed information, which can be visualized using the Cray Apprentice2 tool .","title":"How to use CrayPat-lite"},{"location":"user-guide/profile/#further-help","text":"CrayPat-lite User Guide","title":"Further help"},{"location":"user-guide/profile/#craypat","text":"The Cray Performance Analysis Tool (CrayPAT) is a powerful framework for analysing a parallel application\u2019s performance on Cray supercomputers. It can provide very detailed information about the timing and performance of individual application procedures. CrayPat can perform two types of performance analysis, sampling experiments and tracing experiments. A sampling experiment probes the code at a predefined interval and produces a report based on the data collected. A tracing experiment explicitly monitors the code performance within named routines. Typically, the overhead associated with a tracing experiment is higher than that associated with a sampling experiment but provides much more detailed information. The key to getting useful data out of a sampling experiment is to run your profiling for a representative length of time.","title":"CrayPat"},{"location":"user-guide/profile/#sampling-analysis","text":"Ensure the perftools-base module is loaded. module list Load perftools module. module load perftools Compile your code in the standard way always using the Cray compiler wrappers (ftn, cc and CC). Object files need to be made available to CrayPat to correctly build an instrumented executable for profiling or tracing, this means that the compile and link stage should be separated by using the -c compile flag. auser@ln01:/work/t01/t01/auser> cc -h std=c99 -c jacobi.c auser@ln01:/work/t01/t01/auser> cc jacobi.o -o jacobi To instrument the binary, run the pat_build command. This will generate a new binary with +pat appended to the end (e.g. jacobi+pat ). auser@ln:/work/t01/t01/auser> pat_build jacobi Run the new executable with +pat appended as you would with the regular executable. Each run will produce its own 'experiment directory' containing the performance data as .xf files inside a subdirectory called xf-files (e.g. running the jacobi+pat instrumented executable might produce jacobi+pat+12265-1573s/xf-files ). Generate report data with pat_report . The .xf files contain the raw sampling data from the run and need to be post-processed to produce useful results. This is done using the pat_report tool which converts all the raw data into a summarised and readable form. You should provide the name of the experiment directory as the argument to pat_report . auser@ln:/work/t01/t01/auser> pat_report jacobi+pat+12265-1573s Table 1: Profile by Function (limited entries shown) Samp% | Samp | Imb. | Imb. | Group | | Samp | Samp% | Function | | | | PE=HIDE 100.0% | 849.5 | -- | -- | Total |-------------------------------------------------- | 56.7% | 481.4 | -- | -- | MPI ||------------------------------------------------- || 48.7% | 414.1 | 50.9 | 11.0% | MPI_Allreduce || 4.4% | 37.5 | 118.5 | 76.6% | MPI_Waitall || 3.0% | 25.2 | 44.8 | 64.5% | MPI_Isend ||================================================= | 29.9% | 253.9 | 55.1 | 18.0% | USER ||------------------------------------------------- || 29.9% | 253.9 | 55.1 | 18.0% | main ||================================================= | 13.4% | 114.1 | -- | -- | ETC ||------------------------------------------------- || 13.4% | 113.9 | 26.1 | 18.8% | __cray_memcpy_SNB |================================================== This report will generate more files with the extension .ap2 in the experiment directory. These hold the same data as the .xf files but in the post-processed form. Another file produced has an .apa extension and is a text file with a suggested configuration for generating a traced experiment. The .ap2 files generated are used to view performance data graphically with the Cray Apprentice2 tool . The pat_report command is able to produce many different profile reports from the profiling data. You can select a predefined report with the -O flag to pat_report . A selection of the most generally useful predefined report types are:= listed below. ca+src - Show the callers (bottom-up view) leading to the routines that have a high use in the report and include source code line numbers for the calls and time-consuming statements. load_balance - Show load-balance statistics for the high-use routines in the program. Parallel processes with minimum, maximum and median times for routines will be displayed. Only available with tracing experiments. mpi_callers - Show MPI message statistics. Only available with tracing experiments. Example output: auser@ln01:/work/t01/t01/auser> pat_report -O ca+src,load_balance jacobi+pat+12265-1573s Table 1: Profile by Function and Callers, with Line Numbers (limited entries shown) Samp% | Samp | Imb. | Imb. | Group | | Samp | Samp% | Function | | | | PE=HIDE 100.0% | 849.5 | -- | -- | Total |-------------------------------------------------- |-------------------------------------- | 56.7% | 481.4 | MPI ||------------------------------------- || 48.7% | 414.1 | MPI_Allreduce 3| | | main:jacobi.c:line.80 || 4.4% | 37.5 | MPI_Waitall 3| | | main:jacobi.c:line.73 || 3.0% | 25.2 | MPI_Isend |||------------------------------------ 3|| 1.6% | 13.2 | main:jacobi.c:line.65 3|| 1.4% | 12.0 | main:jacobi.c:line.69 ||===================================== | 29.9% | 253.9 | USER ||------------------------------------- || 29.9% | 253.9 | main |||------------------------------------ 3|| 18.7% | 159.0 | main:jacobi.c:line.76 3|| 9.1% | 76.9 | main:jacobi.c:line.84 |||==================================== ||===================================== | 13.4% | 114.1 | ETC ||------------------------------------- || 13.4% | 113.9 | __cray_memcpy_SNB 3| | | __cray_memcpy_SNB |======================================","title":"Sampling analysis"},{"location":"user-guide/profile/#tracing-analysis","text":"","title":"Tracing analysis"},{"location":"user-guide/profile/#automatic-program-analysis-apa","text":"We can produce a focused tracing experiment based on the results from the sampling experiment using pat_build with the .apa file produced during the sampling. auser@ln01:/work/t01/t01/auser> pat_build -O jacobi+pat+12265-1573s/build-options.apa This will produce a third binary with extension +apa . This binary should once again be run on the compute nodes and the name of the executable changed to jacobi+apa . As with the sampling analysis, a report can be produced using pat_report . For example: auser@ln01:/work/t01/t01/auser> pat_report jacobi+apa+13955-1573t Table 1: Profile by Function Group and Function (limited entries shown) Time% | Time | Imb. | Imb. | Calls | Group | | Time | Time% | | Function | | | | | PE=HIDE 100.0% | 12.987762 | -- | -- | 1,387,544.9 | Total |------------------------------------------------------------------------- | 44.9% | 5.831320 | -- | -- | 2.0 | USER ||------------------------------------------------------------------------ || 44.9% | 5.831229 | 0.398671 | 6.4% | 1.0 | main ||======================================================================== | 29.2% | 3.789904 | -- | -- | 199,111.0 | MPI_SYNC ||------------------------------------------------------------------------ || 29.2% | 3.789115 | 1.792050 | 47.3% | 199,109.0 | MPI_Allreduce(sync) ||======================================================================== | 25.9% | 3.366537 | -- | -- | 1,188,431.9 | MPI ||------------------------------------------------------------------------ || 18.0% | 2.334765 | 0.164646 | 6.6% | 199,109.0 | MPI_Allreduce || 3.7% | 0.486714 | 0.882654 | 65.0% | 199,108.0 | MPI_Waitall || 3.3% | 0.428731 | 0.557342 | 57.0% | 395,104.9 | MPI_Isend |=========================================================================","title":"Automatic Program Analysis (APA)"},{"location":"user-guide/profile/#manual-program-analysis","text":"CrayPat allows you to manually choose your profiling preference. This is particularly useful if the APA mode does not meet your tracing analysis requirements. The entire program can be traced as a whole using -w : auser@ln01:/work/t01/t01/auser> pat_build -w jacobi Using -g , a program can be instrumented to trace all function entry point references belonging to the trace function group (mpi, libsci, lapack, scalapack, heap, etc): auser@ln01:/work/t01/t01/auser> pat_build -w -g mpi jacobi","title":"Manual Program Analysis"},{"location":"user-guide/profile/#dynamically-linked-binaries","text":"CrayPat allows you to profile un-instrumented, dynamically linked binaries with the pat_run utility. pat_run delivers profiling information for codes that cannot easily be rebuilt. To use pat_run : Load the perftools-base module if it is not already loaded. module load perftools-base Run your application normally including the pat_run command right after your srun options. srun [srun-options] pat_run [pat_run-options] program [program-options] Use pat_report to examine any data collected during the execution of your application. auser@ln01:/work/t01/t01/auser> pat_report jacobi+pat+12265-1573s Some useful pat_run options are as follows. -w Collect data by tracing. -g Trace functions belonging to group names. See the -g option in pat_build(1) for a list of valid tracegroup values. -r Generate a text report upon successful execution.","title":"Dynamically-linked binaries"},{"location":"user-guide/profile/#further-help_1","text":"CrayPat User Guide","title":"Further help"},{"location":"user-guide/profile/#cray-apprentice2","text":"Cray Apprentice2 is an optional GUI tool that is used to visualize and manipulate the performance analysis data captured during program execution. Cray Apprentice2 can display a wide variety of reports and graphs, depending on the type of program being analyzed, the way in which the program was instrumented for data capture, and the data that was collected during program execution. You will need to use CrayPat to first instrument your program and capture performance analysis data, and then pat_report to generate the .ap2 files from the results. You may then use Cray Apprentice2 to visualize and explore those files. The number and appearance of the reports that can be generated using Cray Apprentice2 is determined by the kind and quantity of data captured during program execution, which in turn is determined by the way in which the program was instrumented and the environment variables in effect at the time of program execution. For example, changing the PAT_RT_SUMMARY environment variable to 0 before executing the instrumented program nearly doubles the number of reports available when analyzing the resulting data in Cray Apprentice2. export PAT_RT_SUMMARY=0 To use Cray Apprentice2 ( app2 ), load perftools-base module if it is not already loaded. module load perftools-base Next, open the experiment directory generated during the instrumentation phase with Apprentice2. auser@ln01:/work/t01/t01/auser> app2 jacobi+pat+12265-1573s","title":"Cray Apprentice2"},{"location":"user-guide/profile/#hardware-performance-counters","text":"Note Information on hardware counters will be added soon.","title":"Hardware Performance Counters"},{"location":"user-guide/profile/#performance-and-profiling-data-in-slurm","text":"Slurm commands on the login nodes can be used to quickly and simply retrieve information about memory usage for currently running and completed jobs. There are three commands you can use on ARCHER2 to query job data from Slurm, two are standard Slurm commands and one is a script that provides information on running jobs: The sstat command is used to display status information of a running job or job step The sacct command is used to display accounting data for all finished jobs and job steps within the Slurm job database. The archer2jobload command is used to show CPU and memory usage information for running jobs. (This script is based on one originally written for the COSMA HPC facility at the University of Durham.) We provide examples of the use of these three commands below. For the sacct and sstat command, the memory properties we print out below are: AveRSS - The mean memory use per node over the length of the job MaxRSS - The maximum memory use per node measured during the job MaxRSSTask - The maximum memory use from any process in the job","title":"Performance and profiling data in Slurm"},{"location":"user-guide/profile/#example-1-sstat-for-running-jobs","text":"To display the current memory use of a running job with the ID 123456: sstat --format=JobID,AveCPU,AveRSS,MaxRSS,MaxRSSTask -j 123456","title":"Example 1: sstat for running jobs"},{"location":"user-guide/profile/#example-2-sacct-for-finished-jobs","text":"To display the memory use of a completed job with the ID 123456: sacct --format=JobID,JobName,AveRSS,MaxRSS,MaxRSSTask -j 123456 Another usage of sacct is to display when a job was submitted, started running and ended for a particular user: sacct --format=JobID,Submit,Start,End -u auser","title":"Example 2: sacct for finished jobs"},{"location":"user-guide/profile/#example-3-archer2jobload-for-running-jobs","text":"Using the archer2jobload command on its own with no options will show the current CPU and memory use across compute nodes for all running jobs. More usefully, you can provide a job ID to archer2jobload and it will show a summary of the CPU and memory use for a specific job. For example, to get the usage data for job 123456, you would use: auser@ln01:~> archer2jobload 123456 # JOB: 123456 CPU_LOAD MEMORY ALLOCMEM FREE_MEM TMP_DISK NODELIST 127.35-127.86 256000 239872 169686-208172 0 nid[001481,001638-00 This shows the minimum CPU load on a compute node is 126.04 (close to the limit of 128 cores) with the maximum load 127.41 (indicating all the nodes are being used evenly). The minimum free memory is 171893 MB and the maximum free memory is 177224 MB. If you add the -l option, you will see a breakdown per node: auser@ln01:~> archer2jobload -l 276236 # JOB: 123456 NODELIST CPU_LOAD MEMORY ALLOCMEM FREE_MEM TMP_DISK nid001481 127.86 256000 239872 169686 0 nid001638 127.60 256000 239872 171060 0 nid001639 127.64 256000 239872 171253 0 nid001677 127.85 256000 239872 173820 0 nid001678 127.75 256000 239872 173170 0 nid001891 127.63 256000 239872 173316 0 nid001921 127.65 256000 239872 207562 0 nid001922 127.35 256000 239872 208172 0","title":"Example 3: archer2jobload for running jobs"},{"location":"user-guide/profile/#further-help-with-slurm","text":"The definitions of any variables discussed here and more usage information can be found in the man pages of sstat and sacct .","title":"Further help with Slurm"},{"location":"user-guide/python/","text":"Using Python Python is supported on ARCHER2 both for running intensive parallel jobs and also as an analysis tool. This section describes how to use Python in either of these scenarios. The Python installations on ARCHER2 contain some of the most commonly used modules. If you wish to install additional Python modules, we recommend that you use the pip command after loading the cray-python module. This is described in more detail below. Note When you log onto ARCHER2, no Python module is loaded by default. You will generally need to load the cray-python module to access the functionality described below. Running python without loading a module first will result in your using the operating system default Python which is likely not what you intend. HPE Cray Python distribution The recommended way to use Python on ARCHER2 is to use the HPE Cray Python distribution. The HPE Cray distribution provides Python 3 along with some of the most common packages used for scientific computation and data analysis. These include: numpy and scipy - built using GCC against HPE Cray LibSci mpi4py - built using GCC against HPE Cray MPICH dask The HPE Cray Python distribution can be loaded (either on the front-end or in a submission script) using: module load cray-python Tip The HPE Cray Python distribution provides Python 3. There is no Python 2 version as Python 2 is now deprecated. Tip The HPE Cray Python distribution is built using GCC compilers. If you wish to compile your own Python, C/C++ or Fortran code to use with HPE Cray Python, you should ensure that you compile using PrgEnv-gnu to make sure they are compatible. Adding your own packages If the packages you require are not included in the HPE Cray Python distribution, further packages can be added using pip . However, as the /home file systems are not available on the compute nodes, you will need to modify the default install location that pip uses to point to a location on the /work file systems (by default, pip installs into $HOME/.local ). To do this, you set the PYTHONUSERBASE environment variable to point to a location on /work, for example: export PYTHONUSERBASE=/work/t01/t01/auser/.local You will also need to ensure that: the location of commands installed by pip are available on the command line by modifying the PATH environment variable; any packages you install are available to Python by modifying the PYTHONPATH environment variable. You can do this in the following way (once you have set PYTHONUSERBASE as described above): export PATH=$PYTHONUSERBASE/bin:$PATH export PYTHONPATH=$PYTHONUSERBASE/lib/python3.8/site-packages:$PYTHONPATH We would recommend adding all three of these commands to your $HOME/.bashrc file to ensure they are set by default when you log in to ARCHER2. Once, you have done this, you can use pip to add packages on top of the HPE Cray Python environment. This can be done using: module load cray-python pip install --user <package_name> This uses the --user flag to ensure the packages are installed in the directory specified by PYTHONUSERBASE . Setting up virtual environments We recommend that you use the pipenv and/or virtualenv packages to manage your Python environments. A summary of how to get a virtual environment set up is contained in the below, but for further information, see: Pipenv and Virtual Environments Sometimes, you may need several different versions of the same package installed, for example, due to dependency issues. Virtual environments allow you to manage these conflicting requirements. The first step is to run the commands contained in the above section so that you can install the virtualenv package which will manage your environments. To install virtualenv run: pip install --user virtualenv # The --user flag indicates this should be installed in the user's package folder Next you must create a folder for the virtual environment's files to live in and tell virtualenv to set this folder up for storing virtual environment 'stuff'. This is done by running the command mkdir /work/t01/t01/auser/<<name of your virtual environment>> # Create the folder virtualenv -p /opt/cray/pe/python/3.8.5.0/bin/python /work/t01/t01/asuser/<<name of your virtual environment>> # -p flag means use this python interpreter Finally, you're ready to activate your environment. This is done by running source /work/t01/t01/auser/<<name of your virtual environment>>/bin/activate Once your environment is activated you will be able to install packages as usual using pip install <<package name>> . These packages will only be available within this environment. When running code that requires these packages you must activate the environment, by adding the above source ... activate line of code to any submission scripts. Running Python on the compute nodes In this section we provide example Python job submission scripts for a variety of scenarios of using Python on the ARCHER2 compute nodes. Example serial Python submission script Full system #!/bin/bash --login #SBATCH --job-name=python_test #SBATCH --nodes=1 #SBATCH --tasks-per-node=1 #SBATCH --cpus-per-task=1 #SBATCH --time=00:10:00 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Load the Python module module load cray-python # If using a virtual environment source <<path to virtual environment>>/bin/activate # Run your Python progamme python python_test.py Tip If you have installed your own packages you will need to set PATH and PYTHONPATH as described above within your job submission script in order to accesss the commands and packages you have installed. Example mpi4py job submission script Programs that have been parallelised with mpi4py can be run on multiple processors on ARCHER2. A sample submission script is given below. The primary difference from the Python submission script in the previous section is that we must run the program using srun python my_prog.py instead of python my_prog,py . Failing to do so will cause a segmentation fault in your program when it reaches the line \" from mpi4py import MPI \". Full system #!/bin/bash --login # Slurm job options (job-name, compute nodes, job time) #SBATCH --job-name=mpi4py_test #SBATCH --nodes=1 #SBATCH --tasks-per-node=2 #SBATCH --cpus-per-task=1 #SBATCH --time=0:10:0 # Replace [budget code] below with your budget code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Load the Python module module load cray-python # Run your Python programme # Note that srun MUST be used to wrap the call to python, otherwise an error # will occur srun python mpi4py_test.py Using JupyterLab on ARCHER2 It is possible to view and run Jupyter notebooks from both login nodes and compute nodes on ARCHER2. Note You can test such notebooks on the login nodes, but please do not attempt to run any computationally intensive work. Jobs may get killed once they hit a CPU limit on login nodes. Please follow these steps. Install JupyterLab in your work directory. module load cray-python export PYTHONUSERBASE=/work/t01/t01/auser/.local export PATH=$PYTHONUSERBASE/bin:$PATH # source <<path to virtual environment>>/bin/activate # If using a virtualenvironment uncomment this line and remove the --user flag from the next pip install --user jupyterlab If you want to test JupyterLab on the login node please go straight to step 3. To run your Jupyter notebook on a compute node, you first need to run an interactive session. srun --nodes=1 --exclusive --time=00:20:00 --account=<your_budget> \\ --partition=standard --qos=short --reservation=shortqos \\ --pty /bin/bash Your prompt will change to something like below. auser@nid001015:/tmp> In this case, the node id is nid001015 . Now execute the following on the compute node. cd /work/t01/t01/auser # Update the path to your work directory export PYTHONUSERBASE=$(pwd)/.local export PATH=$PYTHONUSERBASE/bin:$PATH export HOME=$(pwd) module load cray-python # source <<path to virtual environment>>/bin/activate # If using a virtualenvironment uncomment this line Run the JupyterLab server. export JUPYTER_RUNTIME_DIR=$(pwd) jupyter lab --ip=0.0.0.0 --no-browser Once it's started, you will see a URL printed in the terminal window of the form http://127.0.0.1:<port_number>/lab?token=<string> ; we'll need this URL for step 6. Please skip this step if you are connecting from a machine running Windows. Open a new terminal window on your laptop and run the following command. ssh <username>@login.archer2.ac.uk -L<port_number>:<node_id>:<port_number> where <username> is your username, and <node_id> is the id of the node you're currently on (for a login node, this will be uan01 , or similar; on a compute node, it will be a mix of numbers and letters). In our example, <node_id> is nid001015 . Note, please use the same port number as that shown in the URL of step 3. This number may vary, likely values are 8888 or 8889. Please skip this step if you are connecting from Linux or macOS. If you are connecting from Windows, you should use MobaXterm to configure an SSH tunnel as follows. Click on the Tunnelling button above the MobaXterm terminal. Create a new tunnel by clicking on New SSH tunnel in the window that opens. In the new window that opens, make sure the Local port forwarding radio button is selected. In the forwarded port text box on the left under My computer with MobaXterm , enter the port number indicated in the JupyterLab server output (e.g., 8888 or 8890). In the three text boxes on the bottom right under SSH server enter login.archer2.ac.uk , your ARCHER2 username and then 22 . At the top right, under Remote server , enter the id of the login or compute node running the JupyterLab server and the associated port number. Click on the Save button. In the tunnelling window, you will now see a new row for the settings you just entered. If you like, you can give a name to the tunnel in the leftmost column to identify it. Click on the small key icon close to the right for the new connection to tell MobaXterm which SSH private key to use when connecting to ARCHER2. You should tell it to use the same .ppk private key that you normally use when connecting to ARCHER2. The tunnel should now be configured. Click on the small start button (like a play '>' icon) for the new tunnel to open it. You'll be asked to enter your ARCHER2 account password -- please do so. Now, if you open a browser window locally, you should be able to navigate to the URL from step 3, and this should display the JupyterLab server. If JupyterLab is running on a compute node, the notebook will be available for the length of the interactive session you have requested. Warning Please do not use the other http address given by the JupyterLab output, the one formatted http://<node_id>:<port_number>/lab?token=<string> . Your local browser will not recognise the <node_id> part of the address. Using Dask Job-Queue on ARCHER2 The Dask-jobqueue project makes it easy to deploy Dask on ARCHER2. You can find more information in the Dask Job-Queue documentation . Please follow these steps: Install Dask-Jobqueue module load cray-python export PYTHONUSERBASE=/work/t01/t01/auser/.local export PATH=$PYTHONUSERBASE/bin:$PATH pip install --user dask-jobqueue --upgrade Using Dask Dask-jobqueue creates a Dask Scheduler in the Python process where the cluster object is instantiated. A script for running dask jobs on ARCHER2 might look something like this: from dask_jobqueue import SLURMCluster cluster = SLURMCluster(cores=128, processes=16, memory='256GB', queue='standard', header_skip=['--mem'], job_extra=['--qos=\"standard\"'], python='srun python', project='z19', walltime=\"01:00:00\", shebang=\"#!/bin/bash --login\", local_directory='$PWD' env_extra=['module load cray-python', 'export PYTHONUSERBASE=/work/t01/t01/auser/.local/', 'export PATH=$PYTHONUSERBASE/bin:$PATH', 'export PYTHONPATH=$PYTHONUSERBASE/lib/python3.8/site-packages:$PYTHONPATH']) cluster.scale(jobs=2) # Deploy two single-node jobs from dask.distributed import Client client = Client(cluster) # Connect this local process to remote workers # wait for jobs to arrive, depending on the queue, this may take some time import dask.array as da x = \u2026 # Dask commands now use these distributed resources This script can be run on the login nodes and it submits the Dask jobs to the job queue. Users should ensure that the computationally intensive work is done with the Dask commands which run on the compute nodes. The cluster object parameters specify the characteristics for running on a single compute node. The header_skip option is required as we are running on exclusive nodes where you should not specify the memory requirements, however Dask requires you to supply this option. Jobs are be deployed with the cluster.scale command, where the jobs option sets the number of single node jobs requested. Job scripts are generated (from the cluster object) and these are submitted to the queue to begin running once the resources are available. You can check the status of the jobs by running squeue -u $USER in a separate terminal. If you wish to see the generated job script you can use: print(cluster.job_script())","title":"Using Python"},{"location":"user-guide/python/#using-python","text":"Python is supported on ARCHER2 both for running intensive parallel jobs and also as an analysis tool. This section describes how to use Python in either of these scenarios. The Python installations on ARCHER2 contain some of the most commonly used modules. If you wish to install additional Python modules, we recommend that you use the pip command after loading the cray-python module. This is described in more detail below. Note When you log onto ARCHER2, no Python module is loaded by default. You will generally need to load the cray-python module to access the functionality described below. Running python without loading a module first will result in your using the operating system default Python which is likely not what you intend.","title":"Using Python"},{"location":"user-guide/python/#hpe-cray-python-distribution","text":"The recommended way to use Python on ARCHER2 is to use the HPE Cray Python distribution. The HPE Cray distribution provides Python 3 along with some of the most common packages used for scientific computation and data analysis. These include: numpy and scipy - built using GCC against HPE Cray LibSci mpi4py - built using GCC against HPE Cray MPICH dask The HPE Cray Python distribution can be loaded (either on the front-end or in a submission script) using: module load cray-python Tip The HPE Cray Python distribution provides Python 3. There is no Python 2 version as Python 2 is now deprecated. Tip The HPE Cray Python distribution is built using GCC compilers. If you wish to compile your own Python, C/C++ or Fortran code to use with HPE Cray Python, you should ensure that you compile using PrgEnv-gnu to make sure they are compatible.","title":"HPE Cray Python distribution"},{"location":"user-guide/python/#adding-your-own-packages","text":"If the packages you require are not included in the HPE Cray Python distribution, further packages can be added using pip . However, as the /home file systems are not available on the compute nodes, you will need to modify the default install location that pip uses to point to a location on the /work file systems (by default, pip installs into $HOME/.local ). To do this, you set the PYTHONUSERBASE environment variable to point to a location on /work, for example: export PYTHONUSERBASE=/work/t01/t01/auser/.local You will also need to ensure that: the location of commands installed by pip are available on the command line by modifying the PATH environment variable; any packages you install are available to Python by modifying the PYTHONPATH environment variable. You can do this in the following way (once you have set PYTHONUSERBASE as described above): export PATH=$PYTHONUSERBASE/bin:$PATH export PYTHONPATH=$PYTHONUSERBASE/lib/python3.8/site-packages:$PYTHONPATH We would recommend adding all three of these commands to your $HOME/.bashrc file to ensure they are set by default when you log in to ARCHER2. Once, you have done this, you can use pip to add packages on top of the HPE Cray Python environment. This can be done using: module load cray-python pip install --user <package_name> This uses the --user flag to ensure the packages are installed in the directory specified by PYTHONUSERBASE .","title":"Adding your own packages"},{"location":"user-guide/python/#setting-up-virtual-environments","text":"We recommend that you use the pipenv and/or virtualenv packages to manage your Python environments. A summary of how to get a virtual environment set up is contained in the below, but for further information, see: Pipenv and Virtual Environments Sometimes, you may need several different versions of the same package installed, for example, due to dependency issues. Virtual environments allow you to manage these conflicting requirements. The first step is to run the commands contained in the above section so that you can install the virtualenv package which will manage your environments. To install virtualenv run: pip install --user virtualenv # The --user flag indicates this should be installed in the user's package folder Next you must create a folder for the virtual environment's files to live in and tell virtualenv to set this folder up for storing virtual environment 'stuff'. This is done by running the command mkdir /work/t01/t01/auser/<<name of your virtual environment>> # Create the folder virtualenv -p /opt/cray/pe/python/3.8.5.0/bin/python /work/t01/t01/asuser/<<name of your virtual environment>> # -p flag means use this python interpreter Finally, you're ready to activate your environment. This is done by running source /work/t01/t01/auser/<<name of your virtual environment>>/bin/activate Once your environment is activated you will be able to install packages as usual using pip install <<package name>> . These packages will only be available within this environment. When running code that requires these packages you must activate the environment, by adding the above source ... activate line of code to any submission scripts.","title":"Setting up virtual environments"},{"location":"user-guide/python/#running-python-on-the-compute-nodes","text":"In this section we provide example Python job submission scripts for a variety of scenarios of using Python on the ARCHER2 compute nodes.","title":"Running Python on the compute nodes"},{"location":"user-guide/python/#example-serial-python-submission-script","text":"Full system #!/bin/bash --login #SBATCH --job-name=python_test #SBATCH --nodes=1 #SBATCH --tasks-per-node=1 #SBATCH --cpus-per-task=1 #SBATCH --time=00:10:00 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Load the Python module module load cray-python # If using a virtual environment source <<path to virtual environment>>/bin/activate # Run your Python progamme python python_test.py Tip If you have installed your own packages you will need to set PATH and PYTHONPATH as described above within your job submission script in order to accesss the commands and packages you have installed.","title":"Example serial Python submission script"},{"location":"user-guide/python/#example-mpi4py-job-submission-script","text":"Programs that have been parallelised with mpi4py can be run on multiple processors on ARCHER2. A sample submission script is given below. The primary difference from the Python submission script in the previous section is that we must run the program using srun python my_prog.py instead of python my_prog,py . Failing to do so will cause a segmentation fault in your program when it reaches the line \" from mpi4py import MPI \". Full system #!/bin/bash --login # Slurm job options (job-name, compute nodes, job time) #SBATCH --job-name=mpi4py_test #SBATCH --nodes=1 #SBATCH --tasks-per-node=2 #SBATCH --cpus-per-task=1 #SBATCH --time=0:10:0 # Replace [budget code] below with your budget code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Load the Python module module load cray-python # Run your Python programme # Note that srun MUST be used to wrap the call to python, otherwise an error # will occur srun python mpi4py_test.py","title":"Example mpi4py job submission script"},{"location":"user-guide/python/#using-jupyterlab-on-archer2","text":"It is possible to view and run Jupyter notebooks from both login nodes and compute nodes on ARCHER2. Note You can test such notebooks on the login nodes, but please do not attempt to run any computationally intensive work. Jobs may get killed once they hit a CPU limit on login nodes. Please follow these steps. Install JupyterLab in your work directory. module load cray-python export PYTHONUSERBASE=/work/t01/t01/auser/.local export PATH=$PYTHONUSERBASE/bin:$PATH # source <<path to virtual environment>>/bin/activate # If using a virtualenvironment uncomment this line and remove the --user flag from the next pip install --user jupyterlab If you want to test JupyterLab on the login node please go straight to step 3. To run your Jupyter notebook on a compute node, you first need to run an interactive session. srun --nodes=1 --exclusive --time=00:20:00 --account=<your_budget> \\ --partition=standard --qos=short --reservation=shortqos \\ --pty /bin/bash Your prompt will change to something like below. auser@nid001015:/tmp> In this case, the node id is nid001015 . Now execute the following on the compute node. cd /work/t01/t01/auser # Update the path to your work directory export PYTHONUSERBASE=$(pwd)/.local export PATH=$PYTHONUSERBASE/bin:$PATH export HOME=$(pwd) module load cray-python # source <<path to virtual environment>>/bin/activate # If using a virtualenvironment uncomment this line Run the JupyterLab server. export JUPYTER_RUNTIME_DIR=$(pwd) jupyter lab --ip=0.0.0.0 --no-browser Once it's started, you will see a URL printed in the terminal window of the form http://127.0.0.1:<port_number>/lab?token=<string> ; we'll need this URL for step 6. Please skip this step if you are connecting from a machine running Windows. Open a new terminal window on your laptop and run the following command. ssh <username>@login.archer2.ac.uk -L<port_number>:<node_id>:<port_number> where <username> is your username, and <node_id> is the id of the node you're currently on (for a login node, this will be uan01 , or similar; on a compute node, it will be a mix of numbers and letters). In our example, <node_id> is nid001015 . Note, please use the same port number as that shown in the URL of step 3. This number may vary, likely values are 8888 or 8889. Please skip this step if you are connecting from Linux or macOS. If you are connecting from Windows, you should use MobaXterm to configure an SSH tunnel as follows. Click on the Tunnelling button above the MobaXterm terminal. Create a new tunnel by clicking on New SSH tunnel in the window that opens. In the new window that opens, make sure the Local port forwarding radio button is selected. In the forwarded port text box on the left under My computer with MobaXterm , enter the port number indicated in the JupyterLab server output (e.g., 8888 or 8890). In the three text boxes on the bottom right under SSH server enter login.archer2.ac.uk , your ARCHER2 username and then 22 . At the top right, under Remote server , enter the id of the login or compute node running the JupyterLab server and the associated port number. Click on the Save button. In the tunnelling window, you will now see a new row for the settings you just entered. If you like, you can give a name to the tunnel in the leftmost column to identify it. Click on the small key icon close to the right for the new connection to tell MobaXterm which SSH private key to use when connecting to ARCHER2. You should tell it to use the same .ppk private key that you normally use when connecting to ARCHER2. The tunnel should now be configured. Click on the small start button (like a play '>' icon) for the new tunnel to open it. You'll be asked to enter your ARCHER2 account password -- please do so. Now, if you open a browser window locally, you should be able to navigate to the URL from step 3, and this should display the JupyterLab server. If JupyterLab is running on a compute node, the notebook will be available for the length of the interactive session you have requested. Warning Please do not use the other http address given by the JupyterLab output, the one formatted http://<node_id>:<port_number>/lab?token=<string> . Your local browser will not recognise the <node_id> part of the address.","title":"Using JupyterLab on ARCHER2"},{"location":"user-guide/python/#using-dask-job-queue-on-archer2","text":"The Dask-jobqueue project makes it easy to deploy Dask on ARCHER2. You can find more information in the Dask Job-Queue documentation . Please follow these steps: Install Dask-Jobqueue module load cray-python export PYTHONUSERBASE=/work/t01/t01/auser/.local export PATH=$PYTHONUSERBASE/bin:$PATH pip install --user dask-jobqueue --upgrade Using Dask Dask-jobqueue creates a Dask Scheduler in the Python process where the cluster object is instantiated. A script for running dask jobs on ARCHER2 might look something like this: from dask_jobqueue import SLURMCluster cluster = SLURMCluster(cores=128, processes=16, memory='256GB', queue='standard', header_skip=['--mem'], job_extra=['--qos=\"standard\"'], python='srun python', project='z19', walltime=\"01:00:00\", shebang=\"#!/bin/bash --login\", local_directory='$PWD' env_extra=['module load cray-python', 'export PYTHONUSERBASE=/work/t01/t01/auser/.local/', 'export PATH=$PYTHONUSERBASE/bin:$PATH', 'export PYTHONPATH=$PYTHONUSERBASE/lib/python3.8/site-packages:$PYTHONPATH']) cluster.scale(jobs=2) # Deploy two single-node jobs from dask.distributed import Client client = Client(cluster) # Connect this local process to remote workers # wait for jobs to arrive, depending on the queue, this may take some time import dask.array as da x = \u2026 # Dask commands now use these distributed resources This script can be run on the login nodes and it submits the Dask jobs to the job queue. Users should ensure that the computationally intensive work is done with the Dask commands which run on the compute nodes. The cluster object parameters specify the characteristics for running on a single compute node. The header_skip option is required as we are running on exclusive nodes where you should not specify the memory requirements, however Dask requires you to supply this option. Jobs are be deployed with the cluster.scale command, where the jobs option sets the number of single node jobs requested. Job scripts are generated (from the cluster object) and these are submitted to the queue to begin running once the resources are available. You can check the status of the jobs by running squeue -u $USER in a separate terminal. If you wish to see the generated job script you can use: print(cluster.job_script())","title":"Using Dask Job-Queue on ARCHER2"},{"location":"user-guide/scheduler/","text":"Running jobs on ARCHER2 As with most HPC services, ARCHER2 uses a scheduler to manage access to resources and ensure that the thousands of different users of system are able to share the system and all get access to the resources they require. ARCHER2 uses the Slurm software to schedule jobs. Writing a submission script is typically the most convenient way to submit your job to the scheduler. Example submission scripts (with explanations) for the most common job types are provided below. Interactive jobs are also available and can be particularly useful for developing and debugging applications. More details are available below. Hint If you have any questions on how to run jobs on ARCHER2 do not hesitate to contact the ARCHER2 Service Desk . You typically interact with Slurm by issuing Slurm commands from the login nodes (to submit, check and cancel jobs), and by specifying Slurm directives that describe the resources required for your jobs in job submission scripts. Resources CUs Time used on ARCHER2 is measured in CUs. 1 CU = 1 Node Hour for a standard 128 core node. The CU calculator will help you to calculate the CU cost for your jobs. Checking available budget You can check in SAFE by selecting Login accounts from the menu, select the login account you want to query. Under Login account details you will see each of the budget codes you have access to listed e.g. e123 resources and then under Resource Pool to the right of this, a note of the remaining budget in CUs. When logged in to the machine you can also use the command sacctmgr show assoc where user=$LOGNAME format=account,user,maxtresmins This will list all the budget codes that you have access to e.g. Account User MaxTRESMins ---------- ---------- ------------- e123 userx cpu=0 e123-test userx This shows that userx is a member of budgets e123 and e123-test . However, the cpu=0 indicates that the e123 budget is empty or disabled. This user can submit jobs using the e123-test budget. To see the number of CUs remaining you must check in SAFE . Charging Jobs run on ARCHER2 are charged for the time they use i.e. from the time the job begins to run until the time the job ends (not the full wall time requested). Jobs are charged for the full number of nodes which are requested, even if they are not all used. Charging takes place at the time the job ends, and the job is charged in full to the budget which is live at the end time. Basic Slurm commands There are four key commands used to interact with the Slurm on the command line: sinfo - Get information on the partitions and resources available sbatch jobscript.slurm - Submit a job submission script (in this case called: jobscript.slurm ) to the scheduler squeue - Get the current status of jobs submitted to the scheduler scancel 12345 - Cancel a job (in this case with the job ID 12345 ) We cover each of these commands in more detail below. sinfo : information on resources sinfo is used to query information about available resources and partitions. Without any options, sinfo lists the status of all resources and partitions, e.g. auser@ln01:~> sinfo PARTITION AVAIL TIMELIMIT NODES STATE NODELIST standard up 1 -00:00:00 105 down* nid [ 001006 ,...,002014 ] standard up 1 -00:00:00 12 drain nid [ 001016 ,...,001969 ] standard up 1 -00:00:00 5 resv nid [ 001000 ,001002-001004,001114 ] standard up 1 -00:00:00 683 alloc nid [ 001001 ,...,001970-001991 ] standard up 1 -00:00:00 214 idle nid [ 001022 -001023,...,002015-002023 ] standard up 1 -00:00:00 2 down nid [ 001021 ,001050 ] Here we see the number of nodes in different states. For example, 683 nodes are allocated (running jobs), and 214 are idle (available to run jobs). Note that long lists of node IDs have been abbreviated with ... . sbatch : submitting jobs sbatch is used to submit a job script to the job submission system. The script will typically contain one or more srun commands to launch parallel tasks. When you submit the job, the scheduler provides the job ID, which is used to identify this job in other Slurm commands and when looking at resource usage in SAFE. auser@ln01:~> sbatch test-job.slurm Submitted batch job 12345 squeue : monitoring jobs squeue without any options or arguments shows the current status of all jobs known to the scheduler. For example: auser@ln01:~> squeue will list all jobs on ARCHER2. The output of this is often overwhelmingly large. You can restrict the output to just your jobs by adding the -u $USER option: auser@ln01:~> squeue -u $USER scancel : deleting jobs scancel is used to delete a jobs from the scheduler. If the job is waiting to run it is simply cancelled, if it is a running job then it is stopped immediately. If you only want to cancel a specific job you need to provide the job ID of the job you wish to cancel/stop. For example: auser@ln01:~> scancel 12345 will cancel (if waiting) or stop (if running) the job with ID 12345 . scancel can take other options. For example, if you want to cancel all your pending (queued) jobs but leave the running jobs running, you could use: auser@ln01:~> scancel --state = PENDING --user = $USER Resource Limits The ARCHER2 resource limits for any given job are covered by three separate attributes. The amount of primary resource you require, i.e., number of compute nodes. The partition that you want to use - this specifies the nodes that are eligible to run your job. The Quality of Service (QoS) that you want to use - this specifies the job limits that apply. Primary resource The primary resource you can request for your job is the compute node. Information The --exclusive option is enforced on ARCHER2 which means you will always have access to all of the memory on the compute node regardless of how many processes are actually running on the node. Note You will not generally have access to the full amount of memory resource on the the node as some is retained for running the operating system and other system processes. Partitions On ARCHER2, compute nodes are grouped into partitions. You will have to specify a partition using the --partition option in your Slurm submission script. The following table has a list of active partitions on ARCHER2. Full system Partition Description Max nodes available standard CPU nodes with AMD EPYC 7742 64-core processor \u00d7 2, 256/512 GB memory 5860 highmem CPU nodes with AMD EPYC 7742 64-core processor \u00d7 2, 512 GB memory 584 serial CPU nodes with AMD EPYC 7742 64-core processor \u00d7 2, 512 GB memory 2 Note The standard partition includes both the standard memory and high memory nodes but standard memory nodes are preferentially chosen for jobs where possible. To guarantee access to high memory nodes you should specify the highmem partition. Quality of Service (QoS) On ARCHER2, job limits are defined by the requested Quality of Service (QoS), as specified by the --qos Slurm directive. The following table lists the active QoS on ARCHER2. Full system QoS Max Nodes Per Job Max Walltime Jobs Queued Jobs Running Partition(s) Notes standard 1024 24 hrs 64 16 standard Maximum of 1024 nodes in use by any one user at any time highmem 256 24 hrs 16 16 highmem Maximum of 512 nodes in use by any one user at any time taskfarm 16 24 hrs 128 32 standard Maximum of 256 nodes in use by any one user at any time short 32 20 mins 16 4 standard long 64 48 hrs 16 16 standard Minimum walltime of 24 hrs, maximum 512 nodes in use by any one user at any time, maximum of 2048 nodes in use by QoS largescale 5860 12 hrs 8 1 standard Minimum job size of 1025 nodes lowpriority 1024 24 hrs 16 16 standard Jobs not charged but requires at least 1 CU in budget to use. serial 32 cores and/or 128 GB memory 24 hrs 32 4 serial Jobs not charged but requires at least 1 CU in budget to use. Maximum of 32 cores and/or 128 GB in use by any one user at any time. reservation Size of reservation Length of reservation No limit no limit standard You can find out the QoS that you can use by running the following command: Full system auser@ln01:~> sacctmgr show assoc user = $USER cluster = archer2 format = cluster,account,user,qos%50 Hint If you have needs which do not fit within the current QoS, please contact the Service Desk and we can discuss how to accommodate your requirements. E-mail notifications E-mail notifications from the scheduler are not currently available on ARCHER2. Priority Job priority on ARCHER2 depends on a number of different factors: The QoS your job has specified The amount of time you have been queuing for The number of nodes you have requested (job size) Your current fairshare factor Each of these factors is normalised to a value between 0 and 1, is multiplied with a weight and the resulting values combined to produce a priority for the job. The current job priority formula on Tursa is: Priority = [10000 * P(QoS)] + [500 * P(Age)] + [300 * P(Fairshare)] + [100 * P(size)] The priority factors are: P(QoS) - The QoS priority normalised to a value between 0 and 1. The maximum raw value is 10000 and the minimum is 0. Most QoS on ARCHER2 have a raw prioity of 500; the lowpriority QoS has a raw priority of 1. P(Age) - The priority based on the job age normalised to a value between 0 and 1. The maximum raw value is 14 days (where P(Age) = 1). P(Fairshare) - The fairshare priority normalised to a value between 0 and 1. Your fairshare priority is determined by a combination of your budget code fairshare value and your user fairshare value within that budget code. The more use that the budget code you are using has made of the system recently relative to other budget codes on the system, the lower the budget code fairshare value will be; and the more use you have made of the system recently relative to other users within your budget code, the lower your user fairshare value will be. The decay half life for fairshare on ARCHER2 is set to 14 days. More information on the Slurm fairshare algorithm . P(Size) - The priority based on the job size normalised to a value between 0 and 1. The maximum size is the total number of ARCHER2 compute nodes. You can view the priorities for current queued jobs on the system with the sprio command: auser@ln04:~> sprio -l JOBID PARTITION PRIORITY SITE AGE FAIRSHARE JOBSIZE QOS 828764 standard 1049 0 45 0 4 1000 828765 standard 1049 0 45 0 4 1000 828770 standard 1049 0 45 0 4 1000 828771 standard 1012 0 8 0 4 1000 828773 standard 1012 0 8 0 4 1000 828791 standard 1012 0 8 0 4 1000 828797 standard 1118 0 115 0 4 1000 828800 standard 1154 0 150 0 4 1000 828801 standard 1154 0 150 0 4 1000 828805 standard 1118 0 115 0 4 1000 828806 standard 1154 0 150 0 4 1000 Troubleshooting Slurm error messages An incorrect submission will cause Slurm to return an error. Some common problems are listed below, with a suggestion about the likely cause: sbatch: unrecognized option <text> One of your options is invalid or has a typo. man sbatch to help. error: Batch job submission failed: No partition specified or system default partition A --partition= option is missing. You must specify the partition (see the list above). This is most often --partition=standard . error: invalid partition specified: <partition> error: Batch job submission failed: Invalid partition name specified Check the partition exists and check the spelling is correct. error: Batch job submission failed: Invalid account or account/partition combination specified This probably means an invalid account has been given. Check the --account= options against valid accounts in SAFE. error: Batch job submission failed: Invalid qos specification A QoS option is either missing or invalid. Check the script has a --qos= option and that the option is a valid one from the table above. (Check the spelling of the QoS is correct.) error: Your job has no time specification (--time=)... Add an option of the form --time=hours:minutes:seconds to the submission script. E.g., --time=01:30:00 gives a time limit of 90 minutes. error: QOSMaxWallDurationPerJobLimit error: Batch job submission failed: Job violates accounting/QOS policy (job submit limit, user's size and/or time limits) The script has probably specified a time limit which is too long for the corresponding QoS. E.g., the time limit for the short QoS is 20 minutes. Slurm job state codes The squeue command allows users to view information for jobs managed by Slurm. Jobs typically go through the following states: PENDING, RUNNING, COMPLETING, and COMPLETED. The first table provides a description of some job state codes. The second table provides a description of the reasons that cause a job to be in a state. Status Code Description PENDING PD Job is awaiting resource allocation. RUNNING R Job currently has an allocation. SUSPENDED S Job currently has an allocation. COMPLETING CG Job is in the process of completing. Some processes on some nodes may still be active. COMPLETED CD Job has terminated all processes on all nodes with an exit code of zero. TIMEOUT TO Job terminated upon reaching its time limit. STOPPED ST Job has an allocation, but execution has been stopped with SIGSTOP signal. CPUS have been retained by this job. OUT_OF_MEMORY OOM Job experienced out of memory error. FAILED F Job terminated with non-zero exit code or other failure condition. NODE_FAIL NF Job terminated due to failure of one or more allocated nodes. CANCELLED CA Job was explicitly cancelled by the user or system administrator. The job may or may not have been initiated. For a full list of see Job State Codes . Slurm queued reasons Reason Description Priority One or more higher priority jobs exist for this partition or advanced reservation. Resources The job is waiting for resources to become available. BadConstraints The job's constraints can not be satisfied. BeginTime The job's earliest start time has not yet been reached. Dependency This job is waiting for a dependent job to complete. Licenses The job is waiting for a license. WaitingForScheduling No reason has been set for this job yet. Waiting for the scheduler to determine the appropriate reason. Prolog Its PrologSlurmctld program is still running. JobHeldAdmin The job is held by a system administrator. JobHeldUser The job is held by the user. JobLaunchFailure The job could not be launched. This may be due to a file system problem, invalid program name, etc. NonZeroExitCode The job terminated with a non-zero exit code. InvalidAccount The job's account is invalid. InvalidQOS The job's QOS is invalid. QOSUsageThreshold Required QOS threshold has been breached. QOSJobLimit The job's QOS has reached its maximum job count. QOSResourceLimit The job's QOS has reached some resource limit. QOSTimeLimit The job's QOS has reached its time limit. NodeDown A node required by the job is down. TimeLimit The job exhausted its time limit. ReqNodeNotAvail Some node specifically required by the job is not currently available. The node may currently be in use, reserved for another job, in an advanced reservation, DOWN, DRAINED, or not responding. Nodes which are DOWN, DRAINED, or not responding will be identified as part of the job's \"reason\" field as \"UnavailableNodes\". Such nodes will typically require the intervention of a system administrator to make available. For a full list of see Job Reasons . Output from Slurm jobs Slurm places standard output (STDOUT) and standard error (STDERR) for each job in the file slurm_<JobID>.out . This file appears in the job's working directory once your job starts running. Hint Output may be buffered - to enable live output, e.g. for monitoring job status, add --unbuffered to the srun command in your Slurm script. Specifying resources in job scripts You specify the resources you require for your job using directives at the top of your job submission script using lines that start with the directive #SBATCH . Hint Most options provided using #SBATCH directives can also be specified as command line options to srun . If you do not specify any options, then the default for each option will be applied. As a minimum, all job submissions must specify the budget that they wish to charge the job too with the option: --account=<budgetID> your budget ID is usually something like t01 or t01-test . You can see which budget codes you can charge to in SAFE. Other common options that are used are: --time=<hh:mm:ss> the maximum walltime for your job. e.g. For a 6.5 hour walltime, you would use --time=6:30:0 . --job-name=<jobname> set a name for the job to help identify it in the queue To prevent the behaviour of batch scripts being dependent on the user environment at the point of submission, the option --export=none prevents the user environment from being exported to the batch system. Using the --export=none means that the behaviour of batch submissions should be repeatable. We strongly recommend its use. Additional options for parallel jobs In addition, parallel jobs will also need to specify how many nodes, parallel processes and threads they require. --nodes=<nodes> the number of nodes to use for the job. --tasks-per-node=<processes per node> the number of parallel processes (e.g. MPI ranks) per node. --cpus-per-task=1 if you are using parallel processes only with no threading then you should set the number of CPUs (cores) per parallel process to 1. Important: if you are using threading (e.g. with OpenMP) then you will need to change this option as described below. For parallel jobs that use threading (e.g. OpenMP), you will also need to change the --cpus-per-task option. --cpus-per-task=<threads per task> the number of threads per parallel process (e.g. number of OpenMP threads per MPI task for hybrid MPI/OpenMP jobs). Important: you must also set the OMP_NUM_THREADS environment variable if using OpenMP in your job. Note For parallel jobs, ARCHER2 operates in a node exclusive way. This means that you are assigned resources in the units of full compute nodes for your jobs ( i.e. 128 cores) and that no other user can share those compute nodes with you. Hence, the minimum amount of resource you can request for a parallel job is 1 node (or 128 cores). Options for jobs on the data analysis nodes The data analysis nodes are shared between all users and can be used to run jobs that require small numbers of cores and/or access to an external network to transfer data. These jobs are often serial jobs that only require a single core. To run jobs on the data analysis node you require the following options: --partition=serial to select the data analysis nodes --qos=serial to select the data analysis QoS (see above for QoS limits) --ntasks=<number of cores> to select the number of cores you want to use in this job (up to the maximum defined in the QoS) --mem=<amount of memory> to select the amount of memory you require (up to the maximum defined in the QoS). More information on using the data analysis nodes (including example job submission scripts) can be found in the Data Analysis section of the User and Best Practice Guide. srun : Launching parallel jobs If you are running parallel jobs, your job submission script should contain one or more srun commands to launch the parallel executable across the compute nodes. In most cases you will want to add the options --distribution=block:block and --hint=nomultithread to your srun command to ensure you get the correct pinning of processes to cores on a compute node. Warning If you do not add the --distribution=block:block and --hint=nomultithread options to your srun command the default process placement may lead to a drop in performance for your jobs on ARCHER2. A brief explanation of these options: - --hint=nomultithread - do not use hyperthreads/SMP - --distribution=block:block - the first block means use a block distribution of processes across nodes (i.e. fill nodes before moving onto the next one) and the second block means use a block distribution of processes across NUMA regions within a node (i.e. fill a NUMA region before moving on to the next one). bolt: Job submission script creation tool The bolt job submission script creation tool has been written by EPCC to simplify the process of writing job submission scripts for modern multicore architectures. Based on the options you supply, bolt will generate a job submission script that uses ARCHER2 in a reasonable way. MPI, OpenMP and hybrid MPI/OpenMP jobs are supported. Warning The tool will allow you to generate scripts for jobs that use the long QoS but you will need to manually modify the resulting script to change the QoS to long . If there are problems or errors in your job parameter specifications then bolt will print warnings or errors. However, bolt cannot detect all problems. Basic Usage The basic syntax for using bolt is: bolt -n [parallel tasks] -N [parallel tasks per node] -d [number of threads per task] \\ -t [wallclock time (h:m:s)] -o [script name] -j [job name] -A [project code] [arguments...] Example 1: to generate a job script to run an executable called my_prog.x for 24 hours using 8192 parallel (MPI) processes and 128 (MPI) processes per compute node you would use something like: bolt -n 8192 -N 128 -t 24:0:0 -o my_job.bolt -j my_job -A z01-budget my_prog.x arg1 arg2 (remember to substitute z01-budget for your actual budget code.) Example 2: to generate a job script to run an executable called my_prog.x for 3 hours using 2048 parallel (MPI) processes and 64 (MPI) processes per compute node (i.e. using half of the cores on a compute node), you would use: bolt -n 2048 -N 64 -t 3:0:0 -o my_job.bolt -j my_job -A z01-budget my_prog.x arg1 arg2 These examples generate the job script my_job.bolt with the correct options to run my_prog.x with command line arguments arg1 and arg2 . The project code against which the job will be charged is specified with the ' -A ' option. As usual, the job script is submitted as follows: sbatch my_job.bolt Hint If you do not specify the script name with the '-o' option then your script will be a file called a.bolt . Hint If you do not specify the number of parallel tasks then bolt will try to generate a serial job submission script (and throw an error on the ARCHER2 4 cabinet system as serial jobs are not supported). Hint If you do not specify a project code, bolt will use your default project code (set by your login account). Hint If you do not specify a job name, bolt will use either bolt_ser_job (for serial jobs) or bolt_par_job (for parallel jobs). Further help You can access further help on using bolt on ARCHER2 with the ' -h ' option: bolt -h A selection of other useful options are: -s Write and submit the job script rather than just writing the job script. -p Force the job to be parallel even if it only uses a single parallel task. checkScript job submission script validation tool The checkScript tool has been written to allow users to validate their job submission scripts before submitting their jobs. The tool will read your job submission script and try to identify errors, problems or inconsistencies. An example of the sort of output the tool can give would be: auser@ln01:/work/t01/t01/auser> checkScript submit.slurm =========================================================================== checkScript --------------------------------------------------------------------------- Copyright 2011-2020 EPCC, The University of Edinburgh This program comes with ABSOLUTELY NO WARRANTY. This is free software, and you are welcome to redistribute it under certain conditions. =========================================================================== Script details --------------- User: auser Script file: submit.slurm Directory: /work/t01/t01/auser (ok) Job name: test (ok) Partition: standard (ok) QoS: standard (ok) Combination: (ok) Requested resources ------------------- nodes = 3 (ok) tasks per node = 16 cpus per task = 8 cores per node = 128 (ok) OpenMP defined = True (ok) walltime = 1:0:0 (ok) CU Usage Estimate (if full job time used) ------------------------------------------ CU = 3.000 checkScript finished: 0 warning(s) and 0 error(s). Checking scripts and estimating start time with --test-only sbatch --test-only validates the batch script and returns an estimate of when the job would be scheduled to run given the current job queue. Please note that it is just an estimate, the actual start time may differ as the job queue status when the start time was estimated may be different from the moment that the estimation took place. The job is not actually submitted. auser@ln01:~> sbatch --test-only submit.slurm sbatch: Job 1039497 to start at 2022-02-01T23:20:51 using 256 processors on nodes nid002836 in partition standard Example job submission scripts A subset of example job submission scripts are included in full below. Examples are provided for both the full system and the 4-cabinet system. Example: job submission script for MPI parallel job A simple MPI job submission script to submit a job using 4 compute nodes and 128 MPI ranks per node for 20 minutes would look like: Full system #!/bin/bash # Slurm job options (job-name, compute nodes, job time) #SBATCH --job-name=Example_MPI_Job #SBATCH --time=0:20:0 #SBATCH --nodes=4 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 # Replace [budget code] below with your budget code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Set the number of threads to 1 # This prevents any threaded system libraries from automatically # using threading. export OMP_NUM_THREADS = 1 # Launch the parallel job # Using 512 MPI processes and 128 MPI processes per node # srun picks up the distribution from the sbatch options srun --distribution = block:block --hint = nomultithread ./my_mpi_executable.x This will run your executable \"my_mpi_executable.x\" in parallel on 512 MPI processes using 4 nodes (128 cores per node, i.e. not using hyper-threading). Slurm will allocate 4 nodes to your job and srun will place 128 MPI processes on each node (one per physical core). See above for a more detailed discussion of the different sbatch options Example: job submission script for MPI+OpenMP (mixed mode) parallel job Mixed mode codes that use both MPI (or another distributed memory parallel model) and OpenMP should take care to ensure that the shared memory portion of the process/thread placement does not span more than one NUMA region. Nodes on ARCHER2 are made up of two sockets each containing 4 NUMA regions of 16 cores, i.e. there are 8 NUMA regions in total. Therefore the total number of threads should ideally not be greater than 16, and also needs to be a factor of 16. Sensible choices for the number of threads are therefore 1 (single-threaded), 2, 4, 8, and 16. More information about using OpenMP and MPI+OpenMP can be found in the Tuning chapter. To ensure correct placement of MPI processes the number of cpus-per-task needs to match the number of OpenMP threads, and the number of tasks-per-node should be set to ensure the entire node is filled with MPI tasks. In the example below, we are using 4 nodes for 6 hours. There are 32 MPI processes in total (8 MPI processes per node) and 16 OpenMP threads per MPI process. This results in all 128 physical cores per node being used. Hint Note the use of the export OMP_PLACES=cores environment option to generate the correct thread pinning. Full system #!/bin/bash # Slurm job options (job-name, compute nodes, job time) #SBATCH --job-name=Example_MPI_Job #SBATCH --time=0:20:0 #SBATCH --nodes=4 #SBATCH --tasks-per-node=8 #SBATCH --cpus-per-task=16 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Set the number of threads to 16 and specify placement # There are 16 OpenMP threads per MPI process # We want one thread per physical core export OMP_NUM_THREADS = 16 export OMP_PLACES = cores # Launch the parallel job # Using 32 MPI processes # 8 MPI processes per node # 16 OpenMP threads per MPI process # Additional srun options to pin one thread per physical core srun --hint = nomultithread --distribution = block:block ./my_mixed_executable.x arg1 arg2 Job arrays The Slurm job scheduling system offers the job array concept, for running collections of almost-identical jobs. For example, running the same program several times with different arguments or input data. Each job in a job array is called a subjob . The subjobs of a job array can be submitted and queried as a unit, making it easier and cleaner to handle the full set, compared to individual jobs. All subjobs in a job array are started by running the same job script. The job script also contains information on the number of jobs to be started, and Slurm provides a subjob index which can be passed to the individual subjobs or used to select the input data per subjob. Job script for a job array As an example, the following script runs 56 subjobs, with the subjob index as the only argument to the executable. Each subjob requests a single node and uses all 128 cores on the node by placing 1 MPI process per core and specifies 4 hours maximum runtime per subjob: Full system #!/bin/bash # Slurm job options (job-name, compute nodes, job time) #SBATCH --job-name=Example_Array_Job #SBATCH --time=04:00:00 #SBATCH --nodes=1 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 #SBATCH --array=0-55 # Replace [budget code] below with your budget code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Set the number of threads to 1 # This prevents any threaded system libraries from automatically # using threading. export OMP_NUM_THREADS = 1 srun --distribution = block:block --hint = nomultithread /path/to/exe $SLURM_ARRAY_TASK_ID Submitting a job array Job arrays are submitted using sbatch in the same way as for standard jobs: sbatch job_script.pbs Job chaining Job dependencies can be used to construct complex pipelines or chain together long simulations requiring multiple steps. Hint The --parsable option to sbatch can simplify working with job dependencies. It returns the job ID in a format that can be used as the input to other commands. For example: jobid=$(sbatch --parsable first_job.sh) sbatch --dependency=afterok:$jobid second_job.sh or for a longer chain: jobid1=$(sbatch --parsable first_job.sh) jobid2=$(sbatch --parsable --dependency=afterok:$jobid1 second_job.sh) jobid3=$(sbatch --parsable --dependency=afterok:$jobid1 third_job.sh) sbatch --dependency=afterok:$jobid2,afterok:$jobid3 last_job.sh Using multiple srun commands in a single job script You can use multiple srun commands within in a Slurm job submission script to allow you to use the resource requested more flexibly. For example, you could run a collection of smaller jobs within the requested resources or you could even subdivide nodes if your individual calculations do not scale up to use all 128 cores on a node. In this guide we will cover two scenarios: Subdividing the job into multiple full-node or multi-node subjobs, e.g. requesting 100 nodes and running 100, 1-node subjobs or 50, 2-node subjobs. Subdividing the job into multiple subjobs that each use a fraction of a node, e.g. requesting 2 nodes and running 256, 1-core subjobs or 16, 16-core subjobs. Running multiple, full-node subjobs within a larger job When subdivding a larger job into smaller subjobs you typically need to overwrite the --nodes option to srun and add the --ntasks option to ensure that each subjob runs on the correct number of nodes and that subjobs are placed correctly onto separate nodes. For example, we will show how to request 100 nodes and then run 100 separate 1-node jobs, each of which use 128 MPI processes and which run on a different compute node. We start by showing the job script that would achieve this and then explain how this works and the options used. In our case, we will run 100 copies of the xthi program that prints the process placement on the node it is running on. Full system #!/bin/bash # Slurm job options (job-name, compute nodes, job time) #SBATCH --job-name=multi_xthi #SBATCH --time=0:20:0 #SBATCH --nodes=100 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 # Replace [budget code] below with your budget code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Load the xthi module module load xthi # Set the number of threads to 1 # This prevents any threaded system libraries from automatically # using threading. export OMP_NUM_THREADS = 1 # Loop over 100 subjobs starting each of them on a separate node for i in $( seq 1 100 ) do # Launch this subjob on 1 node, note nodes and ntasks options and & to place subjob in the background srun --nodes = 1 --ntasks = 128 --distribution = block:block --hint = nomultithread xthi > placement ${ i } .txt & done # Wait for all background subjobs to finish wait Key points from the example job script: The #SBATCH options select 100 full nodes in the usual way. Each subjob srun command sets the following: --nodes=1 We need override this setting from the main job so that each subjob only uses 1 node --ntasks=128 For normal jobs, the number of parallel tasks (MPI processes) is calculated from the number of nodes you request and the number of tasks per node. We need to explicitly tell srun how many we require for this subjob. --distribution=block:block --hint=nomultithread These options ensure correct placement of processes within the compute nodes. & Each subjob srun command ends with an ampersand to place the process in the background and move on to the next loop iteration (and subjob submission). Without this, the script would wait for this subjob to complete before moving on to submit the next. Finally, there is the wait command to tell the script to wait for all the background subjobs to complete before exiting. If we did not have this in place, the script would exit as soon as the last subjob was submitted and kill all running subjobs. Running multiple subjobs that each use a fraction of a node As the ARCHER2 nodes contain a large number of cores (128 per node) it may sometimes be useful to be able to run multiple executables on a single node. For example, you may want to run 128 copies of a serial executable or Python script; or, you may want to run multiple copies of parallel executables that use fewer than 128 cores each. This use model is possible using multiple srun commands in a job script on ARCHER2 Note You can never share a compute node with another user. Although you can use srun to place multiple copies of an executable or script on a compute node, you still have exclusive use of that node. The minimum amount of resources you can reserve for your use on ARCHER2 is a single node. When using srun to place multiple executables or scripts on a compute node you must be aware of a few things: The srun command must specify any Slurm options that differ in value from those specified to sbatch . This typically means that you need to specify the --nodes , --ntasks and --tasks-per-node options to srun . You will need to include the --exact flag to your srun command. With this flag on, Slurm will ensure that the resources you request are assigned to your subjob. Furthermore, if the resources are not currently available, Slurm will output a message letting you know that this is the case and stall the launch of this subjob until enough of your previous subjobs have completed to free up the resources for this subjob. You will need to define the memory required by each subjob with the --mem=<amount of memory> flag. The amount of memory is given in MiB by default but other units can be specified. If you do not know how much memory to specify, we recommend that you specify 1500M (1,500 MiB) per core being used. You will need to place each srun command into the background and then use the wait command at the end of the submission script to make sure it does not exit before the commands are complete. If you want to use more than one node in the job and use multiple srun per node (e.g. 256 single core processes across 2 nodes) then you need to pass the node ID to the srun commands otherwise Slurm will oversubscribe cores on the first node. Below, we provide four examples or running multiple subjobs in a node: one that runs 128 serial processes across a single node; one that runs 8 subjobs each of which use 8 MPI processes with 2 OpenMP threads per MPI process; one that runs four inhomogeneous jobs, each of which requires a different number of MPI processes and OpenMP threads per process; and one that runs 256 serial processes across two nodes. Example 1: 128 serial tasks running on a single node For our first example, we will run 128 single-core copies of the xthi program (which prints process/thread placement) on a single ARCHER2 compute node with each copy of xthi pinned to a different core. The job submission script for this example would look like: Full system #!/bin/bash # Slurm job options (job-name, compute nodes, job time) #SBATCH --job-name=MultiSerialOnCompute #SBATCH --time=0:10:0 #SBATCH --nodes=1 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 # Replace [budget code] below with your budget code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Make xthi available module load xthi # Set the number of threads to 1 # This prevents any threaded system libraries from automatically # using threading. export OMP_NUM_THREADS = 1 # Loop over 128 subjobs pinning each to a different core for i in $( seq 1 128 ) do # Launch subjob overriding job settings as required and in the background # Make sure to change the amount specified by the `--mem=` flag to the amount # of memory required. The amount of memory is given in MiB by default but other # units can be specified. If you do not know how much memory to specify, we # recommend that you specify `--mem=1500M` (1,500 MiB). srun --nodes = 1 --ntasks = 1 --tasks-per-node = 1 \\ --exact --mem = 1500M xthi > placement ${ i } .txt & done # Wait for all subjobs to finish wait Example 2: 8 subjobs on 1 node each with 8 MPI processes and 2 OpenMP threads per process For our second example, we will run 8 subjobs, each running the xthi program (which prints process/thread placement) across 1 node. Each subjob will use 8 MPI processes and 2 OpenMP threads per process. The job submission script for this example would look like: Full system #!/bin/bash # Slurm job options (job-name, compute nodes, job time) #SBATCH --job-name=MultiParallelOnCompute #SBATCH --time=0:10:0 #SBATCH --nodes=1 #SBATCH --tasks-per-node=64 #SBATCH --cpus-per-task=2 # Replace [budget code] below with your budget code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Make xthi available module load xthi # Set the number of threads to 2 as required by all subjobs export OMP_NUM_THREADS = 2 # Loop over 8 subjobs for i in $( seq 1 8 ) do echo $j $i # Launch subjob overriding job settings as required and in the background # Make sure to change the amount specified by the `--mem=` flag to the amount # of memory required. The amount of memory is given in MiB by default but other # units can be specified. If you do not know how much memory to specify, we # recommend that you specify `--mem=12500M` (12,500 MiB). srun --nodes = 1 --ntasks = 8 --tasks-per-node = 8 --cpus-per-task = 2 \\ --exact --mem = 12500M xthi > placement ${ i } .txt & done # Wait for all subjobs to finish wait Example 3: Running inhomogeneous subjobs on one node For our third example, we will run 4 subjobs, each running the xthi program (which prints process/thread placement) across 1 node. Our subjobs will each run with a different number of MPI processes and OpenMP threads. We will run: one job with 64 MPI processes and 1 OpenMP process per thread; one job with 16 MPI processes and 2 threads per process; one job with 4 MPI processes and 4 OpenMP threads per job; and, one job with 1 MPI process and 16 OpenMP threads per job. To be able to change the number of MPI processes and OpenMP threads per process, we will need to forgo using the #SBATCH --tasks-per-node and the #SBATCH cpus-per-task commands -- if you set these Slurm will not let you alter the OMP_NUM_THREADS variable and you will not be able to change the number of OpenMP threads per process between each job. Before each srun command, you will need to define the number of OpenMP threads per process you want by changing the OMP_NUM_THREADS variable. Furthermore, for each srun command, you will need to set the --ntasks flag to equal the number of MPI processes you want to use. You will also need to set the --cpus-per-task flag to equal the number of OpenMP threads per process you want to use. Full system #!/bin/bash # Slurm job options (job-name, compute nodes, job time) #SBATCH --job-name=MultiParallelOnCompute #SBATCH --time=0:10:0 #SBATCH --nodes=1 #SBATCH --hint=nomultithread #SBATCH --distribution=block:block # Replace [budget code] below with your budget code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Make xthi available module load xthi # Set the number of threads to value required by the first job export OMP_NUM_THREADS = 1 srun --ntasks = 64 --cpus-per-task = ${ OMP_NUM_THREADS } \\ --exact --mem = 12500M xthi > placement ${ OMP_NUM_THREADS } .txt & # Set the number of threads to the value required by the second job export OMP_NUM_THREADS = 2 srun --ntasks = 16 --cpus-per-task = ${ OMP_NUM_THREADS } \\ --exact --mem = 12500M xthi > placement ${ OMP_NUM_THREADS } .txt & # Set the number of threads to the value required by the second job export OMP_NUM_THREADS = 4 srun --ntasks = 4 --cpus-per-task = ${ OMP_NUM_THREADS } \\ --exact --mem = 12500M xthi > placement ${ OMP_NUM_THREADS } .txt & # Set the number of threads to the value required by the second job export OMP_NUM_THREADS = 16 srun --ntasks = 1 --cpus-per-task = ${ OMP_NUM_THREADS } \\ --exact --mem = 12500M xthi > placement ${ OMP_NUM_THREADS } .txt & # Wait for all subjobs to finish wait Example 4: 256 serial tasks running across two nodes For our fourth example, we will run 256 single-core copies of the xthi program (which prints process/thread placement) across two ARCHER2 compute nodes with each copy of xthi pinned to a different core. We will illustrate a mechanism for getting the node IDs to pass to srun as this is required to ensure that the individual subjobs are assigned to the correct node. This mechanism uses the scontrol command to turn the nodelist from sbatch into a format we can use as input to srun . The job submission script for this example would look like: Full system #!/bin/bash # Slurm job options (job-name, compute nodes, job time) #SBATCH --job-name=MultiSerialOnComputes #SBATCH --time=0:10:0 #SBATCH --nodes=2 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 # Replace [budget code] below with your budget code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Make xthi available module load xthi # Set the number of threads to 1 # This prevents any threaded system libraries from automatically # using threading. export OMP_NUM_THREADS = 1 # Get a list of the nodes assigned to this job in a format we can use. # scontrol converts the condensed node IDs in the sbatch environment # variable into a list of full node IDs that we can use with srun to # ensure the subjobs are placed on the correct node. e.g. this converts # \"nid[001234,002345]\" to \"nid001234 nid002345\" nodelist = $( scontrol show hostnames $SLURM_JOB_NODELIST ) # Loop over the nodes assigned to the job for nodeid in $nodelist do # Loop over 128 subjobs on each node pinning each to a different core for i in $( seq 1 128 ) do # Launch subjob overriding job settings as required and in the background # Make sure to change the amount specified by the `--mem=` flag to the amount # of memory required. The amount of memory is given in MiB by default but other # units can be specified. If you do not know how much memory to specify, we # recommend that you specify `--mem=1500M` (1,500 MiB). srun --nodelist = ${ nodeid } --nodes = 1 --ntasks = 1 --tasks-per-node = 1 \\ --exact --mem = 1500M xthi > placement_ ${ nodeid } _ ${ i } .txt & done done # Wait for all subjobs to finish wait Process placement There are many occasions where you may want to control (usually, MPI) process placement and change it from the default, for example: You may want to place processes to NUMA regions in a round-robin way rather than the default sequential placement You may be using fewer than 128 processes per node and want to ensure that processes are placed evenly across NUMA regions (16-core blocks) or core complexes (4-core blocks that share an L3 cache) There are a number of different methods for defining process placement, below we cover two different options: using Slurm options and using the MPICH_RANK_REORDER_METHOD environment variable. Most users will likely use the Slurm options approach. Default process placement The default is to place processes sequentially on nodes until the maximum number of tasks is reached. You can use the xthi program to verify this for MPI process placement: auser@ln04:/work/t01/t01/auser> salloc --nodes=2 --tasks-per-node=128 \\ --cpus-per-task=1 --time=0:10:0 --partition=standard --qos=short \\ --account=[your account] salloc: Pending job allocation 1170365 salloc: job 1170365 queued and waiting for resources salloc: job 1170365 has been allocated resources salloc: Granted job allocation 1170365 salloc: Waiting for resource configuration salloc: Nodes nid[002526-002527] are ready for job auser@ln04:/work/t01/t01/auser> module load xthi auser@ln04:/work/t01/t01/auser> export OMP_NUM_THREADS=1 auser@ln04:/work/t01/t01/auser> srun --distribution=block:block --hint=nomultithread xthi Node summary for 2 nodes: Node 0, hostname nid002526, mpi 128, omp 1, executable xthi Node 1, hostname nid002527, mpi 128, omp 1, executable xthi MPI summary: 256 ranks Node 0, rank 0, thread 0, (affinity = 0) Node 0, rank 1, thread 0, (affinity = 1) Node 0, rank 2, thread 0, (affinity = 2) Node 0, rank 3, thread 0, (affinity = 3) ...output trimmed... Node 0, rank 124, thread 0, (affinity = 124) Node 0, rank 125, thread 0, (affinity = 125) Node 0, rank 126, thread 0, (affinity = 126) Node 0, rank 127, thread 0, (affinity = 127) Node 1, rank 128, thread 0, (affinity = 0) Node 1, rank 129, thread 0, (affinity = 1) Node 1, rank 130, thread 0, (affinity = 2) Node 1, rank 131, thread 0, (affinity = 3) ...output trimmed... Note For MPI programs on ARCHER2, each rank corresponds to a process . Important To get good performance out of MPI collective operations, MPI processes should be placed sequentially on cores as in the default placement described above. Setting process placement using Slurm options For underpopulation of nodes with processes When you are using fewer processes than cores on compute nodes (i.e. < 128 processes per node) the basic Slurm options (usually supplied in your script as options to sbatch ) for process placement are: --ntasks-per-node=X Place X processes on each node --cpus-per-task=Y Set a stride of Y cores between each placed process In addition, the following options are added to your srun commands in your job submission script: --hint=nomultithread Only use physical cores (avoids use of SMT/hyperthreads) --distribution=block:block Allocate processes to cores in a sequential fashion For example, to place 32 processes per node and have 1 process per 4-core block (corresponding to a core complex that shares an L3 cache), you would set: --ntasks-per-node=32 Place 32 processes on each node --cpus-per-task=4 Set a stride of 4 cores between each placed process Here is the output from xthi : auser@ln04:/work/t01/t01/auser> salloc --nodes=2 --tasks-per-node=32 \\ --cpus-per-task=4 --time=0:10:0 --partition=standard --qos=short \\ --account=[your account] salloc: Pending job allocation 1170383 salloc: job 1170383 queued and waiting for resources salloc: job 1170383 has been allocated resources salloc: Granted job allocation 1170383 salloc: Waiting for resource configuration salloc: Nodes nid[002526-002527] are ready for job auser@ln04:/work/t01/t01/auser> module load xthi auser@ln04:/work/t01/t01/auser> export OMP_NUM_THREADS=1 auser@ln04:/work/t01/t01/auser> srun --distribution=block:block --hint=nomultithread xthi Node summary for 2 nodes: Node 0, hostname nid002526, mpi 32, omp 1, executable xthi Node 1, hostname nid002527, mpi 32, omp 1, executable xthi MPI summary: 64 ranks Node 0, rank 0, thread 0, (affinity = 0-3) Node 0, rank 1, thread 0, (affinity = 4-7) Node 0, rank 2, thread 0, (affinity = 8-11) Node 0, rank 3, thread 0, (affinity = 12-15) Node 0, rank 4, thread 0, (affinity = 16-19) Node 0, rank 5, thread 0, (affinity = 20-23) Node 0, rank 6, thread 0, (affinity = 24-27) Node 0, rank 7, thread 0, (affinity = 28-31) Node 0, rank 8, thread 0, (affinity = 32-35) Node 0, rank 9, thread 0, (affinity = 36-39) Node 0, rank 10, thread 0, (affinity = 40-43) Node 0, rank 11, thread 0, (affinity = 44-47) Node 0, rank 12, thread 0, (affinity = 48-51) Node 0, rank 13, thread 0, (affinity = 52-55) Node 0, rank 14, thread 0, (affinity = 56-59) Node 0, rank 15, thread 0, (affinity = 60-63) Node 0, rank 16, thread 0, (affinity = 64-67) Node 0, rank 17, thread 0, (affinity = 68-71) Node 0, rank 18, thread 0, (affinity = 72-75) Node 0, rank 19, thread 0, (affinity = 76-79) Node 0, rank 20, thread 0, (affinity = 80-83) Node 0, rank 21, thread 0, (affinity = 84-87) Node 0, rank 22, thread 0, (affinity = 88-91) Node 0, rank 23, thread 0, (affinity = 92-95) Node 0, rank 24, thread 0, (affinity = 96-99) Node 0, rank 25, thread 0, (affinity = 100-103) Node 0, rank 26, thread 0, (affinity = 104-107) Node 0, rank 27, thread 0, (affinity = 108-111) Node 0, rank 28, thread 0, (affinity = 112-115) Node 0, rank 29, thread 0, (affinity = 116-119) Node 0, rank 30, thread 0, (affinity = 120-123) Node 0, rank 31, thread 0, (affinity = 124-127) Node 1, rank 32, thread 0, (affinity = 0-3) Node 1, rank 33, thread 0, (affinity = 4-7) Node 1, rank 34, thread 0, (affinity = 8-11) Node 1, rank 35, thread 0, (affinity = 12-15) Node 1, rank 36, thread 0, (affinity = 16-19) Node 1, rank 37, thread 0, (affinity = 20-23) Node 1, rank 38, thread 0, (affinity = 24-27) Node 1, rank 39, thread 0, (affinity = 28-31) Node 1, rank 40, thread 0, (affinity = 32-35) Node 1, rank 41, thread 0, (affinity = 36-39) Node 1, rank 42, thread 0, (affinity = 40-43) Node 1, rank 43, thread 0, (affinity = 44-47) Node 1, rank 44, thread 0, (affinity = 48-51) Node 1, rank 45, thread 0, (affinity = 52-55) Node 1, rank 46, thread 0, (affinity = 56-59) Node 1, rank 47, thread 0, (affinity = 60-63) Node 1, rank 48, thread 0, (affinity = 64-67) Node 1, rank 49, thread 0, (affinity = 68-71) Node 1, rank 50, thread 0, (affinity = 72-75) Node 1, rank 51, thread 0, (affinity = 76-79) Node 1, rank 52, thread 0, (affinity = 80-83) Node 1, rank 53, thread 0, (affinity = 84-87) Node 1, rank 54, thread 0, (affinity = 88-91) Node 1, rank 55, thread 0, (affinity = 92-95) Node 1, rank 56, thread 0, (affinity = 96-99) Node 1, rank 57, thread 0, (affinity = 100-103) Node 1, rank 58, thread 0, (affinity = 104-107) Node 1, rank 59, thread 0, (affinity = 108-111) Node 1, rank 60, thread 0, (affinity = 112-115) Node 1, rank 61, thread 0, (affinity = 116-119) Node 1, rank 62, thread 0, (affinity = 120-123) Node 1, rank 63, thread 0, (affinity = 124-127) Tip You usually only want to use physical cores on ARCHER2, so ( tasks-per-node ) \u00d7 ( cpus-per-task ) should generally be equal to 128. Full node population with non-sequential process placement If you want to change the order processes are placed on nodes and cores using Slurm options then you should use the --distribution option to srun to change this. For example, to place processes sequentially on nodes but round-robin on the 16-core NUMA regions in a single node, you would use the --distribution=block:cyclic option to srun . This type of process placement can be beneficial when a code is memory bound. auser@ln04:/work/t01/t01/auser> salloc --nodes=2 --tasks-per-node=128 \\ --cpus-per-task=1 --time=0:10:0 --partition=standard --qos=short \\ --account=[your account] salloc: Pending job allocation 1170594 salloc: job 1170594 queued and waiting for resources salloc: job 1170594 has been allocated resources salloc: Granted job allocation 1170594 salloc: Waiting for resource configuration salloc: Nodes nid[002616,002621] are ready for job auser@ln04:/work/t01/t01/auser> module load xthi auser@ln04:/work/t01/t01/auser> export OMP_NUM_THREADS=1 auser@ln04:/work/t01/t01/auser> srun --distribution=block:cyclic --hint=nomultithread xthi Node summary for 2 nodes: Node 0, hostname nid002616, mpi 128, omp 1, executable xthi Node 1, hostname nid002621, mpi 128, omp 1, executable xthi MPI summary: 256 ranks Node 0, rank 0, thread 0, (affinity = 0) Node 0, rank 1, thread 0, (affinity = 16) Node 0, rank 2, thread 0, (affinity = 32) Node 0, rank 3, thread 0, (affinity = 48) Node 0, rank 4, thread 0, (affinity = 64) Node 0, rank 5, thread 0, (affinity = 80) Node 0, rank 6, thread 0, (affinity = 96) Node 0, rank 7, thread 0, (affinity = 112) Node 0, rank 8, thread 0, (affinity = 1) Node 0, rank 9, thread 0, (affinity = 17) Node 0, rank 10, thread 0, (affinity = 33) Node 0, rank 11, thread 0, (affinity = 49) Node 0, rank 12, thread 0, (affinity = 65) Node 0, rank 13, thread 0, (affinity = 81) Node 0, rank 14, thread 0, (affinity = 97) Node 0, rank 15, thread 0, (affinity = 113 ...output trimmed... Node 0, rank 120, thread 0, (affinity = 15) Node 0, rank 121, thread 0, (affinity = 31) Node 0, rank 122, thread 0, (affinity = 47) Node 0, rank 123, thread 0, (affinity = 63) Node 0, rank 124, thread 0, (affinity = 79) Node 0, rank 125, thread 0, (affinity = 95) Node 0, rank 126, thread 0, (affinity = 111) Node 0, rank 127, thread 0, (affinity = 127) Node 1, rank 128, thread 0, (affinity = 0) Node 1, rank 129, thread 0, (affinity = 16) Node 1, rank 130, thread 0, (affinity = 32) Node 1, rank 131, thread 0, (affinity = 48) Node 1, rank 132, thread 0, (affinity = 64) Node 1, rank 133, thread 0, (affinity = 80) Node 1, rank 134, thread 0, (affinity = 96) Node 1, rank 135, thread 0, (affinity = 112) ...output trimmed... If you wish to place processes round robin on both nodes and 16-core NUMA regions within in a node you would use --distribution=cyclic:cyclic : auser@ln04:/work/t01/t01/auser> salloc --nodes=2 --tasks-per-node=128 \\ --cpus-per-task=1 --time=0:10:0 --partition=standard --qos=short \\ --account=[your account] salloc: Pending job allocation 1170594 salloc: job 1170594 queued and waiting for resources salloc: job 1170594 has been allocated resources salloc: Granted job allocation 1170594 salloc: Waiting for resource configuration salloc: Nodes nid[002616,002621] are ready for job auser@ln04:/work/t01/t01/auser> module load xthi auser@ln04:/work/t01/t01/auser> export OMP_NUM_THREADS=1 auser@ln04:/work/t01/t01/auser> srun --distribution=cyclic:cyclic --hint=nomultithread xthi Node summary for 2 nodes: Node 0, hostname nid002616, mpi 128, omp 1, executable xthi Node 1, hostname nid002621, mpi 128, omp 1, executable xthi MPI summary: 256 ranks Node 0, rank 0, thread 0, (affinity = 0) Node 0, rank 2, thread 0, (affinity = 16) Node 0, rank 4, thread 0, (affinity = 32) Node 0, rank 6, thread 0, (affinity = 48) Node 0, rank 8, thread 0, (affinity = 64) Node 0, rank 10, thread 0, (affinity = 80) Node 0, rank 12, thread 0, (affinity = 96) Node 0, rank 14, thread 0, (affinity = 112) Node 0, rank 16, thread 0, (affinity = 1) Node 0, rank 18, thread 0, (affinity = 17) Node 0, rank 20, thread 0, (affinity = 33) Node 0, rank 22, thread 0, (affinity = 49) Node 0, rank 24, thread 0, (affinity = 65) Node 0, rank 26, thread 0, (affinity = 81) Node 0, rank 28, thread 0, (affinity = 97) Node 0, rank 30, thread 0, (affinity = 113) ...output trimmed... Node 1, rank 1, thread 0, (affinity = 0) Node 1, rank 3, thread 0, (affinity = 16) Node 1, rank 5, thread 0, (affinity = 32) Node 1, rank 7, thread 0, (affinity = 48) Node 1, rank 9, thread 0, (affinity = 64) Node 1, rank 11, thread 0, (affinity = 80) Node 1, rank 13, thread 0, (affinity = 96) Node 1, rank 15, thread 0, (affinity = 112) Node 1, rank 17, thread 0, (affinity = 1) Node 1, rank 19, thread 0, (affinity = 17) Node 1, rank 21, thread 0, (affinity = 33) Node 1, rank 23, thread 0, (affinity = 49) Node 1, rank 25, thread 0, (affinity = 65) Node 1, rank 27, thread 0, (affinity = 81) Node 1, rank 29, thread 0, (affinity = 97) Node 1, rank 31, thread 0, (affinity = 113) ...output trimmed... Remember, MPI collective performance is generally much worse if processes are not placed sequentially on a node (so adjacent MPI ranks are as close to each other as possible). This is the reason that the default recommended placement on ARCHER2 is sequential rather than round-robin. MPICH_RANK_REORDER_METHOD for MPI process placement The MPICH_RANK_REORDER_METHOD environment variable can also be used to specify other types of MPI task placement. For example, setting it to \"0\" results in a round-robin placement on both nodes and NUMA regions in a node (equivalent to the --distribution=cyclic:cyclic option to srun ). Note, we do not specify the --distribution option to srun in this case as the environment variable is controlling placement: salloc --nodes=8 --tasks-per-node=2 --cpus-per-task=1 --time=0:10:0 --account=t01 salloc: Granted job allocation 24236 salloc: Waiting for resource configuration salloc: Nodes cn13 are ready for job module load xthi export OMP_NUM_THREADS=1 export MPICH_RANK_REORDER_METHOD=0 srun --hint=nomultithread xthi Node summary for 2 nodes: Node 0, hostname nid002616, mpi 128, omp 1, executable xthi Node 1, hostname nid002621, mpi 128, omp 1, executable xthi MPI summary: 256 ranks Node 0, rank 0, thread 0, (affinity = 0) Node 0, rank 2, thread 0, (affinity = 16) Node 0, rank 4, thread 0, (affinity = 32) Node 0, rank 6, thread 0, (affinity = 48) Node 0, rank 8, thread 0, (affinity = 64) Node 0, rank 10, thread 0, (affinity = 80) Node 0, rank 12, thread 0, (affinity = 96) Node 0, rank 14, thread 0, (affinity = 112) Node 0, rank 16, thread 0, (affinity = 1) Node 0, rank 18, thread 0, (affinity = 17) Node 0, rank 20, thread 0, (affinity = 33) Node 0, rank 22, thread 0, (affinity = 49) Node 0, rank 24, thread 0, (affinity = 65) Node 0, rank 26, thread 0, (affinity = 81) Node 0, rank 28, thread 0, (affinity = 97) Node 0, rank 30, thread 0, (affinity = 113) ...output trimmed... There are other modes available with the MPICH_RANK_REORDER_METHOD environment variable, including one which lets the user provide a file called MPICH_RANK_ORDER which contains a list of each task's placement on each node. These options are described in detail in the intro_mpi man page. grid_order For MPI applications which perform a large amount of nearest-neighbor communication, e.g., stencil-based applications on structured grids, HPE provide a tool in the perftools-base module (Loaded by default for all users) called grid_order which can generate a MPICH_RANK_ORDER file automatically by taking as parameters the dimensions of the grid, core count, etc. For example, to place 256 MPI parameters in row-major order on a Cartesian grid of size $(8, 8, 4)$, using 128 cores per node: grid_order -R -c 128 -g 8,8,4 # grid_order -R -Z -c 128 -g 8,8,4 # Region 3: 0,0,1 (0..255) 0,1,2,3,32,33,34,35,64,65,66,67,96,97,98,99,128,129,130,131,160,161,162,163,192,193,194,195,224,225,226,227,4,5,6,7,36,37,38,39,68,69,70,71,100,101,102,103,132,133,134,135,164,165,166,167,196,197,198,199,228,229,230,231,8,9,10,11,40,41,42,43,72,73,74,75,104,105,106,107,136,137,138,139,168,169,170,171,200,201,202,203,232,233,234,235,12,13,14,15,44,45,46,47,76,77,78,79,108,109,110,111,140,141,142,143,172,173,174,175,204,205,206,207,236,237,238,239 16,17,18,19,48,49,50,51,80,81,82,83,112,113,114,115,144,145,146,147,176,177,178,179,208,209,210,211,240,241,242,243,20,21,22,23,52,53,54,55,84,85,86,87,116,117,118,119,148,149,150,151,180,181,182,183,212,213,214,215,244,245,246,247,24,25,26,27,56,57,58,59,88,89,90,91,120,121,122,123,152,153,154,155,184,185,186,187,216,217,218,219,248,249,250,251,28,29,30,31,60,61,62,63,92,93,94,95,124,125,126,127,156,157,158,159,188,189,190,191,220,221,222,223,252,253,254,255 One can then save this output to a file called MPICH_RANK_ORDER and then set MPICH_RANK_REORDER_METHOD=3 before running the job, which tells Cray MPI to read the MPICH_RANK_ORDER file to set the MPI task placement. For more information, please see the man page man grid_order . Interactive Jobs Using salloc to reserve resources When you are developing or debugging code you often want to run many short jobs with a small amount of editing the code between runs. This can be achieved by using the login nodes to run MPI but you may want to test on the compute nodes (e.g. you may want to test running on multiple nodes across the high performance interconnect). One of the best ways to achieve this on ARCHER2 is to use interactive jobs. An interactive job allows you to issue srun commands directly from the command line without using a job submission script, and to see the output from your program directly in the terminal. You use the salloc command to reserve compute nodes for interactive jobs. To submit a request for an interactive job reserving 8 nodes (1024 physical cores) for 20 minutes on the short queue you would issue the following command from the command line: Full system auser@ln01:> salloc --nodes = 8 --tasks-per-node = 128 --cpus-per-task = 1 \\ --time = 00 :20:00 --partition = standard --qos = short \\ --account =[ budget code ] When you submit this job your terminal will display something like: Full system salloc: Granted job allocation 24236 salloc: Waiting for resource configuration salloc: Nodes nid000002 are ready for job auser@ln01:> It may take some time for your interactive job to start. Once it runs you will enter a standard interactive terminal session (a new shell). Note that this shell is still on the front end (the prompt has not change). Whilst the interactive session lasts you will be able to run parallel jobs on the compute nodes by issuing the srun --distribution=block:block --hint=nomultithread command directly at your command prompt using the same syntax as you would inside a job script. The maximum number of nodes you can use is limited by resources requested in the salloc command. If you know you will be doing a lot of intensive debugging you may find it useful to request an interactive session lasting the expected length of your working session, say a full day. Your session will end when you hit the requested walltime. If you wish to finish before this you should use the exit command - this will return you to your prompt before you issued the salloc command. Using srun directly A second way to run an interactive job is to use srun directly in the following way (here using the \"short queue\"): Full system auser@ln01:/work/t01/t01/auser> srun --nodes=1 --exclusive --time=00:20:00 \\ --partition=standard --qos=short --reservation=shortqos \\ --account=[budget code] --pty /bin/bash auser@nid001261:/work/t01/t01/auser> hostname nid001261 The --pty /bin/bash will cause a new shell to be started on the first node of a new allocation . This is perhaps closer to what many people consider an 'interactive' job than the method using salloc appears. One can now issue shell commands in the usual way. A further invocation of srun is required to launch a parallel job in the allocation. Note When using srun within an interactive srun session, you will need to include the --oversubscribe flag and specify the number of cores you want to use: auser@nid001261:/work/t01/t01/auser> srun --oversubscribe --distribution=block:block \\ --hint=nomultithread --ntasks=128 ./my_mpi_executable.x When finished, type exit to relinquish the allocation and control will be returned to the front end. By default, the interactive shell will retain the environment of the parent. If you want a clean shell, remember to specify --export=none . Heterogeneous jobs The Slurm submissions discussed above involve a single executable image. However, there are situtions where two or more distinct executables are coupled and need to be run at the same time. This is most easily handled via the Slurm heterogeneous job mechanism. Two common cases are discussed below: first, a client server model in which client and server each have a different MPI_COMM_WORLD , and second the case were two or more executables share MPI_COMM_WORLD . Heterogeneous jobs for a client/server model: distinct MPI_COMM_WORLDs The essential feature of a heterogeneous job here is to create a single batch submission which specifies the resource requirements for the individual components. Schematically, we would use #!/bin/bash # Slurm specifications for the first component #SBATCH --partition=standard ... #SBATCH hetjob # Slurm specifications for the second component #SBATCH --partition=standard ... where new each component beyond the first is introduced by the special token #SBATCH hetjob (note this is not a normal option and is not --hetjob ). Each component must specify a partition. Such a job will appear in the queue system as, e.g., 50098+0 standard qscript- user PD 0:00 1 (None) 50098+1 standard qscript- user PD 0:00 2 (None) and counts as (in this case) two separate jobs from the point of QoS limits. Consider a case where we have two executables which may both be parallel (in that they use MPI), both run at the same time, and communicate with each other by some means other than MPI. In the following example, we run two different executables, both of which must finish before the jobs completes. #!/bin/bash #SBATCH --time=00:20:00 #SBATCH --exclusive #SBATCH --export=none #SBATCH --partition=standard #SBATCH --qos=standard #SBATCH --nodes=1 #SBATCH --ntasks-per-node=8 #SBATCH hetjob #SBATCH --partition=standard #SBATCH --qos=standard #SBATCH --nodes=2 #SBATCH --ntasks-per-node=4 # Run two execuatables with separate MPI_COMM_WORLD srun --distribution=block:block --hint=nomultithread --het-group=0 ./xthi-a & srun --distribution=block:block --hint=nomultithread --het-group=1 ./xthi-b & wait In this case, each executable is launched with a separate call to srun but specifies a different heterogeneous group via the --het-group option. The first group is --het-group=0 . Both are run in the background with & and the wait is required to ensure both executables have completed before the job submission exits. In this rather artificial example, where each component makes a simple report about its placement, the output might be Node 0, hostname nid001028, mpi 4, omp 1, executable xthi-b Node 1, hostname nid001048, mpi 4, omp 1, executable xthi-b Node 0, rank 0, thread 0, (affinity = 0) Node 0, rank 1, thread 0, (affinity = 1) Node 0, rank 2, thread 0, (affinity = 2) Node 0, rank 3, thread 0, (affinity = 3) Node 1, rank 4, thread 0, (affinity = 0) Node 1, rank 5, thread 0, (affinity = 1) Node 1, rank 6, thread 0, (affinity = 2) Node 1, rank 7, thread 0, (affinity = 3) Node 0, hostname nid001027, mpi 8, omp 1, executable xthi-a Node 0, rank 0, thread 0, (affinity = 0) Node 0, rank 1, thread 0, (affinity = 1) Node 0, rank 2, thread 0, (affinity = 2) Node 0, rank 3, thread 0, (affinity = 3) Node 0, rank 4, thread 0, (affinity = 4) Node 0, rank 5, thread 0, (affinity = 5) Node 0, rank 6, thread 0, (affinity = 6) Node 0, rank 7, thread 0, (affinity = 7) Here we have the first executable running on one node with a communicator size 8 (ranks 0-7). The second executable runs on two nodes also with communicator size 8 (ranks 0-7, 4 ranks per node). Further examples of placement for heterogenenous jobs are given below. Heterogeneous jobs for a shared MPI_COM_WORLD Full system Note The directive SBATCH hetjob can no longer be used for jobs requiring a shared MPI_COMM_WORLD If two or more heterogeneous components need to share a unique MPI_COMM_WORLD , a single srun invocation with the differrent components separated by a colon : should be used. Arguements to the individual components of the srun control the placement of the tasks and threads for each component. For example: #!/bin/bash #SBATCH --time=00:20:00 #SBATCH --exclusive #SBATCH --export=none #SBATCH --account=[...] #SBATCH --partition=standard #SBATCH --qos=standard # We must specify correctly the total number of nodes required. #SBATCH --nodes=3 SHARED_ARGS=\"--distribution=block:block --hint=nomultithread\" srun --het-group=0 --nodes=1 --tasks-per-node=8 ${SHARED_ARGS} ./xthi-a : \\ --het-group=1 --nodes=2 --tasks-per-node=4 ${SHARED_ARGS} ./xthi-b The output should confirm we have a single MPI_COMM_WORLD with a total of three nodes, and ranks 0-15. Node summary for 3 nodes: Node 0, hostname nid002668, mpi 8, omp 1, executable xthi-a Node 1, hostname nid002669, mpi 4, omp 1, executable xthi-b Node 2, hostname nid002670, mpi 4, omp 1, executable xthi-b MPI summary: 16 ranks Node 0, rank 0, thread 0, (affinity = 0) Node 0, rank 1, thread 0, (affinity = 1) Node 0, rank 2, thread 0, (affinity = 2) Node 0, rank 3, thread 0, (affinity = 3) Node 0, rank 4, thread 0, (affinity = 4) Node 0, rank 5, thread 0, (affinity = 5) Node 0, rank 6, thread 0, (affinity = 6) Node 0, rank 7, thread 0, (affinity = 7) Node 1, rank 8, thread 0, (affinity = 0) Node 1, rank 9, thread 0, (affinity = 1) Node 1, rank 10, thread 0, (affinity = 2) Node 1, rank 11, thread 0, (affinity = 3) Node 2, rank 12, thread 0, (affinity = 0) Node 2, rank 13, thread 0, (affinity = 1) Node 2, rank 14, thread 0, (affinity = 2) Node 2, rank 15, thread 0, (affinity = 3) Heterogeneous placement for mixed MPI/OpenMP work Some care may be required for placement of tasks/threads in heterogeneous jobs in which the number of threads needs to be specified differently for different components. In the following we have two components. The first component runs 8 MPI tasks each with 16 OpenMP threads. The second component runs 8 MPI tasks with one task per NUMA region on one node; each task has one thread. An appropriate Slurm submission might be: Full system #!/bin/bash #SBATCH --time=00:20:00 #SBATCH --exclusive #SBATCH --export=none #SBATCH --account=[...] #SBATCH --partition=standard #SBATCH --qos=standard #SBATCH --nodes=2 SHARED_ARGS=\"--distribution=block:block --hint=nomultithread \\ --nodes=1 --tasks-per-node=8 --cpus-per-task=16\" # Do not set OMP_NUM_THREADS in the calling environment unset OMP_NUM_THREADS export OMP_PROC_BIND=spread srun --het-group=0 ${SHARED_ARGS} --export=all,OMP_NUM_THREADS=16 ./xthi-a : \\ --het-group=1 ${SHARED_ARGS} --export=all,OMP_NUM_THREADS=1 ./xthi-b The important point here is that OMP_NUM_THREADS must not be set in the environment that calls srun in order that the different specifications for the separate groups via --export on the srun command line take effect. If OMP_NUM_THREADS is set in the calling environment, then that value takes precedence, and each component will see the same value of OMP_NUM_THREADS . The output would be: Node 0, hostname nid001111, mpi 8, omp 16, executable xthi-a Node 1, hostname nid001126, mpi 8, omp 1, executable xthi-b Node 0, rank 0, thread 0, (affinity = 0) Node 0, rank 0, thread 1, (affinity = 1) Node 0, rank 0, thread 2, (affinity = 2) Node 0, rank 0, thread 3, (affinity = 3) Node 0, rank 0, thread 4, (affinity = 4) Node 0, rank 0, thread 5, (affinity = 5) Node 0, rank 0, thread 6, (affinity = 6) Node 0, rank 0, thread 7, (affinity = 7) Node 0, rank 0, thread 8, (affinity = 8) Node 0, rank 0, thread 9, (affinity = 9) Node 0, rank 0, thread 10, (affinity = 10) Node 0, rank 0, thread 11, (affinity = 11) Node 0, rank 0, thread 12, (affinity = 12) Node 0, rank 0, thread 13, (affinity = 13) Node 0, rank 0, thread 14, (affinity = 14) Node 0, rank 0, thread 15, (affinity = 15) Node 0, rank 1, thread 0, (affinity = 16) Node 0, rank 1, thread 1, (affinity = 17) ... Node 0, rank 7, thread 14, (affinity = 126) Node 0, rank 7, thread 15, (affinity = 127) Node 1, rank 8, thread 0, (affinity = 0) Node 1, rank 9, thread 0, (affinity = 16) Node 1, rank 10, thread 0, (affinity = 32) Node 1, rank 11, thread 0, (affinity = 48) Node 1, rank 12, thread 0, (affinity = 64) Node 1, rank 13, thread 0, (affinity = 80) Node 1, rank 14, thread 0, (affinity = 96) Node 1, rank 15, thread 0, (affinity = 112) Low priority access Low priority jobs are not charged against your allocation but will only run when other, higher-priority, jobs cannot be run or there are no higher-priority jobs in the queue. Although low priority jobs are not charged, you do need a valid, positive budget to be able to submit and run low priority jobs, i.e. you need at least 1 CU in your budget. Low priority access is always available and has the following limits: Full system 1024 node maximum job size Maximum 16 low priority jobs in the queue per user Maximum 16 low priority job running per user (of the 16 queued) Maximum runtime of 24 hours You submit a low priority job on ARCHER2 by using the lowpriority QoS. For example, you would usually have the following line in your job submission script sbatch options: #SBATCH --qos=lowpriority Reservations Reservations are available on ARCHER2. These allow users to reserve a number of nodes for a specified length of time starting at a particular time on the system. Reservations require justification. They will only be approved if the request could not be fulfilled with the standard queues. For instance, you require a job/jobs to run at a particular time e.g. for a demonstration or course. Note Reservation requests must be submitted at least 60 hours in advance of the reservation start time. If requesting a reservation for a Monday at 18:00, please ensure this is received by the Friday at 12:00 the latest. The same applies over Service Holidays. Note Reservations are only valid for standard compute nodes, high memory compute nodes and/or PP nodes cannot be included in reservations. Reservations will be charged at 1.5 times the usual CU rate and our policy is that they will be charged the full rate for the entire reservation at the time of booking, whether or not you use the nodes for the full time. In addition, you will not be refunded the CUs if you fail to use them due to a job issue unless this issue is due to a system failure. Bug At the moment, we are only able to charge for jobs in reservations, not for the full reservation itself. Jobs in reservations are charged at 1.5x the standard rate. To request a reservation you complete a form on SAFE: Log into SAFE Under the \"Login accounts\" menu, choose the \"Request reservation\" option On the first page, you need to provide the following: The start time and date of the reservation. The end time and date of the reservation. Your justification for the reservation -- this must be provided or the request will be rejected. The number of nodes required. On the second page, you will need to specify which username you wish the reservation to be charged against and, once the username has been selected, the budget you want to charge the reservation to. Your request will be checked by the ARCHER2 User Administration team and, if approved, you will be provided a reservation ID which can be used on the system. To submit jobs to a reservation, you need to add --reservation=<reservation ID> and --qos=reservation options to your job submission script or command. Important You must have at least 1 CU in the budget to submit a job on ARCHER2, even to a pre-paid reservation. Tip You can submit jobs to a reservation as soon as the reservation has been set up; jobs will remain queued until the reservation starts. Serial jobs You can run serial jobs on the shared data analysis nodes. More information on using the data analysis nodes (including example job submission scripts) can be found in the Data Analysis section of the User and Best Practice Guide. Best practices for job submission This guidance is adapted from the advice provided by NERSC Time Limits Due to backfill scheduling, short and variable-length jobs generally start quickly resulting in much better job throughput. You can specify a minimum time for your job with the --time-min option to SBATCH: #SBATCH --time-min=<lower_bound> #SBATCH --time=<upper_bound> Within your job script, you can get the time remaining in the job with squeue -h -j ${Slurm_JOBID} -o %L to allow you to deal with potentially varying runtimes when using this option. Long Running Jobs Simulations which must run for a long period of time achieve the best throughput when composed of many small jobs using a checkpoint and restart method chained together (see above for how to chain jobs together). However, this method does occur a startup and shutdown overhead for each job as the state is saved and loaded so you should experiment to find the best balance between runtime (long runtimes minimise the checkpoint/restart overheads) and throughput (short runtimes maximise throughput). Interconnect locality For jobs which are sensitive to interconnect (MPI) performance and utilise 128 nodes or less it is possible to request that all nodes are in a single Slingshot dragonfly group. The maximum number of nodes in a group on ARCHER2 is 128. Slurm has a concept of \"switches\" which on ARCHER2 are configured to map to Slingshot electrical groups; where all compute nodes have all-to-all electrical connections which minimises latency. Since this places an additional constraint on the scheduler a maximum time to wait for the requested topology can be specified - after this time, the job will be placed without the constraint. For example, to specify that all requested nodes should come from one electrical group and to wait for up to 6 hours (360 minutes) for that placement, you would use the following option in your job: #SBATCH --switches=1@360 You can request multiple groups using this option if you are using more nodes than are in a single group to maximise the number of nodes that share electrical connetions in the job. For example, to request 4 groups (maximum of 512 nodes) and have this as an absolute constraint with no timeout, you would use: #SBATCH --switches=4 Danger When specifying the number of groups take care to request enough groups to satisfy the requested number of nodes. If the number is too low then an unneccesary delay will be added due to the unsatisfiable request. A useful heuristic to ensure this is the case is to ensure that the total nodes requested is less than or equal to the number of groups multiplied by 128. Large Jobs Large jobs may take longer to start up. The sbcast command is recommended for large jobs requesting over 1500 MPI tasks. By default, Slurm reads the executable on the allocated compute nodes from the location where it is installed; this may take long time when the file system (where the executable resides) is slow or busy. The sbcast command, the executable can be copied to the /tmp directory on each of the compute nodes. Since /tmp is part of the memory on the compute nodes, it can speed up the job startup time. sbcast --compress=lz4 /path/to/exe /tmp/exe srun /tmp/exe Huge pages Huge pages are virtual memory pages which are bigger than the default page size of 4K bytes. Huge pages can improve memory performance for common access patterns on large data sets since it helps to reduce the number of virtual to physical address translations when compared to using the default 4KB. To use huge pages for an application (with the 2 MB huge pages as an example): module load craype-hugepages2M cc -o mycode.exe mycode.c And also load the same huge pages module at runtime. Warning Due to the huge pages memory fragmentation issue, applications may get Cannot allocate memory warnings or errors when there are not enough hugepages on the compute node, such as: libhugetlbfs [nid0000xx:xxxxx]: WARNING: New heap segment map at 0x10000000 failed: Cannot allocate memory`` By default, The verbosity level of libhugetlbfs HUGETLB_VERBOSE is set to 0 on ARCHER2 to surpress debugging messages. Users can adjust this value to obtain more information on huge pages use. When to Use Huge Pages For MPI applications, map the static data and/or heap onto huge pages. For an application which uses shared memory, which needs to be concurrently registered with the high speed network drivers for remote communication. For SHMEM applications, map the static data and/or private heap onto huge pages. For applications written in Unified Parallel C, Coarray Fortran, and other languages based on the PGAS programming model, map the static data and/or private heap onto huge pages. For an application doing heavy I/O. To improve memory performance for common access patterns on large data sets. When to Avoid Huge Pages Applications sometimes consist of many steering programs in addition to the core application. Applying huge page behavior to all processes would not provide any benefit and would consume huge pages that would otherwise benefit the core application. The runtime environment variable HUGETLB_RESTRICT_EXE can be used to specify the susbset of the programs to use hugepages. For certain applications if using hugepages either causes issues or slows down performance. One such example is that when an application forks more subprocesses (such as pthreads) and these threads allocate memory, the newly allocated memory are the default 4 KB pages.","title":"Running jobs"},{"location":"user-guide/scheduler/#running-jobs-on-archer2","text":"As with most HPC services, ARCHER2 uses a scheduler to manage access to resources and ensure that the thousands of different users of system are able to share the system and all get access to the resources they require. ARCHER2 uses the Slurm software to schedule jobs. Writing a submission script is typically the most convenient way to submit your job to the scheduler. Example submission scripts (with explanations) for the most common job types are provided below. Interactive jobs are also available and can be particularly useful for developing and debugging applications. More details are available below. Hint If you have any questions on how to run jobs on ARCHER2 do not hesitate to contact the ARCHER2 Service Desk . You typically interact with Slurm by issuing Slurm commands from the login nodes (to submit, check and cancel jobs), and by specifying Slurm directives that describe the resources required for your jobs in job submission scripts.","title":"Running jobs on ARCHER2"},{"location":"user-guide/scheduler/#resources","text":"","title":"Resources"},{"location":"user-guide/scheduler/#cus","text":"Time used on ARCHER2 is measured in CUs. 1 CU = 1 Node Hour for a standard 128 core node. The CU calculator will help you to calculate the CU cost for your jobs.","title":"CUs"},{"location":"user-guide/scheduler/#checking-available-budget","text":"You can check in SAFE by selecting Login accounts from the menu, select the login account you want to query. Under Login account details you will see each of the budget codes you have access to listed e.g. e123 resources and then under Resource Pool to the right of this, a note of the remaining budget in CUs. When logged in to the machine you can also use the command sacctmgr show assoc where user=$LOGNAME format=account,user,maxtresmins This will list all the budget codes that you have access to e.g. Account User MaxTRESMins ---------- ---------- ------------- e123 userx cpu=0 e123-test userx This shows that userx is a member of budgets e123 and e123-test . However, the cpu=0 indicates that the e123 budget is empty or disabled. This user can submit jobs using the e123-test budget. To see the number of CUs remaining you must check in SAFE .","title":"Checking available budget"},{"location":"user-guide/scheduler/#charging","text":"Jobs run on ARCHER2 are charged for the time they use i.e. from the time the job begins to run until the time the job ends (not the full wall time requested). Jobs are charged for the full number of nodes which are requested, even if they are not all used. Charging takes place at the time the job ends, and the job is charged in full to the budget which is live at the end time.","title":"Charging"},{"location":"user-guide/scheduler/#basic-slurm-commands","text":"There are four key commands used to interact with the Slurm on the command line: sinfo - Get information on the partitions and resources available sbatch jobscript.slurm - Submit a job submission script (in this case called: jobscript.slurm ) to the scheduler squeue - Get the current status of jobs submitted to the scheduler scancel 12345 - Cancel a job (in this case with the job ID 12345 ) We cover each of these commands in more detail below.","title":"Basic Slurm commands"},{"location":"user-guide/scheduler/#sinfo-information-on-resources","text":"sinfo is used to query information about available resources and partitions. Without any options, sinfo lists the status of all resources and partitions, e.g. auser@ln01:~> sinfo PARTITION AVAIL TIMELIMIT NODES STATE NODELIST standard up 1 -00:00:00 105 down* nid [ 001006 ,...,002014 ] standard up 1 -00:00:00 12 drain nid [ 001016 ,...,001969 ] standard up 1 -00:00:00 5 resv nid [ 001000 ,001002-001004,001114 ] standard up 1 -00:00:00 683 alloc nid [ 001001 ,...,001970-001991 ] standard up 1 -00:00:00 214 idle nid [ 001022 -001023,...,002015-002023 ] standard up 1 -00:00:00 2 down nid [ 001021 ,001050 ] Here we see the number of nodes in different states. For example, 683 nodes are allocated (running jobs), and 214 are idle (available to run jobs). Note that long lists of node IDs have been abbreviated with ... .","title":"sinfo: information on resources"},{"location":"user-guide/scheduler/#sbatch-submitting-jobs","text":"sbatch is used to submit a job script to the job submission system. The script will typically contain one or more srun commands to launch parallel tasks. When you submit the job, the scheduler provides the job ID, which is used to identify this job in other Slurm commands and when looking at resource usage in SAFE. auser@ln01:~> sbatch test-job.slurm Submitted batch job 12345","title":"sbatch: submitting jobs"},{"location":"user-guide/scheduler/#squeue-monitoring-jobs","text":"squeue without any options or arguments shows the current status of all jobs known to the scheduler. For example: auser@ln01:~> squeue will list all jobs on ARCHER2. The output of this is often overwhelmingly large. You can restrict the output to just your jobs by adding the -u $USER option: auser@ln01:~> squeue -u $USER","title":"squeue: monitoring jobs"},{"location":"user-guide/scheduler/#scancel-deleting-jobs","text":"scancel is used to delete a jobs from the scheduler. If the job is waiting to run it is simply cancelled, if it is a running job then it is stopped immediately. If you only want to cancel a specific job you need to provide the job ID of the job you wish to cancel/stop. For example: auser@ln01:~> scancel 12345 will cancel (if waiting) or stop (if running) the job with ID 12345 . scancel can take other options. For example, if you want to cancel all your pending (queued) jobs but leave the running jobs running, you could use: auser@ln01:~> scancel --state = PENDING --user = $USER","title":"scancel: deleting jobs"},{"location":"user-guide/scheduler/#resource-limits","text":"The ARCHER2 resource limits for any given job are covered by three separate attributes. The amount of primary resource you require, i.e., number of compute nodes. The partition that you want to use - this specifies the nodes that are eligible to run your job. The Quality of Service (QoS) that you want to use - this specifies the job limits that apply.","title":"Resource Limits"},{"location":"user-guide/scheduler/#primary-resource","text":"The primary resource you can request for your job is the compute node. Information The --exclusive option is enforced on ARCHER2 which means you will always have access to all of the memory on the compute node regardless of how many processes are actually running on the node. Note You will not generally have access to the full amount of memory resource on the the node as some is retained for running the operating system and other system processes.","title":"Primary resource"},{"location":"user-guide/scheduler/#partitions","text":"On ARCHER2, compute nodes are grouped into partitions. You will have to specify a partition using the --partition option in your Slurm submission script. The following table has a list of active partitions on ARCHER2. Full system Partition Description Max nodes available standard CPU nodes with AMD EPYC 7742 64-core processor \u00d7 2, 256/512 GB memory 5860 highmem CPU nodes with AMD EPYC 7742 64-core processor \u00d7 2, 512 GB memory 584 serial CPU nodes with AMD EPYC 7742 64-core processor \u00d7 2, 512 GB memory 2 Note The standard partition includes both the standard memory and high memory nodes but standard memory nodes are preferentially chosen for jobs where possible. To guarantee access to high memory nodes you should specify the highmem partition.","title":"Partitions"},{"location":"user-guide/scheduler/#quality-of-service-qos","text":"On ARCHER2, job limits are defined by the requested Quality of Service (QoS), as specified by the --qos Slurm directive. The following table lists the active QoS on ARCHER2. Full system QoS Max Nodes Per Job Max Walltime Jobs Queued Jobs Running Partition(s) Notes standard 1024 24 hrs 64 16 standard Maximum of 1024 nodes in use by any one user at any time highmem 256 24 hrs 16 16 highmem Maximum of 512 nodes in use by any one user at any time taskfarm 16 24 hrs 128 32 standard Maximum of 256 nodes in use by any one user at any time short 32 20 mins 16 4 standard long 64 48 hrs 16 16 standard Minimum walltime of 24 hrs, maximum 512 nodes in use by any one user at any time, maximum of 2048 nodes in use by QoS largescale 5860 12 hrs 8 1 standard Minimum job size of 1025 nodes lowpriority 1024 24 hrs 16 16 standard Jobs not charged but requires at least 1 CU in budget to use. serial 32 cores and/or 128 GB memory 24 hrs 32 4 serial Jobs not charged but requires at least 1 CU in budget to use. Maximum of 32 cores and/or 128 GB in use by any one user at any time. reservation Size of reservation Length of reservation No limit no limit standard You can find out the QoS that you can use by running the following command: Full system auser@ln01:~> sacctmgr show assoc user = $USER cluster = archer2 format = cluster,account,user,qos%50 Hint If you have needs which do not fit within the current QoS, please contact the Service Desk and we can discuss how to accommodate your requirements.","title":"Quality of Service (QoS)"},{"location":"user-guide/scheduler/#e-mail-notifications","text":"E-mail notifications from the scheduler are not currently available on ARCHER2.","title":"E-mail notifications"},{"location":"user-guide/scheduler/#priority","text":"Job priority on ARCHER2 depends on a number of different factors: The QoS your job has specified The amount of time you have been queuing for The number of nodes you have requested (job size) Your current fairshare factor Each of these factors is normalised to a value between 0 and 1, is multiplied with a weight and the resulting values combined to produce a priority for the job. The current job priority formula on Tursa is: Priority = [10000 * P(QoS)] + [500 * P(Age)] + [300 * P(Fairshare)] + [100 * P(size)] The priority factors are: P(QoS) - The QoS priority normalised to a value between 0 and 1. The maximum raw value is 10000 and the minimum is 0. Most QoS on ARCHER2 have a raw prioity of 500; the lowpriority QoS has a raw priority of 1. P(Age) - The priority based on the job age normalised to a value between 0 and 1. The maximum raw value is 14 days (where P(Age) = 1). P(Fairshare) - The fairshare priority normalised to a value between 0 and 1. Your fairshare priority is determined by a combination of your budget code fairshare value and your user fairshare value within that budget code. The more use that the budget code you are using has made of the system recently relative to other budget codes on the system, the lower the budget code fairshare value will be; and the more use you have made of the system recently relative to other users within your budget code, the lower your user fairshare value will be. The decay half life for fairshare on ARCHER2 is set to 14 days. More information on the Slurm fairshare algorithm . P(Size) - The priority based on the job size normalised to a value between 0 and 1. The maximum size is the total number of ARCHER2 compute nodes. You can view the priorities for current queued jobs on the system with the sprio command: auser@ln04:~> sprio -l JOBID PARTITION PRIORITY SITE AGE FAIRSHARE JOBSIZE QOS 828764 standard 1049 0 45 0 4 1000 828765 standard 1049 0 45 0 4 1000 828770 standard 1049 0 45 0 4 1000 828771 standard 1012 0 8 0 4 1000 828773 standard 1012 0 8 0 4 1000 828791 standard 1012 0 8 0 4 1000 828797 standard 1118 0 115 0 4 1000 828800 standard 1154 0 150 0 4 1000 828801 standard 1154 0 150 0 4 1000 828805 standard 1118 0 115 0 4 1000 828806 standard 1154 0 150 0 4 1000","title":"Priority"},{"location":"user-guide/scheduler/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"user-guide/scheduler/#slurm-error-messages","text":"An incorrect submission will cause Slurm to return an error. Some common problems are listed below, with a suggestion about the likely cause: sbatch: unrecognized option <text> One of your options is invalid or has a typo. man sbatch to help. error: Batch job submission failed: No partition specified or system default partition A --partition= option is missing. You must specify the partition (see the list above). This is most often --partition=standard . error: invalid partition specified: <partition> error: Batch job submission failed: Invalid partition name specified Check the partition exists and check the spelling is correct. error: Batch job submission failed: Invalid account or account/partition combination specified This probably means an invalid account has been given. Check the --account= options against valid accounts in SAFE. error: Batch job submission failed: Invalid qos specification A QoS option is either missing or invalid. Check the script has a --qos= option and that the option is a valid one from the table above. (Check the spelling of the QoS is correct.) error: Your job has no time specification (--time=)... Add an option of the form --time=hours:minutes:seconds to the submission script. E.g., --time=01:30:00 gives a time limit of 90 minutes. error: QOSMaxWallDurationPerJobLimit error: Batch job submission failed: Job violates accounting/QOS policy (job submit limit, user's size and/or time limits) The script has probably specified a time limit which is too long for the corresponding QoS. E.g., the time limit for the short QoS is 20 minutes.","title":"Slurm error messages"},{"location":"user-guide/scheduler/#slurm-job-state-codes","text":"The squeue command allows users to view information for jobs managed by Slurm. Jobs typically go through the following states: PENDING, RUNNING, COMPLETING, and COMPLETED. The first table provides a description of some job state codes. The second table provides a description of the reasons that cause a job to be in a state. Status Code Description PENDING PD Job is awaiting resource allocation. RUNNING R Job currently has an allocation. SUSPENDED S Job currently has an allocation. COMPLETING CG Job is in the process of completing. Some processes on some nodes may still be active. COMPLETED CD Job has terminated all processes on all nodes with an exit code of zero. TIMEOUT TO Job terminated upon reaching its time limit. STOPPED ST Job has an allocation, but execution has been stopped with SIGSTOP signal. CPUS have been retained by this job. OUT_OF_MEMORY OOM Job experienced out of memory error. FAILED F Job terminated with non-zero exit code or other failure condition. NODE_FAIL NF Job terminated due to failure of one or more allocated nodes. CANCELLED CA Job was explicitly cancelled by the user or system administrator. The job may or may not have been initiated. For a full list of see Job State Codes .","title":"Slurm job state codes"},{"location":"user-guide/scheduler/#slurm-queued-reasons","text":"Reason Description Priority One or more higher priority jobs exist for this partition or advanced reservation. Resources The job is waiting for resources to become available. BadConstraints The job's constraints can not be satisfied. BeginTime The job's earliest start time has not yet been reached. Dependency This job is waiting for a dependent job to complete. Licenses The job is waiting for a license. WaitingForScheduling No reason has been set for this job yet. Waiting for the scheduler to determine the appropriate reason. Prolog Its PrologSlurmctld program is still running. JobHeldAdmin The job is held by a system administrator. JobHeldUser The job is held by the user. JobLaunchFailure The job could not be launched. This may be due to a file system problem, invalid program name, etc. NonZeroExitCode The job terminated with a non-zero exit code. InvalidAccount The job's account is invalid. InvalidQOS The job's QOS is invalid. QOSUsageThreshold Required QOS threshold has been breached. QOSJobLimit The job's QOS has reached its maximum job count. QOSResourceLimit The job's QOS has reached some resource limit. QOSTimeLimit The job's QOS has reached its time limit. NodeDown A node required by the job is down. TimeLimit The job exhausted its time limit. ReqNodeNotAvail Some node specifically required by the job is not currently available. The node may currently be in use, reserved for another job, in an advanced reservation, DOWN, DRAINED, or not responding. Nodes which are DOWN, DRAINED, or not responding will be identified as part of the job's \"reason\" field as \"UnavailableNodes\". Such nodes will typically require the intervention of a system administrator to make available. For a full list of see Job Reasons .","title":"Slurm queued reasons"},{"location":"user-guide/scheduler/#output-from-slurm-jobs","text":"Slurm places standard output (STDOUT) and standard error (STDERR) for each job in the file slurm_<JobID>.out . This file appears in the job's working directory once your job starts running. Hint Output may be buffered - to enable live output, e.g. for monitoring job status, add --unbuffered to the srun command in your Slurm script.","title":"Output from Slurm jobs"},{"location":"user-guide/scheduler/#specifying-resources-in-job-scripts","text":"You specify the resources you require for your job using directives at the top of your job submission script using lines that start with the directive #SBATCH . Hint Most options provided using #SBATCH directives can also be specified as command line options to srun . If you do not specify any options, then the default for each option will be applied. As a minimum, all job submissions must specify the budget that they wish to charge the job too with the option: --account=<budgetID> your budget ID is usually something like t01 or t01-test . You can see which budget codes you can charge to in SAFE. Other common options that are used are: --time=<hh:mm:ss> the maximum walltime for your job. e.g. For a 6.5 hour walltime, you would use --time=6:30:0 . --job-name=<jobname> set a name for the job to help identify it in the queue To prevent the behaviour of batch scripts being dependent on the user environment at the point of submission, the option --export=none prevents the user environment from being exported to the batch system. Using the --export=none means that the behaviour of batch submissions should be repeatable. We strongly recommend its use.","title":"Specifying resources in job scripts"},{"location":"user-guide/scheduler/#additional-options-for-parallel-jobs","text":"In addition, parallel jobs will also need to specify how many nodes, parallel processes and threads they require. --nodes=<nodes> the number of nodes to use for the job. --tasks-per-node=<processes per node> the number of parallel processes (e.g. MPI ranks) per node. --cpus-per-task=1 if you are using parallel processes only with no threading then you should set the number of CPUs (cores) per parallel process to 1. Important: if you are using threading (e.g. with OpenMP) then you will need to change this option as described below. For parallel jobs that use threading (e.g. OpenMP), you will also need to change the --cpus-per-task option. --cpus-per-task=<threads per task> the number of threads per parallel process (e.g. number of OpenMP threads per MPI task for hybrid MPI/OpenMP jobs). Important: you must also set the OMP_NUM_THREADS environment variable if using OpenMP in your job. Note For parallel jobs, ARCHER2 operates in a node exclusive way. This means that you are assigned resources in the units of full compute nodes for your jobs ( i.e. 128 cores) and that no other user can share those compute nodes with you. Hence, the minimum amount of resource you can request for a parallel job is 1 node (or 128 cores).","title":"Additional options for parallel jobs"},{"location":"user-guide/scheduler/#options-for-jobs-on-the-data-analysis-nodes","text":"The data analysis nodes are shared between all users and can be used to run jobs that require small numbers of cores and/or access to an external network to transfer data. These jobs are often serial jobs that only require a single core. To run jobs on the data analysis node you require the following options: --partition=serial to select the data analysis nodes --qos=serial to select the data analysis QoS (see above for QoS limits) --ntasks=<number of cores> to select the number of cores you want to use in this job (up to the maximum defined in the QoS) --mem=<amount of memory> to select the amount of memory you require (up to the maximum defined in the QoS). More information on using the data analysis nodes (including example job submission scripts) can be found in the Data Analysis section of the User and Best Practice Guide.","title":"Options for jobs on the data analysis nodes"},{"location":"user-guide/scheduler/#srun-launching-parallel-jobs","text":"If you are running parallel jobs, your job submission script should contain one or more srun commands to launch the parallel executable across the compute nodes. In most cases you will want to add the options --distribution=block:block and --hint=nomultithread to your srun command to ensure you get the correct pinning of processes to cores on a compute node. Warning If you do not add the --distribution=block:block and --hint=nomultithread options to your srun command the default process placement may lead to a drop in performance for your jobs on ARCHER2. A brief explanation of these options: - --hint=nomultithread - do not use hyperthreads/SMP - --distribution=block:block - the first block means use a block distribution of processes across nodes (i.e. fill nodes before moving onto the next one) and the second block means use a block distribution of processes across NUMA regions within a node (i.e. fill a NUMA region before moving on to the next one).","title":"srun: Launching parallel jobs"},{"location":"user-guide/scheduler/#bolt-job-submission-script-creation-tool","text":"The bolt job submission script creation tool has been written by EPCC to simplify the process of writing job submission scripts for modern multicore architectures. Based on the options you supply, bolt will generate a job submission script that uses ARCHER2 in a reasonable way. MPI, OpenMP and hybrid MPI/OpenMP jobs are supported. Warning The tool will allow you to generate scripts for jobs that use the long QoS but you will need to manually modify the resulting script to change the QoS to long . If there are problems or errors in your job parameter specifications then bolt will print warnings or errors. However, bolt cannot detect all problems.","title":"bolt: Job submission script creation tool"},{"location":"user-guide/scheduler/#basic-usage","text":"The basic syntax for using bolt is: bolt -n [parallel tasks] -N [parallel tasks per node] -d [number of threads per task] \\ -t [wallclock time (h:m:s)] -o [script name] -j [job name] -A [project code] [arguments...] Example 1: to generate a job script to run an executable called my_prog.x for 24 hours using 8192 parallel (MPI) processes and 128 (MPI) processes per compute node you would use something like: bolt -n 8192 -N 128 -t 24:0:0 -o my_job.bolt -j my_job -A z01-budget my_prog.x arg1 arg2 (remember to substitute z01-budget for your actual budget code.) Example 2: to generate a job script to run an executable called my_prog.x for 3 hours using 2048 parallel (MPI) processes and 64 (MPI) processes per compute node (i.e. using half of the cores on a compute node), you would use: bolt -n 2048 -N 64 -t 3:0:0 -o my_job.bolt -j my_job -A z01-budget my_prog.x arg1 arg2 These examples generate the job script my_job.bolt with the correct options to run my_prog.x with command line arguments arg1 and arg2 . The project code against which the job will be charged is specified with the ' -A ' option. As usual, the job script is submitted as follows: sbatch my_job.bolt Hint If you do not specify the script name with the '-o' option then your script will be a file called a.bolt . Hint If you do not specify the number of parallel tasks then bolt will try to generate a serial job submission script (and throw an error on the ARCHER2 4 cabinet system as serial jobs are not supported). Hint If you do not specify a project code, bolt will use your default project code (set by your login account). Hint If you do not specify a job name, bolt will use either bolt_ser_job (for serial jobs) or bolt_par_job (for parallel jobs).","title":"Basic Usage"},{"location":"user-guide/scheduler/#further-help","text":"You can access further help on using bolt on ARCHER2 with the ' -h ' option: bolt -h A selection of other useful options are: -s Write and submit the job script rather than just writing the job script. -p Force the job to be parallel even if it only uses a single parallel task.","title":"Further help"},{"location":"user-guide/scheduler/#checkscript-job-submission-script-validation-tool","text":"The checkScript tool has been written to allow users to validate their job submission scripts before submitting their jobs. The tool will read your job submission script and try to identify errors, problems or inconsistencies. An example of the sort of output the tool can give would be: auser@ln01:/work/t01/t01/auser> checkScript submit.slurm =========================================================================== checkScript --------------------------------------------------------------------------- Copyright 2011-2020 EPCC, The University of Edinburgh This program comes with ABSOLUTELY NO WARRANTY. This is free software, and you are welcome to redistribute it under certain conditions. =========================================================================== Script details --------------- User: auser Script file: submit.slurm Directory: /work/t01/t01/auser (ok) Job name: test (ok) Partition: standard (ok) QoS: standard (ok) Combination: (ok) Requested resources ------------------- nodes = 3 (ok) tasks per node = 16 cpus per task = 8 cores per node = 128 (ok) OpenMP defined = True (ok) walltime = 1:0:0 (ok) CU Usage Estimate (if full job time used) ------------------------------------------ CU = 3.000 checkScript finished: 0 warning(s) and 0 error(s).","title":"checkScript job submission script validation tool"},{"location":"user-guide/scheduler/#checking-scripts-and-estimating-start-time-with-test-only","text":"sbatch --test-only validates the batch script and returns an estimate of when the job would be scheduled to run given the current job queue. Please note that it is just an estimate, the actual start time may differ as the job queue status when the start time was estimated may be different from the moment that the estimation took place. The job is not actually submitted. auser@ln01:~> sbatch --test-only submit.slurm sbatch: Job 1039497 to start at 2022-02-01T23:20:51 using 256 processors on nodes nid002836 in partition standard","title":"Checking scripts and estimating start time with --test-only"},{"location":"user-guide/scheduler/#example-job-submission-scripts","text":"A subset of example job submission scripts are included in full below. Examples are provided for both the full system and the 4-cabinet system.","title":"Example job submission scripts"},{"location":"user-guide/scheduler/#example-job-submission-script-for-mpi-parallel-job","text":"A simple MPI job submission script to submit a job using 4 compute nodes and 128 MPI ranks per node for 20 minutes would look like: Full system #!/bin/bash # Slurm job options (job-name, compute nodes, job time) #SBATCH --job-name=Example_MPI_Job #SBATCH --time=0:20:0 #SBATCH --nodes=4 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 # Replace [budget code] below with your budget code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Set the number of threads to 1 # This prevents any threaded system libraries from automatically # using threading. export OMP_NUM_THREADS = 1 # Launch the parallel job # Using 512 MPI processes and 128 MPI processes per node # srun picks up the distribution from the sbatch options srun --distribution = block:block --hint = nomultithread ./my_mpi_executable.x This will run your executable \"my_mpi_executable.x\" in parallel on 512 MPI processes using 4 nodes (128 cores per node, i.e. not using hyper-threading). Slurm will allocate 4 nodes to your job and srun will place 128 MPI processes on each node (one per physical core). See above for a more detailed discussion of the different sbatch options","title":"Example: job submission script for MPI parallel job"},{"location":"user-guide/scheduler/#example-job-submission-script-for-mpiopenmp-mixed-mode-parallel-job","text":"Mixed mode codes that use both MPI (or another distributed memory parallel model) and OpenMP should take care to ensure that the shared memory portion of the process/thread placement does not span more than one NUMA region. Nodes on ARCHER2 are made up of two sockets each containing 4 NUMA regions of 16 cores, i.e. there are 8 NUMA regions in total. Therefore the total number of threads should ideally not be greater than 16, and also needs to be a factor of 16. Sensible choices for the number of threads are therefore 1 (single-threaded), 2, 4, 8, and 16. More information about using OpenMP and MPI+OpenMP can be found in the Tuning chapter. To ensure correct placement of MPI processes the number of cpus-per-task needs to match the number of OpenMP threads, and the number of tasks-per-node should be set to ensure the entire node is filled with MPI tasks. In the example below, we are using 4 nodes for 6 hours. There are 32 MPI processes in total (8 MPI processes per node) and 16 OpenMP threads per MPI process. This results in all 128 physical cores per node being used. Hint Note the use of the export OMP_PLACES=cores environment option to generate the correct thread pinning. Full system #!/bin/bash # Slurm job options (job-name, compute nodes, job time) #SBATCH --job-name=Example_MPI_Job #SBATCH --time=0:20:0 #SBATCH --nodes=4 #SBATCH --tasks-per-node=8 #SBATCH --cpus-per-task=16 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Set the number of threads to 16 and specify placement # There are 16 OpenMP threads per MPI process # We want one thread per physical core export OMP_NUM_THREADS = 16 export OMP_PLACES = cores # Launch the parallel job # Using 32 MPI processes # 8 MPI processes per node # 16 OpenMP threads per MPI process # Additional srun options to pin one thread per physical core srun --hint = nomultithread --distribution = block:block ./my_mixed_executable.x arg1 arg2","title":"Example: job submission script for MPI+OpenMP (mixed mode) parallel job"},{"location":"user-guide/scheduler/#job-arrays","text":"The Slurm job scheduling system offers the job array concept, for running collections of almost-identical jobs. For example, running the same program several times with different arguments or input data. Each job in a job array is called a subjob . The subjobs of a job array can be submitted and queried as a unit, making it easier and cleaner to handle the full set, compared to individual jobs. All subjobs in a job array are started by running the same job script. The job script also contains information on the number of jobs to be started, and Slurm provides a subjob index which can be passed to the individual subjobs or used to select the input data per subjob.","title":"Job arrays"},{"location":"user-guide/scheduler/#job-script-for-a-job-array","text":"As an example, the following script runs 56 subjobs, with the subjob index as the only argument to the executable. Each subjob requests a single node and uses all 128 cores on the node by placing 1 MPI process per core and specifies 4 hours maximum runtime per subjob: Full system #!/bin/bash # Slurm job options (job-name, compute nodes, job time) #SBATCH --job-name=Example_Array_Job #SBATCH --time=04:00:00 #SBATCH --nodes=1 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 #SBATCH --array=0-55 # Replace [budget code] below with your budget code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Set the number of threads to 1 # This prevents any threaded system libraries from automatically # using threading. export OMP_NUM_THREADS = 1 srun --distribution = block:block --hint = nomultithread /path/to/exe $SLURM_ARRAY_TASK_ID","title":"Job script for a job array"},{"location":"user-guide/scheduler/#submitting-a-job-array","text":"Job arrays are submitted using sbatch in the same way as for standard jobs: sbatch job_script.pbs","title":"Submitting a job array"},{"location":"user-guide/scheduler/#job-chaining","text":"Job dependencies can be used to construct complex pipelines or chain together long simulations requiring multiple steps. Hint The --parsable option to sbatch can simplify working with job dependencies. It returns the job ID in a format that can be used as the input to other commands. For example: jobid=$(sbatch --parsable first_job.sh) sbatch --dependency=afterok:$jobid second_job.sh or for a longer chain: jobid1=$(sbatch --parsable first_job.sh) jobid2=$(sbatch --parsable --dependency=afterok:$jobid1 second_job.sh) jobid3=$(sbatch --parsable --dependency=afterok:$jobid1 third_job.sh) sbatch --dependency=afterok:$jobid2,afterok:$jobid3 last_job.sh","title":"Job chaining"},{"location":"user-guide/scheduler/#using-multiple-srun-commands-in-a-single-job-script","text":"You can use multiple srun commands within in a Slurm job submission script to allow you to use the resource requested more flexibly. For example, you could run a collection of smaller jobs within the requested resources or you could even subdivide nodes if your individual calculations do not scale up to use all 128 cores on a node. In this guide we will cover two scenarios: Subdividing the job into multiple full-node or multi-node subjobs, e.g. requesting 100 nodes and running 100, 1-node subjobs or 50, 2-node subjobs. Subdividing the job into multiple subjobs that each use a fraction of a node, e.g. requesting 2 nodes and running 256, 1-core subjobs or 16, 16-core subjobs.","title":"Using multiple srun commands in a single job script"},{"location":"user-guide/scheduler/#running-multiple-full-node-subjobs-within-a-larger-job","text":"When subdivding a larger job into smaller subjobs you typically need to overwrite the --nodes option to srun and add the --ntasks option to ensure that each subjob runs on the correct number of nodes and that subjobs are placed correctly onto separate nodes. For example, we will show how to request 100 nodes and then run 100 separate 1-node jobs, each of which use 128 MPI processes and which run on a different compute node. We start by showing the job script that would achieve this and then explain how this works and the options used. In our case, we will run 100 copies of the xthi program that prints the process placement on the node it is running on. Full system #!/bin/bash # Slurm job options (job-name, compute nodes, job time) #SBATCH --job-name=multi_xthi #SBATCH --time=0:20:0 #SBATCH --nodes=100 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 # Replace [budget code] below with your budget code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Load the xthi module module load xthi # Set the number of threads to 1 # This prevents any threaded system libraries from automatically # using threading. export OMP_NUM_THREADS = 1 # Loop over 100 subjobs starting each of them on a separate node for i in $( seq 1 100 ) do # Launch this subjob on 1 node, note nodes and ntasks options and & to place subjob in the background srun --nodes = 1 --ntasks = 128 --distribution = block:block --hint = nomultithread xthi > placement ${ i } .txt & done # Wait for all background subjobs to finish wait Key points from the example job script: The #SBATCH options select 100 full nodes in the usual way. Each subjob srun command sets the following: --nodes=1 We need override this setting from the main job so that each subjob only uses 1 node --ntasks=128 For normal jobs, the number of parallel tasks (MPI processes) is calculated from the number of nodes you request and the number of tasks per node. We need to explicitly tell srun how many we require for this subjob. --distribution=block:block --hint=nomultithread These options ensure correct placement of processes within the compute nodes. & Each subjob srun command ends with an ampersand to place the process in the background and move on to the next loop iteration (and subjob submission). Without this, the script would wait for this subjob to complete before moving on to submit the next. Finally, there is the wait command to tell the script to wait for all the background subjobs to complete before exiting. If we did not have this in place, the script would exit as soon as the last subjob was submitted and kill all running subjobs.","title":"Running multiple, full-node subjobs within a larger job"},{"location":"user-guide/scheduler/#running-multiple-subjobs-that-each-use-a-fraction-of-a-node","text":"As the ARCHER2 nodes contain a large number of cores (128 per node) it may sometimes be useful to be able to run multiple executables on a single node. For example, you may want to run 128 copies of a serial executable or Python script; or, you may want to run multiple copies of parallel executables that use fewer than 128 cores each. This use model is possible using multiple srun commands in a job script on ARCHER2 Note You can never share a compute node with another user. Although you can use srun to place multiple copies of an executable or script on a compute node, you still have exclusive use of that node. The minimum amount of resources you can reserve for your use on ARCHER2 is a single node. When using srun to place multiple executables or scripts on a compute node you must be aware of a few things: The srun command must specify any Slurm options that differ in value from those specified to sbatch . This typically means that you need to specify the --nodes , --ntasks and --tasks-per-node options to srun . You will need to include the --exact flag to your srun command. With this flag on, Slurm will ensure that the resources you request are assigned to your subjob. Furthermore, if the resources are not currently available, Slurm will output a message letting you know that this is the case and stall the launch of this subjob until enough of your previous subjobs have completed to free up the resources for this subjob. You will need to define the memory required by each subjob with the --mem=<amount of memory> flag. The amount of memory is given in MiB by default but other units can be specified. If you do not know how much memory to specify, we recommend that you specify 1500M (1,500 MiB) per core being used. You will need to place each srun command into the background and then use the wait command at the end of the submission script to make sure it does not exit before the commands are complete. If you want to use more than one node in the job and use multiple srun per node (e.g. 256 single core processes across 2 nodes) then you need to pass the node ID to the srun commands otherwise Slurm will oversubscribe cores on the first node. Below, we provide four examples or running multiple subjobs in a node: one that runs 128 serial processes across a single node; one that runs 8 subjobs each of which use 8 MPI processes with 2 OpenMP threads per MPI process; one that runs four inhomogeneous jobs, each of which requires a different number of MPI processes and OpenMP threads per process; and one that runs 256 serial processes across two nodes.","title":"Running multiple subjobs that each use a fraction of a node"},{"location":"user-guide/scheduler/#example-1-128-serial-tasks-running-on-a-single-node","text":"For our first example, we will run 128 single-core copies of the xthi program (which prints process/thread placement) on a single ARCHER2 compute node with each copy of xthi pinned to a different core. The job submission script for this example would look like: Full system #!/bin/bash # Slurm job options (job-name, compute nodes, job time) #SBATCH --job-name=MultiSerialOnCompute #SBATCH --time=0:10:0 #SBATCH --nodes=1 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 # Replace [budget code] below with your budget code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Make xthi available module load xthi # Set the number of threads to 1 # This prevents any threaded system libraries from automatically # using threading. export OMP_NUM_THREADS = 1 # Loop over 128 subjobs pinning each to a different core for i in $( seq 1 128 ) do # Launch subjob overriding job settings as required and in the background # Make sure to change the amount specified by the `--mem=` flag to the amount # of memory required. The amount of memory is given in MiB by default but other # units can be specified. If you do not know how much memory to specify, we # recommend that you specify `--mem=1500M` (1,500 MiB). srun --nodes = 1 --ntasks = 1 --tasks-per-node = 1 \\ --exact --mem = 1500M xthi > placement ${ i } .txt & done # Wait for all subjobs to finish wait","title":"Example 1: 128 serial tasks running on a single node"},{"location":"user-guide/scheduler/#example-2-8-subjobs-on-1-node-each-with-8-mpi-processes-and-2-openmp-threads-per-process","text":"For our second example, we will run 8 subjobs, each running the xthi program (which prints process/thread placement) across 1 node. Each subjob will use 8 MPI processes and 2 OpenMP threads per process. The job submission script for this example would look like: Full system #!/bin/bash # Slurm job options (job-name, compute nodes, job time) #SBATCH --job-name=MultiParallelOnCompute #SBATCH --time=0:10:0 #SBATCH --nodes=1 #SBATCH --tasks-per-node=64 #SBATCH --cpus-per-task=2 # Replace [budget code] below with your budget code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Make xthi available module load xthi # Set the number of threads to 2 as required by all subjobs export OMP_NUM_THREADS = 2 # Loop over 8 subjobs for i in $( seq 1 8 ) do echo $j $i # Launch subjob overriding job settings as required and in the background # Make sure to change the amount specified by the `--mem=` flag to the amount # of memory required. The amount of memory is given in MiB by default but other # units can be specified. If you do not know how much memory to specify, we # recommend that you specify `--mem=12500M` (12,500 MiB). srun --nodes = 1 --ntasks = 8 --tasks-per-node = 8 --cpus-per-task = 2 \\ --exact --mem = 12500M xthi > placement ${ i } .txt & done # Wait for all subjobs to finish wait","title":"Example 2: 8 subjobs on 1 node each with 8 MPI processes and 2 OpenMP threads per process"},{"location":"user-guide/scheduler/#example-3-running-inhomogeneous-subjobs-on-one-node","text":"For our third example, we will run 4 subjobs, each running the xthi program (which prints process/thread placement) across 1 node. Our subjobs will each run with a different number of MPI processes and OpenMP threads. We will run: one job with 64 MPI processes and 1 OpenMP process per thread; one job with 16 MPI processes and 2 threads per process; one job with 4 MPI processes and 4 OpenMP threads per job; and, one job with 1 MPI process and 16 OpenMP threads per job. To be able to change the number of MPI processes and OpenMP threads per process, we will need to forgo using the #SBATCH --tasks-per-node and the #SBATCH cpus-per-task commands -- if you set these Slurm will not let you alter the OMP_NUM_THREADS variable and you will not be able to change the number of OpenMP threads per process between each job. Before each srun command, you will need to define the number of OpenMP threads per process you want by changing the OMP_NUM_THREADS variable. Furthermore, for each srun command, you will need to set the --ntasks flag to equal the number of MPI processes you want to use. You will also need to set the --cpus-per-task flag to equal the number of OpenMP threads per process you want to use. Full system #!/bin/bash # Slurm job options (job-name, compute nodes, job time) #SBATCH --job-name=MultiParallelOnCompute #SBATCH --time=0:10:0 #SBATCH --nodes=1 #SBATCH --hint=nomultithread #SBATCH --distribution=block:block # Replace [budget code] below with your budget code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Make xthi available module load xthi # Set the number of threads to value required by the first job export OMP_NUM_THREADS = 1 srun --ntasks = 64 --cpus-per-task = ${ OMP_NUM_THREADS } \\ --exact --mem = 12500M xthi > placement ${ OMP_NUM_THREADS } .txt & # Set the number of threads to the value required by the second job export OMP_NUM_THREADS = 2 srun --ntasks = 16 --cpus-per-task = ${ OMP_NUM_THREADS } \\ --exact --mem = 12500M xthi > placement ${ OMP_NUM_THREADS } .txt & # Set the number of threads to the value required by the second job export OMP_NUM_THREADS = 4 srun --ntasks = 4 --cpus-per-task = ${ OMP_NUM_THREADS } \\ --exact --mem = 12500M xthi > placement ${ OMP_NUM_THREADS } .txt & # Set the number of threads to the value required by the second job export OMP_NUM_THREADS = 16 srun --ntasks = 1 --cpus-per-task = ${ OMP_NUM_THREADS } \\ --exact --mem = 12500M xthi > placement ${ OMP_NUM_THREADS } .txt & # Wait for all subjobs to finish wait","title":"Example 3: Running inhomogeneous subjobs on one node"},{"location":"user-guide/scheduler/#example-4-256-serial-tasks-running-across-two-nodes","text":"For our fourth example, we will run 256 single-core copies of the xthi program (which prints process/thread placement) across two ARCHER2 compute nodes with each copy of xthi pinned to a different core. We will illustrate a mechanism for getting the node IDs to pass to srun as this is required to ensure that the individual subjobs are assigned to the correct node. This mechanism uses the scontrol command to turn the nodelist from sbatch into a format we can use as input to srun . The job submission script for this example would look like: Full system #!/bin/bash # Slurm job options (job-name, compute nodes, job time) #SBATCH --job-name=MultiSerialOnComputes #SBATCH --time=0:10:0 #SBATCH --nodes=2 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 # Replace [budget code] below with your budget code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Make xthi available module load xthi # Set the number of threads to 1 # This prevents any threaded system libraries from automatically # using threading. export OMP_NUM_THREADS = 1 # Get a list of the nodes assigned to this job in a format we can use. # scontrol converts the condensed node IDs in the sbatch environment # variable into a list of full node IDs that we can use with srun to # ensure the subjobs are placed on the correct node. e.g. this converts # \"nid[001234,002345]\" to \"nid001234 nid002345\" nodelist = $( scontrol show hostnames $SLURM_JOB_NODELIST ) # Loop over the nodes assigned to the job for nodeid in $nodelist do # Loop over 128 subjobs on each node pinning each to a different core for i in $( seq 1 128 ) do # Launch subjob overriding job settings as required and in the background # Make sure to change the amount specified by the `--mem=` flag to the amount # of memory required. The amount of memory is given in MiB by default but other # units can be specified. If you do not know how much memory to specify, we # recommend that you specify `--mem=1500M` (1,500 MiB). srun --nodelist = ${ nodeid } --nodes = 1 --ntasks = 1 --tasks-per-node = 1 \\ --exact --mem = 1500M xthi > placement_ ${ nodeid } _ ${ i } .txt & done done # Wait for all subjobs to finish wait","title":"Example 4: 256 serial tasks running across two nodes"},{"location":"user-guide/scheduler/#process-placement","text":"There are many occasions where you may want to control (usually, MPI) process placement and change it from the default, for example: You may want to place processes to NUMA regions in a round-robin way rather than the default sequential placement You may be using fewer than 128 processes per node and want to ensure that processes are placed evenly across NUMA regions (16-core blocks) or core complexes (4-core blocks that share an L3 cache) There are a number of different methods for defining process placement, below we cover two different options: using Slurm options and using the MPICH_RANK_REORDER_METHOD environment variable. Most users will likely use the Slurm options approach.","title":"Process placement"},{"location":"user-guide/scheduler/#default-process-placement","text":"The default is to place processes sequentially on nodes until the maximum number of tasks is reached. You can use the xthi program to verify this for MPI process placement: auser@ln04:/work/t01/t01/auser> salloc --nodes=2 --tasks-per-node=128 \\ --cpus-per-task=1 --time=0:10:0 --partition=standard --qos=short \\ --account=[your account] salloc: Pending job allocation 1170365 salloc: job 1170365 queued and waiting for resources salloc: job 1170365 has been allocated resources salloc: Granted job allocation 1170365 salloc: Waiting for resource configuration salloc: Nodes nid[002526-002527] are ready for job auser@ln04:/work/t01/t01/auser> module load xthi auser@ln04:/work/t01/t01/auser> export OMP_NUM_THREADS=1 auser@ln04:/work/t01/t01/auser> srun --distribution=block:block --hint=nomultithread xthi Node summary for 2 nodes: Node 0, hostname nid002526, mpi 128, omp 1, executable xthi Node 1, hostname nid002527, mpi 128, omp 1, executable xthi MPI summary: 256 ranks Node 0, rank 0, thread 0, (affinity = 0) Node 0, rank 1, thread 0, (affinity = 1) Node 0, rank 2, thread 0, (affinity = 2) Node 0, rank 3, thread 0, (affinity = 3) ...output trimmed... Node 0, rank 124, thread 0, (affinity = 124) Node 0, rank 125, thread 0, (affinity = 125) Node 0, rank 126, thread 0, (affinity = 126) Node 0, rank 127, thread 0, (affinity = 127) Node 1, rank 128, thread 0, (affinity = 0) Node 1, rank 129, thread 0, (affinity = 1) Node 1, rank 130, thread 0, (affinity = 2) Node 1, rank 131, thread 0, (affinity = 3) ...output trimmed... Note For MPI programs on ARCHER2, each rank corresponds to a process . Important To get good performance out of MPI collective operations, MPI processes should be placed sequentially on cores as in the default placement described above.","title":"Default process placement"},{"location":"user-guide/scheduler/#setting-process-placement-using-slurm-options","text":"","title":"Setting process placement using Slurm options"},{"location":"user-guide/scheduler/#for-underpopulation-of-nodes-with-processes","text":"When you are using fewer processes than cores on compute nodes (i.e. < 128 processes per node) the basic Slurm options (usually supplied in your script as options to sbatch ) for process placement are: --ntasks-per-node=X Place X processes on each node --cpus-per-task=Y Set a stride of Y cores between each placed process In addition, the following options are added to your srun commands in your job submission script: --hint=nomultithread Only use physical cores (avoids use of SMT/hyperthreads) --distribution=block:block Allocate processes to cores in a sequential fashion For example, to place 32 processes per node and have 1 process per 4-core block (corresponding to a core complex that shares an L3 cache), you would set: --ntasks-per-node=32 Place 32 processes on each node --cpus-per-task=4 Set a stride of 4 cores between each placed process Here is the output from xthi : auser@ln04:/work/t01/t01/auser> salloc --nodes=2 --tasks-per-node=32 \\ --cpus-per-task=4 --time=0:10:0 --partition=standard --qos=short \\ --account=[your account] salloc: Pending job allocation 1170383 salloc: job 1170383 queued and waiting for resources salloc: job 1170383 has been allocated resources salloc: Granted job allocation 1170383 salloc: Waiting for resource configuration salloc: Nodes nid[002526-002527] are ready for job auser@ln04:/work/t01/t01/auser> module load xthi auser@ln04:/work/t01/t01/auser> export OMP_NUM_THREADS=1 auser@ln04:/work/t01/t01/auser> srun --distribution=block:block --hint=nomultithread xthi Node summary for 2 nodes: Node 0, hostname nid002526, mpi 32, omp 1, executable xthi Node 1, hostname nid002527, mpi 32, omp 1, executable xthi MPI summary: 64 ranks Node 0, rank 0, thread 0, (affinity = 0-3) Node 0, rank 1, thread 0, (affinity = 4-7) Node 0, rank 2, thread 0, (affinity = 8-11) Node 0, rank 3, thread 0, (affinity = 12-15) Node 0, rank 4, thread 0, (affinity = 16-19) Node 0, rank 5, thread 0, (affinity = 20-23) Node 0, rank 6, thread 0, (affinity = 24-27) Node 0, rank 7, thread 0, (affinity = 28-31) Node 0, rank 8, thread 0, (affinity = 32-35) Node 0, rank 9, thread 0, (affinity = 36-39) Node 0, rank 10, thread 0, (affinity = 40-43) Node 0, rank 11, thread 0, (affinity = 44-47) Node 0, rank 12, thread 0, (affinity = 48-51) Node 0, rank 13, thread 0, (affinity = 52-55) Node 0, rank 14, thread 0, (affinity = 56-59) Node 0, rank 15, thread 0, (affinity = 60-63) Node 0, rank 16, thread 0, (affinity = 64-67) Node 0, rank 17, thread 0, (affinity = 68-71) Node 0, rank 18, thread 0, (affinity = 72-75) Node 0, rank 19, thread 0, (affinity = 76-79) Node 0, rank 20, thread 0, (affinity = 80-83) Node 0, rank 21, thread 0, (affinity = 84-87) Node 0, rank 22, thread 0, (affinity = 88-91) Node 0, rank 23, thread 0, (affinity = 92-95) Node 0, rank 24, thread 0, (affinity = 96-99) Node 0, rank 25, thread 0, (affinity = 100-103) Node 0, rank 26, thread 0, (affinity = 104-107) Node 0, rank 27, thread 0, (affinity = 108-111) Node 0, rank 28, thread 0, (affinity = 112-115) Node 0, rank 29, thread 0, (affinity = 116-119) Node 0, rank 30, thread 0, (affinity = 120-123) Node 0, rank 31, thread 0, (affinity = 124-127) Node 1, rank 32, thread 0, (affinity = 0-3) Node 1, rank 33, thread 0, (affinity = 4-7) Node 1, rank 34, thread 0, (affinity = 8-11) Node 1, rank 35, thread 0, (affinity = 12-15) Node 1, rank 36, thread 0, (affinity = 16-19) Node 1, rank 37, thread 0, (affinity = 20-23) Node 1, rank 38, thread 0, (affinity = 24-27) Node 1, rank 39, thread 0, (affinity = 28-31) Node 1, rank 40, thread 0, (affinity = 32-35) Node 1, rank 41, thread 0, (affinity = 36-39) Node 1, rank 42, thread 0, (affinity = 40-43) Node 1, rank 43, thread 0, (affinity = 44-47) Node 1, rank 44, thread 0, (affinity = 48-51) Node 1, rank 45, thread 0, (affinity = 52-55) Node 1, rank 46, thread 0, (affinity = 56-59) Node 1, rank 47, thread 0, (affinity = 60-63) Node 1, rank 48, thread 0, (affinity = 64-67) Node 1, rank 49, thread 0, (affinity = 68-71) Node 1, rank 50, thread 0, (affinity = 72-75) Node 1, rank 51, thread 0, (affinity = 76-79) Node 1, rank 52, thread 0, (affinity = 80-83) Node 1, rank 53, thread 0, (affinity = 84-87) Node 1, rank 54, thread 0, (affinity = 88-91) Node 1, rank 55, thread 0, (affinity = 92-95) Node 1, rank 56, thread 0, (affinity = 96-99) Node 1, rank 57, thread 0, (affinity = 100-103) Node 1, rank 58, thread 0, (affinity = 104-107) Node 1, rank 59, thread 0, (affinity = 108-111) Node 1, rank 60, thread 0, (affinity = 112-115) Node 1, rank 61, thread 0, (affinity = 116-119) Node 1, rank 62, thread 0, (affinity = 120-123) Node 1, rank 63, thread 0, (affinity = 124-127) Tip You usually only want to use physical cores on ARCHER2, so ( tasks-per-node ) \u00d7 ( cpus-per-task ) should generally be equal to 128.","title":"For underpopulation of nodes with processes"},{"location":"user-guide/scheduler/#full-node-population-with-non-sequential-process-placement","text":"If you want to change the order processes are placed on nodes and cores using Slurm options then you should use the --distribution option to srun to change this. For example, to place processes sequentially on nodes but round-robin on the 16-core NUMA regions in a single node, you would use the --distribution=block:cyclic option to srun . This type of process placement can be beneficial when a code is memory bound. auser@ln04:/work/t01/t01/auser> salloc --nodes=2 --tasks-per-node=128 \\ --cpus-per-task=1 --time=0:10:0 --partition=standard --qos=short \\ --account=[your account] salloc: Pending job allocation 1170594 salloc: job 1170594 queued and waiting for resources salloc: job 1170594 has been allocated resources salloc: Granted job allocation 1170594 salloc: Waiting for resource configuration salloc: Nodes nid[002616,002621] are ready for job auser@ln04:/work/t01/t01/auser> module load xthi auser@ln04:/work/t01/t01/auser> export OMP_NUM_THREADS=1 auser@ln04:/work/t01/t01/auser> srun --distribution=block:cyclic --hint=nomultithread xthi Node summary for 2 nodes: Node 0, hostname nid002616, mpi 128, omp 1, executable xthi Node 1, hostname nid002621, mpi 128, omp 1, executable xthi MPI summary: 256 ranks Node 0, rank 0, thread 0, (affinity = 0) Node 0, rank 1, thread 0, (affinity = 16) Node 0, rank 2, thread 0, (affinity = 32) Node 0, rank 3, thread 0, (affinity = 48) Node 0, rank 4, thread 0, (affinity = 64) Node 0, rank 5, thread 0, (affinity = 80) Node 0, rank 6, thread 0, (affinity = 96) Node 0, rank 7, thread 0, (affinity = 112) Node 0, rank 8, thread 0, (affinity = 1) Node 0, rank 9, thread 0, (affinity = 17) Node 0, rank 10, thread 0, (affinity = 33) Node 0, rank 11, thread 0, (affinity = 49) Node 0, rank 12, thread 0, (affinity = 65) Node 0, rank 13, thread 0, (affinity = 81) Node 0, rank 14, thread 0, (affinity = 97) Node 0, rank 15, thread 0, (affinity = 113 ...output trimmed... Node 0, rank 120, thread 0, (affinity = 15) Node 0, rank 121, thread 0, (affinity = 31) Node 0, rank 122, thread 0, (affinity = 47) Node 0, rank 123, thread 0, (affinity = 63) Node 0, rank 124, thread 0, (affinity = 79) Node 0, rank 125, thread 0, (affinity = 95) Node 0, rank 126, thread 0, (affinity = 111) Node 0, rank 127, thread 0, (affinity = 127) Node 1, rank 128, thread 0, (affinity = 0) Node 1, rank 129, thread 0, (affinity = 16) Node 1, rank 130, thread 0, (affinity = 32) Node 1, rank 131, thread 0, (affinity = 48) Node 1, rank 132, thread 0, (affinity = 64) Node 1, rank 133, thread 0, (affinity = 80) Node 1, rank 134, thread 0, (affinity = 96) Node 1, rank 135, thread 0, (affinity = 112) ...output trimmed... If you wish to place processes round robin on both nodes and 16-core NUMA regions within in a node you would use --distribution=cyclic:cyclic : auser@ln04:/work/t01/t01/auser> salloc --nodes=2 --tasks-per-node=128 \\ --cpus-per-task=1 --time=0:10:0 --partition=standard --qos=short \\ --account=[your account] salloc: Pending job allocation 1170594 salloc: job 1170594 queued and waiting for resources salloc: job 1170594 has been allocated resources salloc: Granted job allocation 1170594 salloc: Waiting for resource configuration salloc: Nodes nid[002616,002621] are ready for job auser@ln04:/work/t01/t01/auser> module load xthi auser@ln04:/work/t01/t01/auser> export OMP_NUM_THREADS=1 auser@ln04:/work/t01/t01/auser> srun --distribution=cyclic:cyclic --hint=nomultithread xthi Node summary for 2 nodes: Node 0, hostname nid002616, mpi 128, omp 1, executable xthi Node 1, hostname nid002621, mpi 128, omp 1, executable xthi MPI summary: 256 ranks Node 0, rank 0, thread 0, (affinity = 0) Node 0, rank 2, thread 0, (affinity = 16) Node 0, rank 4, thread 0, (affinity = 32) Node 0, rank 6, thread 0, (affinity = 48) Node 0, rank 8, thread 0, (affinity = 64) Node 0, rank 10, thread 0, (affinity = 80) Node 0, rank 12, thread 0, (affinity = 96) Node 0, rank 14, thread 0, (affinity = 112) Node 0, rank 16, thread 0, (affinity = 1) Node 0, rank 18, thread 0, (affinity = 17) Node 0, rank 20, thread 0, (affinity = 33) Node 0, rank 22, thread 0, (affinity = 49) Node 0, rank 24, thread 0, (affinity = 65) Node 0, rank 26, thread 0, (affinity = 81) Node 0, rank 28, thread 0, (affinity = 97) Node 0, rank 30, thread 0, (affinity = 113) ...output trimmed... Node 1, rank 1, thread 0, (affinity = 0) Node 1, rank 3, thread 0, (affinity = 16) Node 1, rank 5, thread 0, (affinity = 32) Node 1, rank 7, thread 0, (affinity = 48) Node 1, rank 9, thread 0, (affinity = 64) Node 1, rank 11, thread 0, (affinity = 80) Node 1, rank 13, thread 0, (affinity = 96) Node 1, rank 15, thread 0, (affinity = 112) Node 1, rank 17, thread 0, (affinity = 1) Node 1, rank 19, thread 0, (affinity = 17) Node 1, rank 21, thread 0, (affinity = 33) Node 1, rank 23, thread 0, (affinity = 49) Node 1, rank 25, thread 0, (affinity = 65) Node 1, rank 27, thread 0, (affinity = 81) Node 1, rank 29, thread 0, (affinity = 97) Node 1, rank 31, thread 0, (affinity = 113) ...output trimmed... Remember, MPI collective performance is generally much worse if processes are not placed sequentially on a node (so adjacent MPI ranks are as close to each other as possible). This is the reason that the default recommended placement on ARCHER2 is sequential rather than round-robin.","title":"Full node population with non-sequential process placement"},{"location":"user-guide/scheduler/#mpich_rank_reorder_method-for-mpi-process-placement","text":"The MPICH_RANK_REORDER_METHOD environment variable can also be used to specify other types of MPI task placement. For example, setting it to \"0\" results in a round-robin placement on both nodes and NUMA regions in a node (equivalent to the --distribution=cyclic:cyclic option to srun ). Note, we do not specify the --distribution option to srun in this case as the environment variable is controlling placement: salloc --nodes=8 --tasks-per-node=2 --cpus-per-task=1 --time=0:10:0 --account=t01 salloc: Granted job allocation 24236 salloc: Waiting for resource configuration salloc: Nodes cn13 are ready for job module load xthi export OMP_NUM_THREADS=1 export MPICH_RANK_REORDER_METHOD=0 srun --hint=nomultithread xthi Node summary for 2 nodes: Node 0, hostname nid002616, mpi 128, omp 1, executable xthi Node 1, hostname nid002621, mpi 128, omp 1, executable xthi MPI summary: 256 ranks Node 0, rank 0, thread 0, (affinity = 0) Node 0, rank 2, thread 0, (affinity = 16) Node 0, rank 4, thread 0, (affinity = 32) Node 0, rank 6, thread 0, (affinity = 48) Node 0, rank 8, thread 0, (affinity = 64) Node 0, rank 10, thread 0, (affinity = 80) Node 0, rank 12, thread 0, (affinity = 96) Node 0, rank 14, thread 0, (affinity = 112) Node 0, rank 16, thread 0, (affinity = 1) Node 0, rank 18, thread 0, (affinity = 17) Node 0, rank 20, thread 0, (affinity = 33) Node 0, rank 22, thread 0, (affinity = 49) Node 0, rank 24, thread 0, (affinity = 65) Node 0, rank 26, thread 0, (affinity = 81) Node 0, rank 28, thread 0, (affinity = 97) Node 0, rank 30, thread 0, (affinity = 113) ...output trimmed... There are other modes available with the MPICH_RANK_REORDER_METHOD environment variable, including one which lets the user provide a file called MPICH_RANK_ORDER which contains a list of each task's placement on each node. These options are described in detail in the intro_mpi man page.","title":"MPICH_RANK_REORDER_METHOD for MPI process placement"},{"location":"user-guide/scheduler/#grid_order","text":"For MPI applications which perform a large amount of nearest-neighbor communication, e.g., stencil-based applications on structured grids, HPE provide a tool in the perftools-base module (Loaded by default for all users) called grid_order which can generate a MPICH_RANK_ORDER file automatically by taking as parameters the dimensions of the grid, core count, etc. For example, to place 256 MPI parameters in row-major order on a Cartesian grid of size $(8, 8, 4)$, using 128 cores per node: grid_order -R -c 128 -g 8,8,4 # grid_order -R -Z -c 128 -g 8,8,4 # Region 3: 0,0,1 (0..255) 0,1,2,3,32,33,34,35,64,65,66,67,96,97,98,99,128,129,130,131,160,161,162,163,192,193,194,195,224,225,226,227,4,5,6,7,36,37,38,39,68,69,70,71,100,101,102,103,132,133,134,135,164,165,166,167,196,197,198,199,228,229,230,231,8,9,10,11,40,41,42,43,72,73,74,75,104,105,106,107,136,137,138,139,168,169,170,171,200,201,202,203,232,233,234,235,12,13,14,15,44,45,46,47,76,77,78,79,108,109,110,111,140,141,142,143,172,173,174,175,204,205,206,207,236,237,238,239 16,17,18,19,48,49,50,51,80,81,82,83,112,113,114,115,144,145,146,147,176,177,178,179,208,209,210,211,240,241,242,243,20,21,22,23,52,53,54,55,84,85,86,87,116,117,118,119,148,149,150,151,180,181,182,183,212,213,214,215,244,245,246,247,24,25,26,27,56,57,58,59,88,89,90,91,120,121,122,123,152,153,154,155,184,185,186,187,216,217,218,219,248,249,250,251,28,29,30,31,60,61,62,63,92,93,94,95,124,125,126,127,156,157,158,159,188,189,190,191,220,221,222,223,252,253,254,255 One can then save this output to a file called MPICH_RANK_ORDER and then set MPICH_RANK_REORDER_METHOD=3 before running the job, which tells Cray MPI to read the MPICH_RANK_ORDER file to set the MPI task placement. For more information, please see the man page man grid_order .","title":"grid_order"},{"location":"user-guide/scheduler/#interactive-jobs","text":"","title":"Interactive Jobs"},{"location":"user-guide/scheduler/#using-salloc-to-reserve-resources","text":"When you are developing or debugging code you often want to run many short jobs with a small amount of editing the code between runs. This can be achieved by using the login nodes to run MPI but you may want to test on the compute nodes (e.g. you may want to test running on multiple nodes across the high performance interconnect). One of the best ways to achieve this on ARCHER2 is to use interactive jobs. An interactive job allows you to issue srun commands directly from the command line without using a job submission script, and to see the output from your program directly in the terminal. You use the salloc command to reserve compute nodes for interactive jobs. To submit a request for an interactive job reserving 8 nodes (1024 physical cores) for 20 minutes on the short queue you would issue the following command from the command line: Full system auser@ln01:> salloc --nodes = 8 --tasks-per-node = 128 --cpus-per-task = 1 \\ --time = 00 :20:00 --partition = standard --qos = short \\ --account =[ budget code ] When you submit this job your terminal will display something like: Full system salloc: Granted job allocation 24236 salloc: Waiting for resource configuration salloc: Nodes nid000002 are ready for job auser@ln01:> It may take some time for your interactive job to start. Once it runs you will enter a standard interactive terminal session (a new shell). Note that this shell is still on the front end (the prompt has not change). Whilst the interactive session lasts you will be able to run parallel jobs on the compute nodes by issuing the srun --distribution=block:block --hint=nomultithread command directly at your command prompt using the same syntax as you would inside a job script. The maximum number of nodes you can use is limited by resources requested in the salloc command. If you know you will be doing a lot of intensive debugging you may find it useful to request an interactive session lasting the expected length of your working session, say a full day. Your session will end when you hit the requested walltime. If you wish to finish before this you should use the exit command - this will return you to your prompt before you issued the salloc command.","title":"Using salloc to reserve resources"},{"location":"user-guide/scheduler/#using-srun-directly","text":"A second way to run an interactive job is to use srun directly in the following way (here using the \"short queue\"): Full system auser@ln01:/work/t01/t01/auser> srun --nodes=1 --exclusive --time=00:20:00 \\ --partition=standard --qos=short --reservation=shortqos \\ --account=[budget code] --pty /bin/bash auser@nid001261:/work/t01/t01/auser> hostname nid001261 The --pty /bin/bash will cause a new shell to be started on the first node of a new allocation . This is perhaps closer to what many people consider an 'interactive' job than the method using salloc appears. One can now issue shell commands in the usual way. A further invocation of srun is required to launch a parallel job in the allocation. Note When using srun within an interactive srun session, you will need to include the --oversubscribe flag and specify the number of cores you want to use: auser@nid001261:/work/t01/t01/auser> srun --oversubscribe --distribution=block:block \\ --hint=nomultithread --ntasks=128 ./my_mpi_executable.x When finished, type exit to relinquish the allocation and control will be returned to the front end. By default, the interactive shell will retain the environment of the parent. If you want a clean shell, remember to specify --export=none .","title":"Using srun directly"},{"location":"user-guide/scheduler/#heterogeneous-jobs","text":"The Slurm submissions discussed above involve a single executable image. However, there are situtions where two or more distinct executables are coupled and need to be run at the same time. This is most easily handled via the Slurm heterogeneous job mechanism. Two common cases are discussed below: first, a client server model in which client and server each have a different MPI_COMM_WORLD , and second the case were two or more executables share MPI_COMM_WORLD .","title":"Heterogeneous jobs"},{"location":"user-guide/scheduler/#heterogeneous-jobs-for-a-clientserver-model-distinct-mpi_comm_worlds","text":"The essential feature of a heterogeneous job here is to create a single batch submission which specifies the resource requirements for the individual components. Schematically, we would use #!/bin/bash # Slurm specifications for the first component #SBATCH --partition=standard ... #SBATCH hetjob # Slurm specifications for the second component #SBATCH --partition=standard ... where new each component beyond the first is introduced by the special token #SBATCH hetjob (note this is not a normal option and is not --hetjob ). Each component must specify a partition. Such a job will appear in the queue system as, e.g., 50098+0 standard qscript- user PD 0:00 1 (None) 50098+1 standard qscript- user PD 0:00 2 (None) and counts as (in this case) two separate jobs from the point of QoS limits. Consider a case where we have two executables which may both be parallel (in that they use MPI), both run at the same time, and communicate with each other by some means other than MPI. In the following example, we run two different executables, both of which must finish before the jobs completes. #!/bin/bash #SBATCH --time=00:20:00 #SBATCH --exclusive #SBATCH --export=none #SBATCH --partition=standard #SBATCH --qos=standard #SBATCH --nodes=1 #SBATCH --ntasks-per-node=8 #SBATCH hetjob #SBATCH --partition=standard #SBATCH --qos=standard #SBATCH --nodes=2 #SBATCH --ntasks-per-node=4 # Run two execuatables with separate MPI_COMM_WORLD srun --distribution=block:block --hint=nomultithread --het-group=0 ./xthi-a & srun --distribution=block:block --hint=nomultithread --het-group=1 ./xthi-b & wait In this case, each executable is launched with a separate call to srun but specifies a different heterogeneous group via the --het-group option. The first group is --het-group=0 . Both are run in the background with & and the wait is required to ensure both executables have completed before the job submission exits. In this rather artificial example, where each component makes a simple report about its placement, the output might be Node 0, hostname nid001028, mpi 4, omp 1, executable xthi-b Node 1, hostname nid001048, mpi 4, omp 1, executable xthi-b Node 0, rank 0, thread 0, (affinity = 0) Node 0, rank 1, thread 0, (affinity = 1) Node 0, rank 2, thread 0, (affinity = 2) Node 0, rank 3, thread 0, (affinity = 3) Node 1, rank 4, thread 0, (affinity = 0) Node 1, rank 5, thread 0, (affinity = 1) Node 1, rank 6, thread 0, (affinity = 2) Node 1, rank 7, thread 0, (affinity = 3) Node 0, hostname nid001027, mpi 8, omp 1, executable xthi-a Node 0, rank 0, thread 0, (affinity = 0) Node 0, rank 1, thread 0, (affinity = 1) Node 0, rank 2, thread 0, (affinity = 2) Node 0, rank 3, thread 0, (affinity = 3) Node 0, rank 4, thread 0, (affinity = 4) Node 0, rank 5, thread 0, (affinity = 5) Node 0, rank 6, thread 0, (affinity = 6) Node 0, rank 7, thread 0, (affinity = 7) Here we have the first executable running on one node with a communicator size 8 (ranks 0-7). The second executable runs on two nodes also with communicator size 8 (ranks 0-7, 4 ranks per node). Further examples of placement for heterogenenous jobs are given below.","title":"Heterogeneous jobs for a client/server model: distinct MPI_COMM_WORLDs"},{"location":"user-guide/scheduler/#heterogeneous-jobs-for-a-shared-mpi_com_world","text":"Full system Note The directive SBATCH hetjob can no longer be used for jobs requiring a shared MPI_COMM_WORLD If two or more heterogeneous components need to share a unique MPI_COMM_WORLD , a single srun invocation with the differrent components separated by a colon : should be used. Arguements to the individual components of the srun control the placement of the tasks and threads for each component. For example: #!/bin/bash #SBATCH --time=00:20:00 #SBATCH --exclusive #SBATCH --export=none #SBATCH --account=[...] #SBATCH --partition=standard #SBATCH --qos=standard # We must specify correctly the total number of nodes required. #SBATCH --nodes=3 SHARED_ARGS=\"--distribution=block:block --hint=nomultithread\" srun --het-group=0 --nodes=1 --tasks-per-node=8 ${SHARED_ARGS} ./xthi-a : \\ --het-group=1 --nodes=2 --tasks-per-node=4 ${SHARED_ARGS} ./xthi-b The output should confirm we have a single MPI_COMM_WORLD with a total of three nodes, and ranks 0-15. Node summary for 3 nodes: Node 0, hostname nid002668, mpi 8, omp 1, executable xthi-a Node 1, hostname nid002669, mpi 4, omp 1, executable xthi-b Node 2, hostname nid002670, mpi 4, omp 1, executable xthi-b MPI summary: 16 ranks Node 0, rank 0, thread 0, (affinity = 0) Node 0, rank 1, thread 0, (affinity = 1) Node 0, rank 2, thread 0, (affinity = 2) Node 0, rank 3, thread 0, (affinity = 3) Node 0, rank 4, thread 0, (affinity = 4) Node 0, rank 5, thread 0, (affinity = 5) Node 0, rank 6, thread 0, (affinity = 6) Node 0, rank 7, thread 0, (affinity = 7) Node 1, rank 8, thread 0, (affinity = 0) Node 1, rank 9, thread 0, (affinity = 1) Node 1, rank 10, thread 0, (affinity = 2) Node 1, rank 11, thread 0, (affinity = 3) Node 2, rank 12, thread 0, (affinity = 0) Node 2, rank 13, thread 0, (affinity = 1) Node 2, rank 14, thread 0, (affinity = 2) Node 2, rank 15, thread 0, (affinity = 3)","title":"Heterogeneous jobs for a shared MPI_COM_WORLD"},{"location":"user-guide/scheduler/#heterogeneous-placement-for-mixed-mpiopenmp-work","text":"Some care may be required for placement of tasks/threads in heterogeneous jobs in which the number of threads needs to be specified differently for different components. In the following we have two components. The first component runs 8 MPI tasks each with 16 OpenMP threads. The second component runs 8 MPI tasks with one task per NUMA region on one node; each task has one thread. An appropriate Slurm submission might be: Full system #!/bin/bash #SBATCH --time=00:20:00 #SBATCH --exclusive #SBATCH --export=none #SBATCH --account=[...] #SBATCH --partition=standard #SBATCH --qos=standard #SBATCH --nodes=2 SHARED_ARGS=\"--distribution=block:block --hint=nomultithread \\ --nodes=1 --tasks-per-node=8 --cpus-per-task=16\" # Do not set OMP_NUM_THREADS in the calling environment unset OMP_NUM_THREADS export OMP_PROC_BIND=spread srun --het-group=0 ${SHARED_ARGS} --export=all,OMP_NUM_THREADS=16 ./xthi-a : \\ --het-group=1 ${SHARED_ARGS} --export=all,OMP_NUM_THREADS=1 ./xthi-b The important point here is that OMP_NUM_THREADS must not be set in the environment that calls srun in order that the different specifications for the separate groups via --export on the srun command line take effect. If OMP_NUM_THREADS is set in the calling environment, then that value takes precedence, and each component will see the same value of OMP_NUM_THREADS . The output would be: Node 0, hostname nid001111, mpi 8, omp 16, executable xthi-a Node 1, hostname nid001126, mpi 8, omp 1, executable xthi-b Node 0, rank 0, thread 0, (affinity = 0) Node 0, rank 0, thread 1, (affinity = 1) Node 0, rank 0, thread 2, (affinity = 2) Node 0, rank 0, thread 3, (affinity = 3) Node 0, rank 0, thread 4, (affinity = 4) Node 0, rank 0, thread 5, (affinity = 5) Node 0, rank 0, thread 6, (affinity = 6) Node 0, rank 0, thread 7, (affinity = 7) Node 0, rank 0, thread 8, (affinity = 8) Node 0, rank 0, thread 9, (affinity = 9) Node 0, rank 0, thread 10, (affinity = 10) Node 0, rank 0, thread 11, (affinity = 11) Node 0, rank 0, thread 12, (affinity = 12) Node 0, rank 0, thread 13, (affinity = 13) Node 0, rank 0, thread 14, (affinity = 14) Node 0, rank 0, thread 15, (affinity = 15) Node 0, rank 1, thread 0, (affinity = 16) Node 0, rank 1, thread 1, (affinity = 17) ... Node 0, rank 7, thread 14, (affinity = 126) Node 0, rank 7, thread 15, (affinity = 127) Node 1, rank 8, thread 0, (affinity = 0) Node 1, rank 9, thread 0, (affinity = 16) Node 1, rank 10, thread 0, (affinity = 32) Node 1, rank 11, thread 0, (affinity = 48) Node 1, rank 12, thread 0, (affinity = 64) Node 1, rank 13, thread 0, (affinity = 80) Node 1, rank 14, thread 0, (affinity = 96) Node 1, rank 15, thread 0, (affinity = 112)","title":"Heterogeneous placement for mixed MPI/OpenMP work"},{"location":"user-guide/scheduler/#low-priority-access","text":"Low priority jobs are not charged against your allocation but will only run when other, higher-priority, jobs cannot be run or there are no higher-priority jobs in the queue. Although low priority jobs are not charged, you do need a valid, positive budget to be able to submit and run low priority jobs, i.e. you need at least 1 CU in your budget. Low priority access is always available and has the following limits: Full system 1024 node maximum job size Maximum 16 low priority jobs in the queue per user Maximum 16 low priority job running per user (of the 16 queued) Maximum runtime of 24 hours You submit a low priority job on ARCHER2 by using the lowpriority QoS. For example, you would usually have the following line in your job submission script sbatch options: #SBATCH --qos=lowpriority","title":"Low priority access"},{"location":"user-guide/scheduler/#reservations","text":"Reservations are available on ARCHER2. These allow users to reserve a number of nodes for a specified length of time starting at a particular time on the system. Reservations require justification. They will only be approved if the request could not be fulfilled with the standard queues. For instance, you require a job/jobs to run at a particular time e.g. for a demonstration or course. Note Reservation requests must be submitted at least 60 hours in advance of the reservation start time. If requesting a reservation for a Monday at 18:00, please ensure this is received by the Friday at 12:00 the latest. The same applies over Service Holidays. Note Reservations are only valid for standard compute nodes, high memory compute nodes and/or PP nodes cannot be included in reservations. Reservations will be charged at 1.5 times the usual CU rate and our policy is that they will be charged the full rate for the entire reservation at the time of booking, whether or not you use the nodes for the full time. In addition, you will not be refunded the CUs if you fail to use them due to a job issue unless this issue is due to a system failure. Bug At the moment, we are only able to charge for jobs in reservations, not for the full reservation itself. Jobs in reservations are charged at 1.5x the standard rate. To request a reservation you complete a form on SAFE: Log into SAFE Under the \"Login accounts\" menu, choose the \"Request reservation\" option On the first page, you need to provide the following: The start time and date of the reservation. The end time and date of the reservation. Your justification for the reservation -- this must be provided or the request will be rejected. The number of nodes required. On the second page, you will need to specify which username you wish the reservation to be charged against and, once the username has been selected, the budget you want to charge the reservation to. Your request will be checked by the ARCHER2 User Administration team and, if approved, you will be provided a reservation ID which can be used on the system. To submit jobs to a reservation, you need to add --reservation=<reservation ID> and --qos=reservation options to your job submission script or command. Important You must have at least 1 CU in the budget to submit a job on ARCHER2, even to a pre-paid reservation. Tip You can submit jobs to a reservation as soon as the reservation has been set up; jobs will remain queued until the reservation starts.","title":"Reservations"},{"location":"user-guide/scheduler/#serial-jobs","text":"You can run serial jobs on the shared data analysis nodes. More information on using the data analysis nodes (including example job submission scripts) can be found in the Data Analysis section of the User and Best Practice Guide.","title":"Serial jobs"},{"location":"user-guide/scheduler/#best-practices-for-job-submission","text":"This guidance is adapted from the advice provided by NERSC","title":"Best practices for job submission"},{"location":"user-guide/scheduler/#time-limits","text":"Due to backfill scheduling, short and variable-length jobs generally start quickly resulting in much better job throughput. You can specify a minimum time for your job with the --time-min option to SBATCH: #SBATCH --time-min=<lower_bound> #SBATCH --time=<upper_bound> Within your job script, you can get the time remaining in the job with squeue -h -j ${Slurm_JOBID} -o %L to allow you to deal with potentially varying runtimes when using this option.","title":"Time Limits"},{"location":"user-guide/scheduler/#long-running-jobs","text":"Simulations which must run for a long period of time achieve the best throughput when composed of many small jobs using a checkpoint and restart method chained together (see above for how to chain jobs together). However, this method does occur a startup and shutdown overhead for each job as the state is saved and loaded so you should experiment to find the best balance between runtime (long runtimes minimise the checkpoint/restart overheads) and throughput (short runtimes maximise throughput).","title":"Long Running Jobs"},{"location":"user-guide/scheduler/#interconnect-locality","text":"For jobs which are sensitive to interconnect (MPI) performance and utilise 128 nodes or less it is possible to request that all nodes are in a single Slingshot dragonfly group. The maximum number of nodes in a group on ARCHER2 is 128. Slurm has a concept of \"switches\" which on ARCHER2 are configured to map to Slingshot electrical groups; where all compute nodes have all-to-all electrical connections which minimises latency. Since this places an additional constraint on the scheduler a maximum time to wait for the requested topology can be specified - after this time, the job will be placed without the constraint. For example, to specify that all requested nodes should come from one electrical group and to wait for up to 6 hours (360 minutes) for that placement, you would use the following option in your job: #SBATCH --switches=1@360 You can request multiple groups using this option if you are using more nodes than are in a single group to maximise the number of nodes that share electrical connetions in the job. For example, to request 4 groups (maximum of 512 nodes) and have this as an absolute constraint with no timeout, you would use: #SBATCH --switches=4 Danger When specifying the number of groups take care to request enough groups to satisfy the requested number of nodes. If the number is too low then an unneccesary delay will be added due to the unsatisfiable request. A useful heuristic to ensure this is the case is to ensure that the total nodes requested is less than or equal to the number of groups multiplied by 128.","title":"Interconnect locality"},{"location":"user-guide/scheduler/#large-jobs","text":"Large jobs may take longer to start up. The sbcast command is recommended for large jobs requesting over 1500 MPI tasks. By default, Slurm reads the executable on the allocated compute nodes from the location where it is installed; this may take long time when the file system (where the executable resides) is slow or busy. The sbcast command, the executable can be copied to the /tmp directory on each of the compute nodes. Since /tmp is part of the memory on the compute nodes, it can speed up the job startup time. sbcast --compress=lz4 /path/to/exe /tmp/exe srun /tmp/exe","title":"Large Jobs"},{"location":"user-guide/scheduler/#huge-pages","text":"Huge pages are virtual memory pages which are bigger than the default page size of 4K bytes. Huge pages can improve memory performance for common access patterns on large data sets since it helps to reduce the number of virtual to physical address translations when compared to using the default 4KB. To use huge pages for an application (with the 2 MB huge pages as an example): module load craype-hugepages2M cc -o mycode.exe mycode.c And also load the same huge pages module at runtime. Warning Due to the huge pages memory fragmentation issue, applications may get Cannot allocate memory warnings or errors when there are not enough hugepages on the compute node, such as: libhugetlbfs [nid0000xx:xxxxx]: WARNING: New heap segment map at 0x10000000 failed: Cannot allocate memory`` By default, The verbosity level of libhugetlbfs HUGETLB_VERBOSE is set to 0 on ARCHER2 to surpress debugging messages. Users can adjust this value to obtain more information on huge pages use.","title":"Huge pages"},{"location":"user-guide/scheduler/#when-to-use-huge-pages","text":"For MPI applications, map the static data and/or heap onto huge pages. For an application which uses shared memory, which needs to be concurrently registered with the high speed network drivers for remote communication. For SHMEM applications, map the static data and/or private heap onto huge pages. For applications written in Unified Parallel C, Coarray Fortran, and other languages based on the PGAS programming model, map the static data and/or private heap onto huge pages. For an application doing heavy I/O. To improve memory performance for common access patterns on large data sets.","title":"When to Use Huge Pages"},{"location":"user-guide/scheduler/#when-to-avoid-huge-pages","text":"Applications sometimes consist of many steering programs in addition to the core application. Applying huge page behavior to all processes would not provide any benefit and would consume huge pages that would otherwise benefit the core application. The runtime environment variable HUGETLB_RESTRICT_EXE can be used to specify the susbset of the programs to use hugepages. For certain applications if using hugepages either causes issues or slows down performance. One such example is that when an application forks more subprocesses (such as pthreads) and these threads allocate memory, the newly allocated memory are the default 4 KB pages.","title":"When to Avoid Huge Pages"},{"location":"user-guide/sw-environment-4cab/","text":"Software environment: 4-cabinet system Important This section covers the software environment on the initial, 4-cabinet ARCHER2 system. For docmentation on the software environment on the full ARCHER2 system, please see Software environment: full system . The software environment on ARCHER2 is primarily controlled through the module command. By loading and switching software modules you control which software and versions are available to you. Information A module is a self-contained description of a software package -- it contains the settings required to run a software package and, usually, encodes required dependencies on other software packages. By default, all users on ARCHER2 start with the default software environment loaded. Software modules on ARCHER2 are provided by both HPE Cray (usually known as the Cray Development Environment, CDE ) and by EPCC, who provide the Service Provision, and Computational Science and Engineering services. In this section, we provide: A brief overview of the module command A brief description of how the module command manipulates your environment Using the module command We only cover basic usage of the module command here. For full documentation please see the Linux manual page on modules The module command takes a subcommand to indicate what operation you wish to perform. Common subcommands are: module list [name] - List modules currently loaded in your environment, optionally filtered by [name] module avail [name] - List modules available, optionally filtered by [name] module savelist - List module collections available (usually used for accessing different programming environments) module restore name - Restore the module collection called name (usually used for setting up a programming environment) module load name - Load the module called name into your environment module remove name - Remove the module called name from your environment module swap old new - Swap module new for module old in your environment module help name - Show help information on module name module show name - List what module name actually does to your environment These are described in more detail below. Information on the available modules The module list command will give the names of the modules and their versions you have presently loaded in your environment: auser@uan01:~> module list Currently Loaded Modulefiles: 1) cpe-aocc 7) cray-dsmml/0.1.2(default) 2) aocc/2.1.0.3(default) 8) perftools-base/20.09.0(default) 3) craype/2.7.0(default) 9) xpmem/2.2.35-7.0.1.0_1.3__gd50fabf.shasta(default) 4) craype-x86-rome 10) cray-mpich/8.0.15(default) 5) libfabric/1.11.0.0.233(default) 11) cray-libsci/20.08.1.2(default) 6) craype-network-ofi Finding out which software modules are available on the system is performed using the module avail command. To list all software modules available, use: auser@uan01:~> module avail ------------------------------- /opt/cray/pe/perftools/20.09.0/modulefiles -------------------------------- perftools perftools-lite-events perftools-lite-hbm perftools-nwpc perftools-lite perftools-lite-gpu perftools-lite-loops perftools-preload ---------------------------------- /opt/cray/pe/craype/2.7.0/modulefiles ---------------------------------- craype-hugepages1G craype-hugepages8M craype-hugepages128M craype-network-ofi craype-hugepages2G craype-hugepages16M craype-hugepages256M craype-network-slingshot10 craype-hugepages2M craype-hugepages32M craype-hugepages512M craype-x86-rome craype-hugepages4M craype-hugepages64M craype-network-none ------------------------------------- /usr/local/Modules/modulefiles -------------------------------------- dot module-git module-info modules null use.own -------------------------------------- /opt/cray/pe/cpe-prgenv/7.0.0 -------------------------------------- cpe-aocc cpe-cray cpe-gnu -------------------------------------------- /opt/modulefiles --------------------------------------------- aocc/2.1.0.3(default) cray-R/4.0.2.0(default) gcc/8.1.0 gcc/9.3.0 gcc/10.1.0(default) ---------------------------------------- /opt/cray/pe/modulefiles ----------------------------------------- atp/3.7.4(default) cray-mpich-abi/8.0.15 craype-dl-plugin-py3/20.06.1(default) cce/10.0.3(default) cray-mpich-ucx/8.0.15 craype/2.7.0(default) cray-ccdb/4.7.1(default) cray-mpich/8.0.15(default) craypkg-gen/1.3.10(default) cray-cti/2.7.3(default) cray-netcdf-hdf5parallel/4.7.4.0 gdb4hpc/4.7.3(default) cray-dsmml/0.1.2(default) cray-netcdf/4.7.4.0 iobuf/2.0.10(default) cray-fftw/3.3.8.7(default) cray-openshmemx/11.1.1(default) papi/6.0.0.2(default) cray-ga/5.7.0.3 cray-parallel-netcdf/1.12.1.0 perftools-base/20.09.0(default) cray-hdf5-parallel/1.12.0.0 cray-pmi-lib/6.0.6(default) valgrind4hpc/2.7.2(default) cray-hdf5/1.12.0.0 cray-pmi/6.0.6(default) cray-libsci/20.08.1.2(default) cray-python/3.8.5.0(default) This will list all the names and versions of the modules available on the service. Not all of them may work in your account though due to, for example, licencing restrictions. You will notice that for many modules we have more than one version, each of which is identified by a version number. One of these versions is the default. As the service develops the default version will change and old versions of software may be deleted. You can list all the modules of a particular type by providing an argument to the module avail command. For example, to list all available versions of the HPE Cray FFTW library, use: auser@uan01:~> module avail cray-fftw ---------------------------------------- /opt/cray/pe/modulefiles ----------------------------------------- cray-fftw/3.3.8.7(default) If you want more info on any of the modules, you can use the module help command: auser@uan01:~> module help cray-fftw ------------------------------------------------------------------- Module Specific Help for /opt/cray/pe/modulefiles/cray-fftw/3.3.8.7: =================================================================== FFTW 3.3.8.7 ============ Release Date: ------------- June 2020 Purpose: -------- This Cray FFTW 3.3.8.7 release is supported on Cray Shasta Systems. FFTW is supported on the host CPU but not on the accelerator of Cray systems. The Cray FFTW 3.3.8.7 release provides the following: - Optimizations for AMD Rome CPUs. See the Product and OS Dependencies section for details [...] The module show command reveals what operations the module actually performs to change your environment when it is loaded. We provide a brief overview of what the significance of these different settings mean below. For example, for the default FFTW module: auser@uan01:~> module show cray-fftw ------------------------------------------------------------------- /opt/cray/pe/modulefiles/cray-fftw/3.3.8.7: conflict cray-fftw conflict fftw setenv FFTW_VERSION 3.3.8.7 setenv CRAY_FFTW_VERSION 3.3.8.7 setenv CRAY_FFTW_PREFIX /opt/cray/pe/fftw/3.3.8.7/x86_rome setenv FFTW_ROOT /opt/cray/pe/fftw/3.3.8.7/x86_rome setenv FFTW_DIR /opt/cray/pe/fftw/3.3.8.7/x86_rome/lib setenv FFTW_INC /opt/cray/pe/fftw/3.3.8.7/x86_rome/include prepend-path PATH /opt/cray/pe/fftw/3.3.8.7/x86_rome/bin prepend-path MANPATH /opt/cray/pe/fftw/3.3.8.7/share/man prepend-path CRAY_LD_LIBRARY_PATH /opt/cray/pe/fftw/3.3.8.7/x86_rome/lib prepend-path PE_PKGCONFIG_PRODUCTS PE_FFTW setenv PE_FFTW_TARGET_x86_skylake x86_skylake setenv PE_FFTW_TARGET_x86_rome x86_rome setenv PE_FFTW_TARGET_x86_cascadelake x86_cascadelake setenv PE_FFTW_TARGET_x86_64 x86_64 setenv PE_FFTW_TARGET_share share setenv PE_FFTW_TARGET_sandybridge sandybridge setenv PE_FFTW_TARGET_mic_knl mic_knl setenv PE_FFTW_TARGET_ivybridge ivybridge setenv PE_FFTW_TARGET_haswell haswell setenv PE_FFTW_TARGET_broadwell broadwell setenv PE_FFTW_VOLATILE_PKGCONFIG_PATH /opt/cray/pe/fftw/3.3.8.7/@PE_FFTW_TARGET@/lib/pkgconfig setenv PE_FFTW_PKGCONFIG_VARIABLES PE_FFTW_OMP_REQUIRES_@openmp@ setenv PE_FFTW_OMP_REQUIRES { } setenv PE_FFTW_OMP_REQUIRES_openmp _mp setenv PE_FFTW_PKGCONFIG_LIBS fftw3_mpi:libfftw3_threads:fftw3:fftw3f_mpi:libfftw3f_threads:fftw3f module-whatis {FFTW 3.3.8.7 - Fastest Fourier Transform in the West} [...] Loading, removing and swapping modules To load a module to use the module load command. For example, to load the default version of HPE Cray FFTW into your environment, use: auser@uan01:~> module load cray-fftw Once you have done this, your environment will be setup to use the HPE Cray FFTW library. The above command will load the default version of HPE Cray FFTW. If you need a specific version of the software, you can add more information: auser@uan01:~> module load cray-fftw/3.3.8.7 will load HPE Cray FFTW version 3.3.8.7 into your environment, regardless of the default. If you want to remove software from your environment, module remove will remove a loaded module: auser@uan01:~> module remove cray-fftw will unload what ever version of cray-fftw (even if it is not the default) you might have loaded. There are many situations in which you might want to change the presently loaded version to a different one, such as trying the latest version which is not yet the default or using a legacy version to keep compatibility with old data. This can be achieved most easily by using module swap oldmodule newmodule . Suppose you have loaded version 3.3.8.7 of cray-fftw , the following command will change to version 3.3.8.5: auser@uan01:~> module swap cray-fftw cray-fftw/3.3.8.5 You did not need to specify the version of the loaded module in your current environment as this can be inferred as it will be the only one you have loaded. Changing Programming Environment The three programming environments PrgEnv-aocc , PrgEnv-cray , PrgEnv-gnu are implemented as module collections. The correct way to change programming environment, that is, change the collection of modules, is therefore via module restore . For example: auser@uan01:~> module restore PrgEnv-gnu !!! note there is only one argument, which is the collection to be restored. The command module restore will output a list of modules in the outgoing collection as they are unloaded, and the modules in the incoming collection as they are loaded. If you prefer not to have messages auser@uan1:~> module -s restore PrgEnv-gnu will suppress the messages. An attempt to restore a collection which is already loaded will result in no operation. Module collections are stored in a user's home directory ${HOME}/.module . However, as the home directory is not available to the back end, module restore may fail for batch jobs. In this case, it is possible to restore one of the three standard programming environments via, e.g., module restore /etc/cray-pe.d/PrgEnv-gnu Capturing your environment for reuse Sometimes it is useful to save the module environment that you are using to compile a piece of code or execute a piece of software. This is saved as a module collection. You can save a collection from your current environment by executing: auser@uan01:~> module save [collection_name] Note If you do not specify the environment name, it is called default . You can find the list of saved module environments by executing: auser@uan01:~> module savelist Named collection list: 1) default 2) PrgEnv-aocc 3) PrgEnv-cray 4) PrgEnv-gnu To list the modules in a collection, you can execute, e.g.,: auser@uan01:~> module saveshow PrgEnv-gnu ------------------------------------------------------------------- /home/t01/t01/auser/.module/default: module use --append /opt/cray/pe/perftools/20.09.0/modulefiles module use --append /opt/cray/pe/craype/2.7.0/modulefiles module use --append /usr/local/Modules/modulefiles module use --append /opt/cray/pe/cpe-prgenv/7.0.0 module use --append /opt/modulefiles module use --append /opt/cray/modulefiles module use --append /opt/cray/pe/modulefiles module use --append /opt/cray/pe/craype-targets/default/modulefiles module load cpe-gnu module load gcc module load craype module load craype-x86-rome module load --notuasked libfabric module load craype-network-ofi module load cray-dsmml module load perftools-base module load xpmem module load cray-mpich module load cray-libsci module load /work/y07/shared/archer2-modules/modulefiles-cse/epcc-setup-env Note again that the details of the collection have been saved to the home directory (the first line of output above). It is possible to save a module collection with a fully qualified path, e.g., auser@uan1:~> module save /work/t01/z01/auser/.module/PrgEnv-gnu which would make it available from the batch system. To delete a module environment, you can execute: auser@uan01:~> module saverm <environment_name> Shell environment overview When you log in to ARCHER2, you are using the bash shell by default. As any other software, the bash shell has loaded a set of environment variables that can be listed by executing printenv or export . The environment variables listed before are useful to define the behaviour of the software you run. For instance, OMP_NUM_THREADS define the number of threads. To define an environment variable, you need to execute: export OMP_NUM_THREADS=4 Please note there are no blanks between the variable name, the assignation symbol, and the value. If the value is a string, enclose the string in double quotation marks. You can show the value of a specific environment variable if you print it: echo $OMP_NUM_THREADS Do not forget the dollar symbol. To remove an environment variable, just execute: unset OMP_NUM_THREADS","title":"Software environment: 4-cabinet system"},{"location":"user-guide/sw-environment-4cab/#software-environment-4-cabinet-system","text":"Important This section covers the software environment on the initial, 4-cabinet ARCHER2 system. For docmentation on the software environment on the full ARCHER2 system, please see Software environment: full system . The software environment on ARCHER2 is primarily controlled through the module command. By loading and switching software modules you control which software and versions are available to you. Information A module is a self-contained description of a software package -- it contains the settings required to run a software package and, usually, encodes required dependencies on other software packages. By default, all users on ARCHER2 start with the default software environment loaded. Software modules on ARCHER2 are provided by both HPE Cray (usually known as the Cray Development Environment, CDE ) and by EPCC, who provide the Service Provision, and Computational Science and Engineering services. In this section, we provide: A brief overview of the module command A brief description of how the module command manipulates your environment","title":"Software environment: 4-cabinet system"},{"location":"user-guide/sw-environment-4cab/#using-the-module-command","text":"We only cover basic usage of the module command here. For full documentation please see the Linux manual page on modules The module command takes a subcommand to indicate what operation you wish to perform. Common subcommands are: module list [name] - List modules currently loaded in your environment, optionally filtered by [name] module avail [name] - List modules available, optionally filtered by [name] module savelist - List module collections available (usually used for accessing different programming environments) module restore name - Restore the module collection called name (usually used for setting up a programming environment) module load name - Load the module called name into your environment module remove name - Remove the module called name from your environment module swap old new - Swap module new for module old in your environment module help name - Show help information on module name module show name - List what module name actually does to your environment These are described in more detail below.","title":"Using the module command"},{"location":"user-guide/sw-environment-4cab/#information-on-the-available-modules","text":"The module list command will give the names of the modules and their versions you have presently loaded in your environment: auser@uan01:~> module list Currently Loaded Modulefiles: 1) cpe-aocc 7) cray-dsmml/0.1.2(default) 2) aocc/2.1.0.3(default) 8) perftools-base/20.09.0(default) 3) craype/2.7.0(default) 9) xpmem/2.2.35-7.0.1.0_1.3__gd50fabf.shasta(default) 4) craype-x86-rome 10) cray-mpich/8.0.15(default) 5) libfabric/1.11.0.0.233(default) 11) cray-libsci/20.08.1.2(default) 6) craype-network-ofi Finding out which software modules are available on the system is performed using the module avail command. To list all software modules available, use: auser@uan01:~> module avail ------------------------------- /opt/cray/pe/perftools/20.09.0/modulefiles -------------------------------- perftools perftools-lite-events perftools-lite-hbm perftools-nwpc perftools-lite perftools-lite-gpu perftools-lite-loops perftools-preload ---------------------------------- /opt/cray/pe/craype/2.7.0/modulefiles ---------------------------------- craype-hugepages1G craype-hugepages8M craype-hugepages128M craype-network-ofi craype-hugepages2G craype-hugepages16M craype-hugepages256M craype-network-slingshot10 craype-hugepages2M craype-hugepages32M craype-hugepages512M craype-x86-rome craype-hugepages4M craype-hugepages64M craype-network-none ------------------------------------- /usr/local/Modules/modulefiles -------------------------------------- dot module-git module-info modules null use.own -------------------------------------- /opt/cray/pe/cpe-prgenv/7.0.0 -------------------------------------- cpe-aocc cpe-cray cpe-gnu -------------------------------------------- /opt/modulefiles --------------------------------------------- aocc/2.1.0.3(default) cray-R/4.0.2.0(default) gcc/8.1.0 gcc/9.3.0 gcc/10.1.0(default) ---------------------------------------- /opt/cray/pe/modulefiles ----------------------------------------- atp/3.7.4(default) cray-mpich-abi/8.0.15 craype-dl-plugin-py3/20.06.1(default) cce/10.0.3(default) cray-mpich-ucx/8.0.15 craype/2.7.0(default) cray-ccdb/4.7.1(default) cray-mpich/8.0.15(default) craypkg-gen/1.3.10(default) cray-cti/2.7.3(default) cray-netcdf-hdf5parallel/4.7.4.0 gdb4hpc/4.7.3(default) cray-dsmml/0.1.2(default) cray-netcdf/4.7.4.0 iobuf/2.0.10(default) cray-fftw/3.3.8.7(default) cray-openshmemx/11.1.1(default) papi/6.0.0.2(default) cray-ga/5.7.0.3 cray-parallel-netcdf/1.12.1.0 perftools-base/20.09.0(default) cray-hdf5-parallel/1.12.0.0 cray-pmi-lib/6.0.6(default) valgrind4hpc/2.7.2(default) cray-hdf5/1.12.0.0 cray-pmi/6.0.6(default) cray-libsci/20.08.1.2(default) cray-python/3.8.5.0(default) This will list all the names and versions of the modules available on the service. Not all of them may work in your account though due to, for example, licencing restrictions. You will notice that for many modules we have more than one version, each of which is identified by a version number. One of these versions is the default. As the service develops the default version will change and old versions of software may be deleted. You can list all the modules of a particular type by providing an argument to the module avail command. For example, to list all available versions of the HPE Cray FFTW library, use: auser@uan01:~> module avail cray-fftw ---------------------------------------- /opt/cray/pe/modulefiles ----------------------------------------- cray-fftw/3.3.8.7(default) If you want more info on any of the modules, you can use the module help command: auser@uan01:~> module help cray-fftw ------------------------------------------------------------------- Module Specific Help for /opt/cray/pe/modulefiles/cray-fftw/3.3.8.7: =================================================================== FFTW 3.3.8.7 ============ Release Date: ------------- June 2020 Purpose: -------- This Cray FFTW 3.3.8.7 release is supported on Cray Shasta Systems. FFTW is supported on the host CPU but not on the accelerator of Cray systems. The Cray FFTW 3.3.8.7 release provides the following: - Optimizations for AMD Rome CPUs. See the Product and OS Dependencies section for details [...] The module show command reveals what operations the module actually performs to change your environment when it is loaded. We provide a brief overview of what the significance of these different settings mean below. For example, for the default FFTW module: auser@uan01:~> module show cray-fftw ------------------------------------------------------------------- /opt/cray/pe/modulefiles/cray-fftw/3.3.8.7: conflict cray-fftw conflict fftw setenv FFTW_VERSION 3.3.8.7 setenv CRAY_FFTW_VERSION 3.3.8.7 setenv CRAY_FFTW_PREFIX /opt/cray/pe/fftw/3.3.8.7/x86_rome setenv FFTW_ROOT /opt/cray/pe/fftw/3.3.8.7/x86_rome setenv FFTW_DIR /opt/cray/pe/fftw/3.3.8.7/x86_rome/lib setenv FFTW_INC /opt/cray/pe/fftw/3.3.8.7/x86_rome/include prepend-path PATH /opt/cray/pe/fftw/3.3.8.7/x86_rome/bin prepend-path MANPATH /opt/cray/pe/fftw/3.3.8.7/share/man prepend-path CRAY_LD_LIBRARY_PATH /opt/cray/pe/fftw/3.3.8.7/x86_rome/lib prepend-path PE_PKGCONFIG_PRODUCTS PE_FFTW setenv PE_FFTW_TARGET_x86_skylake x86_skylake setenv PE_FFTW_TARGET_x86_rome x86_rome setenv PE_FFTW_TARGET_x86_cascadelake x86_cascadelake setenv PE_FFTW_TARGET_x86_64 x86_64 setenv PE_FFTW_TARGET_share share setenv PE_FFTW_TARGET_sandybridge sandybridge setenv PE_FFTW_TARGET_mic_knl mic_knl setenv PE_FFTW_TARGET_ivybridge ivybridge setenv PE_FFTW_TARGET_haswell haswell setenv PE_FFTW_TARGET_broadwell broadwell setenv PE_FFTW_VOLATILE_PKGCONFIG_PATH /opt/cray/pe/fftw/3.3.8.7/@PE_FFTW_TARGET@/lib/pkgconfig setenv PE_FFTW_PKGCONFIG_VARIABLES PE_FFTW_OMP_REQUIRES_@openmp@ setenv PE_FFTW_OMP_REQUIRES { } setenv PE_FFTW_OMP_REQUIRES_openmp _mp setenv PE_FFTW_PKGCONFIG_LIBS fftw3_mpi:libfftw3_threads:fftw3:fftw3f_mpi:libfftw3f_threads:fftw3f module-whatis {FFTW 3.3.8.7 - Fastest Fourier Transform in the West} [...]","title":"Information on the available modules"},{"location":"user-guide/sw-environment-4cab/#loading-removing-and-swapping-modules","text":"To load a module to use the module load command. For example, to load the default version of HPE Cray FFTW into your environment, use: auser@uan01:~> module load cray-fftw Once you have done this, your environment will be setup to use the HPE Cray FFTW library. The above command will load the default version of HPE Cray FFTW. If you need a specific version of the software, you can add more information: auser@uan01:~> module load cray-fftw/3.3.8.7 will load HPE Cray FFTW version 3.3.8.7 into your environment, regardless of the default. If you want to remove software from your environment, module remove will remove a loaded module: auser@uan01:~> module remove cray-fftw will unload what ever version of cray-fftw (even if it is not the default) you might have loaded. There are many situations in which you might want to change the presently loaded version to a different one, such as trying the latest version which is not yet the default or using a legacy version to keep compatibility with old data. This can be achieved most easily by using module swap oldmodule newmodule . Suppose you have loaded version 3.3.8.7 of cray-fftw , the following command will change to version 3.3.8.5: auser@uan01:~> module swap cray-fftw cray-fftw/3.3.8.5 You did not need to specify the version of the loaded module in your current environment as this can be inferred as it will be the only one you have loaded.","title":"Loading, removing and swapping modules"},{"location":"user-guide/sw-environment-4cab/#changing-programming-environment","text":"The three programming environments PrgEnv-aocc , PrgEnv-cray , PrgEnv-gnu are implemented as module collections. The correct way to change programming environment, that is, change the collection of modules, is therefore via module restore . For example: auser@uan01:~> module restore PrgEnv-gnu !!! note there is only one argument, which is the collection to be restored. The command module restore will output a list of modules in the outgoing collection as they are unloaded, and the modules in the incoming collection as they are loaded. If you prefer not to have messages auser@uan1:~> module -s restore PrgEnv-gnu will suppress the messages. An attempt to restore a collection which is already loaded will result in no operation. Module collections are stored in a user's home directory ${HOME}/.module . However, as the home directory is not available to the back end, module restore may fail for batch jobs. In this case, it is possible to restore one of the three standard programming environments via, e.g., module restore /etc/cray-pe.d/PrgEnv-gnu","title":"Changing Programming Environment"},{"location":"user-guide/sw-environment-4cab/#capturing-your-environment-for-reuse","text":"Sometimes it is useful to save the module environment that you are using to compile a piece of code or execute a piece of software. This is saved as a module collection. You can save a collection from your current environment by executing: auser@uan01:~> module save [collection_name] Note If you do not specify the environment name, it is called default . You can find the list of saved module environments by executing: auser@uan01:~> module savelist Named collection list: 1) default 2) PrgEnv-aocc 3) PrgEnv-cray 4) PrgEnv-gnu To list the modules in a collection, you can execute, e.g.,: auser@uan01:~> module saveshow PrgEnv-gnu ------------------------------------------------------------------- /home/t01/t01/auser/.module/default: module use --append /opt/cray/pe/perftools/20.09.0/modulefiles module use --append /opt/cray/pe/craype/2.7.0/modulefiles module use --append /usr/local/Modules/modulefiles module use --append /opt/cray/pe/cpe-prgenv/7.0.0 module use --append /opt/modulefiles module use --append /opt/cray/modulefiles module use --append /opt/cray/pe/modulefiles module use --append /opt/cray/pe/craype-targets/default/modulefiles module load cpe-gnu module load gcc module load craype module load craype-x86-rome module load --notuasked libfabric module load craype-network-ofi module load cray-dsmml module load perftools-base module load xpmem module load cray-mpich module load cray-libsci module load /work/y07/shared/archer2-modules/modulefiles-cse/epcc-setup-env Note again that the details of the collection have been saved to the home directory (the first line of output above). It is possible to save a module collection with a fully qualified path, e.g., auser@uan1:~> module save /work/t01/z01/auser/.module/PrgEnv-gnu which would make it available from the batch system. To delete a module environment, you can execute: auser@uan01:~> module saverm <environment_name>","title":"Capturing your environment for reuse"},{"location":"user-guide/sw-environment-4cab/#shell-environment-overview","text":"When you log in to ARCHER2, you are using the bash shell by default. As any other software, the bash shell has loaded a set of environment variables that can be listed by executing printenv or export . The environment variables listed before are useful to define the behaviour of the software you run. For instance, OMP_NUM_THREADS define the number of threads. To define an environment variable, you need to execute: export OMP_NUM_THREADS=4 Please note there are no blanks between the variable name, the assignation symbol, and the value. If the value is a string, enclose the string in double quotation marks. You can show the value of a specific environment variable if you print it: echo $OMP_NUM_THREADS Do not forget the dollar symbol. To remove an environment variable, just execute: unset OMP_NUM_THREADS","title":"Shell environment overview"},{"location":"user-guide/sw-environment/","text":"Software environment The software environment on ARCHER2 is managed using the Lmod software. Selecting which software is available in your environment is primarily controlled through the module command. By loading and switching software modules you control which software and versions are available to you. Information A module is a self-contained description of a software package -- it contains the settings required to run a software package and, usually, encodes required dependencies on other software packages. By default, all users on ARCHER2 start with the default software environment loaded. Software modules on ARCHER2 are provided by both HPE (usually known as the HPE Cray Programming Environment, CPE ) and by EPCC, who provide the Service Provision, and Computational Science and Engineering services. In this section, we provide: A brief overview of the module command A brief description of how the module command manipulates your environment Using the module command We only cover basic usage of the Lmod module command here. For full documentation please see the Lmod documentation The module command takes a subcommand to indicate what operation you wish to perform. Common subcommands are: module list [name] - List modules currently loaded in your environment, optionally filtered by [name] module avail [name] - List modules available, optionally filtered by [name] module spider [name][/version] - Search available modules (including hidden modules) and provide information on modules module load name - Load the module called name into your environment module remove name - Remove the module called name from your environment module swap old new - Swap module new for module old in your environment module help name - Show help information on module name module show name - List what module name actually does to your environment These are described in more detail below. Tip Lmod allows you to use the ml shortcut command. Without any arguments, ml behaves like module list ; when a module name is specified to ml , ml behaves like module load . Note You will often have to include module commands in any job submission scripts to setup the software to use in your jobs. Generally, if you load modules in interactive sessions, these loaded modules do not carry over into any job submission scripts. Information on the available modules The key commands for getting information on modules are covered in more detail below. They are: module list module avail module spider module help module show module list The module list command will give the names of the modules and their versions you have presently loaded in your environment: auser@ln03:~> module list Currently Loaded Modules: 1) cce/11.0.4 6) perftools-base/21.02.0 2) craype/2.7.6 7) xpmem/2.2.40-7.0.1.0_2.7__g1d7a24d.shasta 3) craype-x86-rome 8) cray-mpich/8.1.4 4) libfabric/1.11.0.4.71 9) cray-libsci/21.04.1.1 5) craype-network-ofi 10) PrgEnv-cray/8.0.0 All users start with a default set of modules loaded corresponding to: The HPE Cray Compiling Environment (CCE): includes the HPE Cray clang and Fortran compilers HPE Cray MPICH: The HPE Cray MPI library HPE Cray LibSci: The HPE Cray numerical libraries (including BLAS/LAPACK and ScaLAPACK) module avail Finding out which software modules are currently available to load on the system is performed using the module avail command. To list all software modules currently available to load, use: auser@uan01:~> module avail ------------- /opt/cray/pe/lmod/modulefiles/mpi/crayclang/10.0/ofi/1.0/cray-mpich/8.0 ------------- cray-hdf5-parallel/1.12.0.3 (D) cray-parallel-netcdf/1.12.1.3 (D) cray-hdf5-parallel/1.12.0.7 cray-parallel-netcdf/1.12.1.7 ------------------------- /opt/cray/pe/lmod/modulefiles/perftools/21.02.0 ------------------------- perftools perftools-lite-events perftools-lite-hbm perftools-preload perftools-lite perftools-lite-gpu perftools-lite-loops ------------------- /opt/cray/pe/lmod/modulefiles/comnet/crayclang/10.0/ofi/1.0 ------------------- cray-mpich-abi/8.1.4 (D) cray-mpich-abi/8.1.9 cray-mpich/8.1.4 (L,D) cray-mpich/8.1.9 ---------------------------- /opt/cray/pe/lmod/modulefiles/net/ofi/1.0 ---------------------------- cray-openshmemx/11.2.0 (D) cray-openshmemx/11.3.3 ------------------------- /opt/cray/pe/lmod/modulefiles/cpu/x86-rome/1.0 -------------------------- cray-fftw/3.3.8.9 (D) cray-fftw/3.3.8.11 ---------------------- /opt/cray/pe/lmod/modulefiles/compiler/crayclang/10.0 ---------------------- cray-hdf5/1.12.0.3 (D) cray-hdf5/1.12.0.7 ---------------- /opt/cray/pe/lmod/modulefiles/mpi/aocc/2.2/ofi/1.0/cray-mpich/8.0 ---------------- cray-hdf5-parallel/1.12.0.3 cray-parallel-netcdf/1.12.1.3 ------------------------------ /usr/share/lmod/lmod/modulefiles/Core ------------------------------ lmod settarg ------------------------------- /opt/cray/pe/lmod/modulefiles/core -------------------------------- PrgEnv-aocc/8.0.0 (D) cray-ccdb/4.12.4 craype/2.7.6 (L,D) PrgEnv-aocc/8.1.0 cray-cti/2.13.6 (D) craype/2.7.10 PrgEnv-cray/8.0.0 (L,D) cray-cti/2.15.5 craypkg-gen/1.3.14 (D) PrgEnv-cray/8.1.0 cray-dsmml/0.1.4 (D) craypkg-gen/1.3.18 aturner@ln03:~> module avail ------------- /opt/cray/pe/lmod/modulefiles/mpi/crayclang/10.0/ofi/1.0/cray-mpich/8.0 ------------- cray-hdf5-parallel/1.12.0.3 (D) cray-parallel-netcdf/1.12.1.3 (D) cray-hdf5-parallel/1.12.0.7 cray-parallel-netcdf/1.12.1.7 ------------------------- /opt/cray/pe/lmod/modulefiles/perftools/21.02.0 ------------------------- perftools perftools-lite-events perftools-lite-hbm perftools-preload perftools-lite perftools-lite-gpu perftools-lite-loops ------------------- /opt/cray/pe/lmod/modulefiles/comnet/crayclang/10.0/ofi/1.0 ------------------- cray-mpich-abi/8.1.4 (D) cray-mpich-abi/8.1.9 cray-mpich/8.1.4 (L,D) cray-mpich/8.1.9 ---------------------------- /opt/cray/pe/lmod/modulefiles/net/ofi/1.0 ---------------------------- cray-openshmemx/11.2.0 (D) cray-openshmemx/11.3.3 ------------------------- /opt/cray/pe/lmod/modulefiles/cpu/x86-rome/1.0 -------------------------- cray-fftw/3.3.8.9 (D) cray-fftw/3.3.8.11 ---------------------- /opt/cray/pe/lmod/modulefiles/compiler/crayclang/10.0 ---------------------- cray-hdf5/1.12.0.3 (D) cray-hdf5/1.12.0.7 ---------------- /opt/cray/pe/lmod/modulefiles/mpi/aocc/2.2/ofi/1.0/cray-mpich/8.0 ---------------- cray-hdf5-parallel/1.12.0.3 cray-parallel-netcdf/1.12.1.3 ------------------------------ /usr/share/lmod/lmod/modulefiles/Core ------------------------------ lmod settarg ------------------------------- /opt/cray/pe/lmod/modulefiles/core -------------------------------- PrgEnv-aocc/8.0.0 (D) cray-ccdb/4.12.4 craype/2.7.6 (L,D) PrgEnv-aocc/8.1.0 cray-cti/2.13.6 (D) craype/2.7.10 PrgEnv-cray/8.0.0 (L,D) cray-cti/2.15.5 craypkg-gen/1.3.14 (D) PrgEnv-cray/8.1.0 cray-dsmml/0.1.4 (D) craypkg-gen/1.3.18 PrgEnv-gnu/8.0.0 (D) cray-dsmml/0.2.1 gcc/9.3.0 PrgEnv-gnu/8.1.0 cray-jemalloc/5.1.0.4 gcc/10.2.0 (D) aocc/2.2.0.1 (D) cray-libpals/1.0.17 gcc/10.3.0 aocc/3.0.0 cray-libsci/21.04.1.1 (L,D) gcc/11.2.0 atp/3.13.1 (D) cray-libsci/21.08.1.2 gdb4hpc/4.12.5 (D) atp/3.14.5 cray-pals/1.0.17 gdb4hpc/4.13.5 cce/11.0.4 (L,D) cray-pmi-lib/6.0.10 (D) iobuf/2.0.10 cce/12.0.3 cray-pmi-lib/6.0.13 papi/6.0.0.6 (D) cpe-cuda/21.09 cray-pmi/6.0.10 (D) papi/6.0.0.9 cpe/21.04 (D) cray-pmi/6.0.13 perftools-base/21.02.0 (L,D) cpe/21.09 cray-python/3.8.5.0 (D) perftools-base/21.09.0 cray-R/4.0.3.0 (D) cray-python/3.9.4.1 valgrind4hpc/2.11.1 (D) cray-R/4.1.1.0 cray-stat/4.10.1 (D) valgrind4hpc/2.12.4 cray-ccdb/4.11.1 (D) cray-stat/4.11.5 ---------------------- /opt/cray/pe/lmod/modulefiles/craype-targets/default ----------------------- craype-accel-amd-gfx908 craype-hugepages256M craype-network-none craype-accel-amd-gfx90a craype-hugepages2G craype-network-ofi (L) craype-accel-host craype-hugepages2M craype-network-ucx craype-accel-nvidia70 craype-hugepages32M craype-x86-milan craype-accel-nvidia80 craype-hugepages4M craype-x86-rome (L) craype-hugepages128M craype-hugepages512M craype-x86-trento craype-hugepages16M craype-hugepages64M craype-hugepages1G craype-hugepages8M -------------------------------------- /opt/cray/modulefiles -------------------------------------- cray-lustre-client/2.12.4.2_cray_63_g79cd827-7.0.1.0_8.1__g79cd827237.shasta cray-shasta-mlnx-firmware/1.0.8 dvs/2.12_4.0.112-7.0.1.0_15.1__ga97f35d9 libfabric/1.11.0.4.71 (L) xpmem/2.2.40-7.0.1.0_2.7__g1d7a24d.shasta (L) ---------------------------------------- /opt/modulefiles ----------------------------------------- aocc/2.2.0.1 aocc/3.0.0 cray-R/4.0.3.0 gcc/8.1.0 gcc/9.3.0 gcc/10.2.0 Where: L: Module is loaded D: Default Module Use \"module spider\" to find all possible modules and extensions. Use \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\". This will list all the names and versions of the modules that you can currently load. Note that other modules may be defined but not available to you as they depend on modules you do not have loaded. Lmod only shows modules that you can currently load, not all those that are defined. You can search for modules that are not currently visble to you using the module spider command - we cover this in more detail below. Note also, that not all modules may work in your account though due to, for example, licencing restrictions. You will notice that for many modules we have more than one version, each of which is identified by a version number. One of these versions is the default. As the service develops the default version will change and old versions of software may be deleted. You can list all the modules of a particular type by providing an argument to the module avail command. For example, to list all available versions of the HPE Cray FFTW library, use: auser@ln03:~> module avail cray-fftw ------------------------- /opt/cray/pe/lmod/modulefiles/cpu/x86-rome/1.0 -------------------------- cray-fftw/3.3.8.9 (D) cray-fftw/3.3.8.11 Where: D: Default Module Use \"module spider\" to find all possible modules and extensions. Use \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\". module spider The module spider command is used to find out which modules are defined on the system. Unlike module avail , this includes modules that are not currently able to be loaded due to the fact you have not yet loaded dependencies to make them directly available. module spider takes 3 forms: module spider without any arguments lists all modules defined on the system module spider <module> shows information on which versions of <module> are defined on the system module spider <module>/<version> shows information on the specific version of the module defined on the system, including dependencies that must be loaded before this module can be loaded (if any) If you cannot find a module that you expect to be on the system using module avail then you can use module spider to find out which dependencies you need to load to make the module available. For example, the module cray-netcdf-hdf5parallel is installed on ARCHER2 but it will not be found by module avail : auser@ln03:~> module avail cray-netcdf-hdf5parallel No module(s) or extension(s) found! Use \"module spider\" to find all possible modules and extensions. Use \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\". We can use module spider without any arguments to verify it exists and list the versions available: auser@ln03:~> module spider ----------------------------------------------------------------------------------------------- The following is a list of the modules and extensions currently available: ----------------------------------------------------------------------------------------------- ...output trimmed... cray-mpich-abi: cray-mpich-abi/8.1.4, cray-mpich-abi/8.1.9 cray-netcdf: cray-netcdf/4.7.4.3, cray-netcdf/4.7.4.7 cray-netcdf-hdf5parallel: cray-netcdf-hdf5parallel/4.7.4.3, cray-netcdf-hdf5parallel/4.7.4.7 cray-openshmemx: cray-openshmemx/11.2.0, cray-openshmemx/11.3.3 ...output trimmed... Now we know which versions are available, we can use module spider cray-netcdf-hdf5parallel/4.7.4.7 to find out how we can make it available: auser@ln03:~> module spider cray-netcdf-hdf5parallel/4.7.4.3 ----------------------------------------------------------------------------------------------- cray-netcdf-hdf5parallel: cray-netcdf-hdf5parallel/4.7.4.3 ----------------------------------------------------------------------------------------------- You will need to load all module(s) on any one of the lines below before the \"cray-netcdf-hdf5parallel/4.7.4.3\" module is available to load. aocc/2.2.0.1 cray-mpich/8.1.4 cray-hdf5-parallel/1.12.0.3 aocc/2.2.0.1 cray-mpich/8.1.4 cray-hdf5-parallel/1.12.0.7 aocc/2.2.0.1 cray-mpich/8.1.9 cray-hdf5-parallel/1.12.0.3 aocc/2.2.0.1 cray-mpich/8.1.9 cray-hdf5-parallel/1.12.0.7 aocc/3.0.0 cray-mpich/8.1.4 cray-hdf5-parallel/1.12.0.3 aocc/3.0.0 cray-mpich/8.1.4 cray-hdf5-parallel/1.12.0.7 aocc/3.0.0 cray-mpich/8.1.9 cray-hdf5-parallel/1.12.0.3 aocc/3.0.0 cray-mpich/8.1.9 cray-hdf5-parallel/1.12.0.7 cce/11.0.4 cray-mpich/8.1.4 cray-hdf5-parallel/1.12.0.3 cce/11.0.4 cray-mpich/8.1.4 cray-hdf5-parallel/1.12.0.7 cce/11.0.4 cray-mpich/8.1.9 cray-hdf5-parallel/1.12.0.3 cce/11.0.4 cray-mpich/8.1.9 cray-hdf5-parallel/1.12.0.7 cce/12.0.3 cray-mpich/8.1.4 cray-hdf5-parallel/1.12.0.3 cce/12.0.3 cray-mpich/8.1.4 cray-hdf5-parallel/1.12.0.7 cce/12.0.3 cray-mpich/8.1.9 cray-hdf5-parallel/1.12.0.3 cce/12.0.3 cray-mpich/8.1.9 cray-hdf5-parallel/1.12.0.7 craype-network-none cray-mpich/8.1.4 cray-hdf5-parallel/1.12.0.3 craype-network-none cray-mpich/8.1.4 cray-hdf5-parallel/1.12.0.7 craype-network-none cray-mpich/8.1.9 cray-hdf5-parallel/1.12.0.3 craype-network-none cray-mpich/8.1.9 cray-hdf5-parallel/1.12.0.7 craype-network-ofi cray-mpich/8.1.4 cray-hdf5-parallel/1.12.0.3 craype-network-ofi cray-mpich/8.1.4 cray-hdf5-parallel/1.12.0.7 craype-network-ofi cray-mpich/8.1.9 cray-hdf5-parallel/1.12.0.3 craype-network-ofi cray-mpich/8.1.9 cray-hdf5-parallel/1.12.0.7 craype-network-ucx cray-mpich/8.1.4 cray-hdf5-parallel/1.12.0.3 craype-network-ucx cray-mpich/8.1.4 cray-hdf5-parallel/1.12.0.7 craype-network-ucx cray-mpich/8.1.9 cray-hdf5-parallel/1.12.0.3 craype-network-ucx cray-mpich/8.1.9 cray-hdf5-parallel/1.12.0.7 gcc/10.2.0 cray-mpich/8.1.4 cray-hdf5-parallel/1.12.0.3 gcc/10.2.0 cray-mpich/8.1.4 cray-hdf5-parallel/1.12.0.7 gcc/10.2.0 cray-mpich/8.1.9 cray-hdf5-parallel/1.12.0.3 gcc/10.2.0 cray-mpich/8.1.9 cray-hdf5-parallel/1.12.0.7 gcc/10.3.0 cray-mpich/8.1.4 cray-hdf5-parallel/1.12.0.3 gcc/10.3.0 cray-mpich/8.1.4 cray-hdf5-parallel/1.12.0.7 gcc/10.3.0 cray-mpich/8.1.9 cray-hdf5-parallel/1.12.0.3 gcc/10.3.0 cray-mpich/8.1.9 cray-hdf5-parallel/1.12.0.7 gcc/11.2.0 cray-mpich/8.1.4 cray-hdf5-parallel/1.12.0.3 gcc/11.2.0 cray-mpich/8.1.4 cray-hdf5-parallel/1.12.0.7 gcc/11.2.0 cray-mpich/8.1.9 cray-hdf5-parallel/1.12.0.3 gcc/11.2.0 cray-mpich/8.1.9 cray-hdf5-parallel/1.12.0.7 gcc/9.3.0 cray-mpich/8.1.4 cray-hdf5-parallel/1.12.0.3 gcc/9.3.0 cray-mpich/8.1.4 cray-hdf5-parallel/1.12.0.7 gcc/9.3.0 cray-mpich/8.1.9 cray-hdf5-parallel/1.12.0.3 gcc/9.3.0 cray-mpich/8.1.9 cray-hdf5-parallel/1.12.0.7 Help: Release info: /opt/cray/pe/netcdf-hdf5parallel/4.7.4.7/release_info There is a lot of information here, but what the output is essentailly telling us is that in order to have cray-netcdf-hdf5parallel/4.7.4.3 available to load we need to have loaded a compiler (any version of CCE, GCC or AOCC), an MPI library (any version of cray-mpich) and cray-hdf5-parallel loaded. As we always have a compiler and MPI library loaded, we can satisfy all of the dependencies by loading cray-hdf5-parallel , and then we can use module avail cray-netcdf-hdf5parallel again to show that the module is now available to load: auser@ln03:~> module load cray-hdf5-parallel auser@ln03:~> module avail cray-netcdf-hdf5parallel ----- /opt/cray/pe/lmod/modulefiles/hdf5-parallel/crayclang/10.0/ofi/1.0/cray-mpich/8.0/cray-hdf5-parallel/1.12.0 ------ cray-netcdf-hdf5parallel/4.7.4.3 (D) cray-netcdf-hdf5parallel/4.7.4.7 Where: D: Default Module Use \"module spider\" to find all possible modules and extensions. Use \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\". module help If you want more info on any of the modules, you can use the module help command: auser@ln03:~> module help gromacs module show The module show command reveals what operations the module actually performs to change your environment when it is loaded. For example, for the default FFTW module: auser@ln03:~> module show gromacs [...] Loading, removing and swapping modules To change your environment and make different software available you use the following commands which we cover in more detail below. module load module remove module swap module load To load a module to use the module load command. For example, to load the default version of GROMACS into your environment, use: auser@ln03:~> module load gromacs Once you have done this, your environment will be setup to use GROMACS. The above command will load the default version of GROMACS. If you need a specific version of the software, you can add more information: auser@uan01:~> module load gromacs/2021.2 will load GROMACS version 2021.2 into your environment, regardless of the default. module remove If you want to remove software from your environment, module remove will remove a loaded module: auser@uan01:~> module remove gromacs will unload what ever version of gromacs you might have loaded (even if it is not the default). module swap There are many situations in which you might want to change the presently loaded version to a different one, such as trying the latest version which is not yet the default or using a legacy version to keep compatibility with old data. This can be achieved most easily by using module swap oldmodule newmodule . For example, to swap from the default CCE (cray) compiler environment to the GCC (gnu) compiler environment, you would use: auser@ln03:~> module swap PrgEnv-cray PrgEnv-gnu You did not need to specify the version of the loaded module in your current environment as this can be inferred as it will be the only one you have loaded. Tip module swap is most commonly used on ARCHER2 to switch between different compiler environments, for example, switching from the HPE Cray Compiler Environment (CCE, PrgEnv-cray) to the Gnu compilers (GCC, PrgEnv-gnu). The available compiler environments are discussed in more detail in the Application Development Environment section. Shell environment overview When you log in to ARCHER2, you are using the bash shell by default. As with any software, the bash shell has loaded a set of environment variables that can be listed by executing printenv or export . The environment variables listed before are useful to define the behaviour of the software you run. For instance, OMP_NUM_THREADS define the number of threads. To define an environment variable, you need to execute: export OMP_NUM_THREADS=4 Please note there are no blanks between the variable name, the assignation symbol, and the value. If the value is a string, enclose the string in double quotation marks. You can show the value of a specific environment variable if you print it: echo $OMP_NUM_THREADS Do not forget the dollar symbol. To remove an environment variable, just execute: unset OMP_NUM_THREADS Note that the dollar symbol is not included when you use the unset command.","title":"Software environment"},{"location":"user-guide/sw-environment/#software-environment","text":"The software environment on ARCHER2 is managed using the Lmod software. Selecting which software is available in your environment is primarily controlled through the module command. By loading and switching software modules you control which software and versions are available to you. Information A module is a self-contained description of a software package -- it contains the settings required to run a software package and, usually, encodes required dependencies on other software packages. By default, all users on ARCHER2 start with the default software environment loaded. Software modules on ARCHER2 are provided by both HPE (usually known as the HPE Cray Programming Environment, CPE ) and by EPCC, who provide the Service Provision, and Computational Science and Engineering services. In this section, we provide: A brief overview of the module command A brief description of how the module command manipulates your environment","title":"Software environment"},{"location":"user-guide/sw-environment/#using-the-module-command","text":"We only cover basic usage of the Lmod module command here. For full documentation please see the Lmod documentation The module command takes a subcommand to indicate what operation you wish to perform. Common subcommands are: module list [name] - List modules currently loaded in your environment, optionally filtered by [name] module avail [name] - List modules available, optionally filtered by [name] module spider [name][/version] - Search available modules (including hidden modules) and provide information on modules module load name - Load the module called name into your environment module remove name - Remove the module called name from your environment module swap old new - Swap module new for module old in your environment module help name - Show help information on module name module show name - List what module name actually does to your environment These are described in more detail below. Tip Lmod allows you to use the ml shortcut command. Without any arguments, ml behaves like module list ; when a module name is specified to ml , ml behaves like module load . Note You will often have to include module commands in any job submission scripts to setup the software to use in your jobs. Generally, if you load modules in interactive sessions, these loaded modules do not carry over into any job submission scripts.","title":"Using the module command"},{"location":"user-guide/sw-environment/#information-on-the-available-modules","text":"The key commands for getting information on modules are covered in more detail below. They are: module list module avail module spider module help module show","title":"Information on the available modules"},{"location":"user-guide/sw-environment/#module-list","text":"The module list command will give the names of the modules and their versions you have presently loaded in your environment: auser@ln03:~> module list Currently Loaded Modules: 1) cce/11.0.4 6) perftools-base/21.02.0 2) craype/2.7.6 7) xpmem/2.2.40-7.0.1.0_2.7__g1d7a24d.shasta 3) craype-x86-rome 8) cray-mpich/8.1.4 4) libfabric/1.11.0.4.71 9) cray-libsci/21.04.1.1 5) craype-network-ofi 10) PrgEnv-cray/8.0.0 All users start with a default set of modules loaded corresponding to: The HPE Cray Compiling Environment (CCE): includes the HPE Cray clang and Fortran compilers HPE Cray MPICH: The HPE Cray MPI library HPE Cray LibSci: The HPE Cray numerical libraries (including BLAS/LAPACK and ScaLAPACK)","title":"module list"},{"location":"user-guide/sw-environment/#module-avail","text":"Finding out which software modules are currently available to load on the system is performed using the module avail command. To list all software modules currently available to load, use: auser@uan01:~> module avail ------------- /opt/cray/pe/lmod/modulefiles/mpi/crayclang/10.0/ofi/1.0/cray-mpich/8.0 ------------- cray-hdf5-parallel/1.12.0.3 (D) cray-parallel-netcdf/1.12.1.3 (D) cray-hdf5-parallel/1.12.0.7 cray-parallel-netcdf/1.12.1.7 ------------------------- /opt/cray/pe/lmod/modulefiles/perftools/21.02.0 ------------------------- perftools perftools-lite-events perftools-lite-hbm perftools-preload perftools-lite perftools-lite-gpu perftools-lite-loops ------------------- /opt/cray/pe/lmod/modulefiles/comnet/crayclang/10.0/ofi/1.0 ------------------- cray-mpich-abi/8.1.4 (D) cray-mpich-abi/8.1.9 cray-mpich/8.1.4 (L,D) cray-mpich/8.1.9 ---------------------------- /opt/cray/pe/lmod/modulefiles/net/ofi/1.0 ---------------------------- cray-openshmemx/11.2.0 (D) cray-openshmemx/11.3.3 ------------------------- /opt/cray/pe/lmod/modulefiles/cpu/x86-rome/1.0 -------------------------- cray-fftw/3.3.8.9 (D) cray-fftw/3.3.8.11 ---------------------- /opt/cray/pe/lmod/modulefiles/compiler/crayclang/10.0 ---------------------- cray-hdf5/1.12.0.3 (D) cray-hdf5/1.12.0.7 ---------------- /opt/cray/pe/lmod/modulefiles/mpi/aocc/2.2/ofi/1.0/cray-mpich/8.0 ---------------- cray-hdf5-parallel/1.12.0.3 cray-parallel-netcdf/1.12.1.3 ------------------------------ /usr/share/lmod/lmod/modulefiles/Core ------------------------------ lmod settarg ------------------------------- /opt/cray/pe/lmod/modulefiles/core -------------------------------- PrgEnv-aocc/8.0.0 (D) cray-ccdb/4.12.4 craype/2.7.6 (L,D) PrgEnv-aocc/8.1.0 cray-cti/2.13.6 (D) craype/2.7.10 PrgEnv-cray/8.0.0 (L,D) cray-cti/2.15.5 craypkg-gen/1.3.14 (D) PrgEnv-cray/8.1.0 cray-dsmml/0.1.4 (D) craypkg-gen/1.3.18 aturner@ln03:~> module avail ------------- /opt/cray/pe/lmod/modulefiles/mpi/crayclang/10.0/ofi/1.0/cray-mpich/8.0 ------------- cray-hdf5-parallel/1.12.0.3 (D) cray-parallel-netcdf/1.12.1.3 (D) cray-hdf5-parallel/1.12.0.7 cray-parallel-netcdf/1.12.1.7 ------------------------- /opt/cray/pe/lmod/modulefiles/perftools/21.02.0 ------------------------- perftools perftools-lite-events perftools-lite-hbm perftools-preload perftools-lite perftools-lite-gpu perftools-lite-loops ------------------- /opt/cray/pe/lmod/modulefiles/comnet/crayclang/10.0/ofi/1.0 ------------------- cray-mpich-abi/8.1.4 (D) cray-mpich-abi/8.1.9 cray-mpich/8.1.4 (L,D) cray-mpich/8.1.9 ---------------------------- /opt/cray/pe/lmod/modulefiles/net/ofi/1.0 ---------------------------- cray-openshmemx/11.2.0 (D) cray-openshmemx/11.3.3 ------------------------- /opt/cray/pe/lmod/modulefiles/cpu/x86-rome/1.0 -------------------------- cray-fftw/3.3.8.9 (D) cray-fftw/3.3.8.11 ---------------------- /opt/cray/pe/lmod/modulefiles/compiler/crayclang/10.0 ---------------------- cray-hdf5/1.12.0.3 (D) cray-hdf5/1.12.0.7 ---------------- /opt/cray/pe/lmod/modulefiles/mpi/aocc/2.2/ofi/1.0/cray-mpich/8.0 ---------------- cray-hdf5-parallel/1.12.0.3 cray-parallel-netcdf/1.12.1.3 ------------------------------ /usr/share/lmod/lmod/modulefiles/Core ------------------------------ lmod settarg ------------------------------- /opt/cray/pe/lmod/modulefiles/core -------------------------------- PrgEnv-aocc/8.0.0 (D) cray-ccdb/4.12.4 craype/2.7.6 (L,D) PrgEnv-aocc/8.1.0 cray-cti/2.13.6 (D) craype/2.7.10 PrgEnv-cray/8.0.0 (L,D) cray-cti/2.15.5 craypkg-gen/1.3.14 (D) PrgEnv-cray/8.1.0 cray-dsmml/0.1.4 (D) craypkg-gen/1.3.18 PrgEnv-gnu/8.0.0 (D) cray-dsmml/0.2.1 gcc/9.3.0 PrgEnv-gnu/8.1.0 cray-jemalloc/5.1.0.4 gcc/10.2.0 (D) aocc/2.2.0.1 (D) cray-libpals/1.0.17 gcc/10.3.0 aocc/3.0.0 cray-libsci/21.04.1.1 (L,D) gcc/11.2.0 atp/3.13.1 (D) cray-libsci/21.08.1.2 gdb4hpc/4.12.5 (D) atp/3.14.5 cray-pals/1.0.17 gdb4hpc/4.13.5 cce/11.0.4 (L,D) cray-pmi-lib/6.0.10 (D) iobuf/2.0.10 cce/12.0.3 cray-pmi-lib/6.0.13 papi/6.0.0.6 (D) cpe-cuda/21.09 cray-pmi/6.0.10 (D) papi/6.0.0.9 cpe/21.04 (D) cray-pmi/6.0.13 perftools-base/21.02.0 (L,D) cpe/21.09 cray-python/3.8.5.0 (D) perftools-base/21.09.0 cray-R/4.0.3.0 (D) cray-python/3.9.4.1 valgrind4hpc/2.11.1 (D) cray-R/4.1.1.0 cray-stat/4.10.1 (D) valgrind4hpc/2.12.4 cray-ccdb/4.11.1 (D) cray-stat/4.11.5 ---------------------- /opt/cray/pe/lmod/modulefiles/craype-targets/default ----------------------- craype-accel-amd-gfx908 craype-hugepages256M craype-network-none craype-accel-amd-gfx90a craype-hugepages2G craype-network-ofi (L) craype-accel-host craype-hugepages2M craype-network-ucx craype-accel-nvidia70 craype-hugepages32M craype-x86-milan craype-accel-nvidia80 craype-hugepages4M craype-x86-rome (L) craype-hugepages128M craype-hugepages512M craype-x86-trento craype-hugepages16M craype-hugepages64M craype-hugepages1G craype-hugepages8M -------------------------------------- /opt/cray/modulefiles -------------------------------------- cray-lustre-client/2.12.4.2_cray_63_g79cd827-7.0.1.0_8.1__g79cd827237.shasta cray-shasta-mlnx-firmware/1.0.8 dvs/2.12_4.0.112-7.0.1.0_15.1__ga97f35d9 libfabric/1.11.0.4.71 (L) xpmem/2.2.40-7.0.1.0_2.7__g1d7a24d.shasta (L) ---------------------------------------- /opt/modulefiles ----------------------------------------- aocc/2.2.0.1 aocc/3.0.0 cray-R/4.0.3.0 gcc/8.1.0 gcc/9.3.0 gcc/10.2.0 Where: L: Module is loaded D: Default Module Use \"module spider\" to find all possible modules and extensions. Use \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\". This will list all the names and versions of the modules that you can currently load. Note that other modules may be defined but not available to you as they depend on modules you do not have loaded. Lmod only shows modules that you can currently load, not all those that are defined. You can search for modules that are not currently visble to you using the module spider command - we cover this in more detail below. Note also, that not all modules may work in your account though due to, for example, licencing restrictions. You will notice that for many modules we have more than one version, each of which is identified by a version number. One of these versions is the default. As the service develops the default version will change and old versions of software may be deleted. You can list all the modules of a particular type by providing an argument to the module avail command. For example, to list all available versions of the HPE Cray FFTW library, use: auser@ln03:~> module avail cray-fftw ------------------------- /opt/cray/pe/lmod/modulefiles/cpu/x86-rome/1.0 -------------------------- cray-fftw/3.3.8.9 (D) cray-fftw/3.3.8.11 Where: D: Default Module Use \"module spider\" to find all possible modules and extensions. Use \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\".","title":"module avail"},{"location":"user-guide/sw-environment/#module-spider","text":"The module spider command is used to find out which modules are defined on the system. Unlike module avail , this includes modules that are not currently able to be loaded due to the fact you have not yet loaded dependencies to make them directly available. module spider takes 3 forms: module spider without any arguments lists all modules defined on the system module spider <module> shows information on which versions of <module> are defined on the system module spider <module>/<version> shows information on the specific version of the module defined on the system, including dependencies that must be loaded before this module can be loaded (if any) If you cannot find a module that you expect to be on the system using module avail then you can use module spider to find out which dependencies you need to load to make the module available. For example, the module cray-netcdf-hdf5parallel is installed on ARCHER2 but it will not be found by module avail : auser@ln03:~> module avail cray-netcdf-hdf5parallel No module(s) or extension(s) found! Use \"module spider\" to find all possible modules and extensions. Use \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\". We can use module spider without any arguments to verify it exists and list the versions available: auser@ln03:~> module spider ----------------------------------------------------------------------------------------------- The following is a list of the modules and extensions currently available: ----------------------------------------------------------------------------------------------- ...output trimmed... cray-mpich-abi: cray-mpich-abi/8.1.4, cray-mpich-abi/8.1.9 cray-netcdf: cray-netcdf/4.7.4.3, cray-netcdf/4.7.4.7 cray-netcdf-hdf5parallel: cray-netcdf-hdf5parallel/4.7.4.3, cray-netcdf-hdf5parallel/4.7.4.7 cray-openshmemx: cray-openshmemx/11.2.0, cray-openshmemx/11.3.3 ...output trimmed... Now we know which versions are available, we can use module spider cray-netcdf-hdf5parallel/4.7.4.7 to find out how we can make it available: auser@ln03:~> module spider cray-netcdf-hdf5parallel/4.7.4.3 ----------------------------------------------------------------------------------------------- cray-netcdf-hdf5parallel: cray-netcdf-hdf5parallel/4.7.4.3 ----------------------------------------------------------------------------------------------- You will need to load all module(s) on any one of the lines below before the \"cray-netcdf-hdf5parallel/4.7.4.3\" module is available to load. aocc/2.2.0.1 cray-mpich/8.1.4 cray-hdf5-parallel/1.12.0.3 aocc/2.2.0.1 cray-mpich/8.1.4 cray-hdf5-parallel/1.12.0.7 aocc/2.2.0.1 cray-mpich/8.1.9 cray-hdf5-parallel/1.12.0.3 aocc/2.2.0.1 cray-mpich/8.1.9 cray-hdf5-parallel/1.12.0.7 aocc/3.0.0 cray-mpich/8.1.4 cray-hdf5-parallel/1.12.0.3 aocc/3.0.0 cray-mpich/8.1.4 cray-hdf5-parallel/1.12.0.7 aocc/3.0.0 cray-mpich/8.1.9 cray-hdf5-parallel/1.12.0.3 aocc/3.0.0 cray-mpich/8.1.9 cray-hdf5-parallel/1.12.0.7 cce/11.0.4 cray-mpich/8.1.4 cray-hdf5-parallel/1.12.0.3 cce/11.0.4 cray-mpich/8.1.4 cray-hdf5-parallel/1.12.0.7 cce/11.0.4 cray-mpich/8.1.9 cray-hdf5-parallel/1.12.0.3 cce/11.0.4 cray-mpich/8.1.9 cray-hdf5-parallel/1.12.0.7 cce/12.0.3 cray-mpich/8.1.4 cray-hdf5-parallel/1.12.0.3 cce/12.0.3 cray-mpich/8.1.4 cray-hdf5-parallel/1.12.0.7 cce/12.0.3 cray-mpich/8.1.9 cray-hdf5-parallel/1.12.0.3 cce/12.0.3 cray-mpich/8.1.9 cray-hdf5-parallel/1.12.0.7 craype-network-none cray-mpich/8.1.4 cray-hdf5-parallel/1.12.0.3 craype-network-none cray-mpich/8.1.4 cray-hdf5-parallel/1.12.0.7 craype-network-none cray-mpich/8.1.9 cray-hdf5-parallel/1.12.0.3 craype-network-none cray-mpich/8.1.9 cray-hdf5-parallel/1.12.0.7 craype-network-ofi cray-mpich/8.1.4 cray-hdf5-parallel/1.12.0.3 craype-network-ofi cray-mpich/8.1.4 cray-hdf5-parallel/1.12.0.7 craype-network-ofi cray-mpich/8.1.9 cray-hdf5-parallel/1.12.0.3 craype-network-ofi cray-mpich/8.1.9 cray-hdf5-parallel/1.12.0.7 craype-network-ucx cray-mpich/8.1.4 cray-hdf5-parallel/1.12.0.3 craype-network-ucx cray-mpich/8.1.4 cray-hdf5-parallel/1.12.0.7 craype-network-ucx cray-mpich/8.1.9 cray-hdf5-parallel/1.12.0.3 craype-network-ucx cray-mpich/8.1.9 cray-hdf5-parallel/1.12.0.7 gcc/10.2.0 cray-mpich/8.1.4 cray-hdf5-parallel/1.12.0.3 gcc/10.2.0 cray-mpich/8.1.4 cray-hdf5-parallel/1.12.0.7 gcc/10.2.0 cray-mpich/8.1.9 cray-hdf5-parallel/1.12.0.3 gcc/10.2.0 cray-mpich/8.1.9 cray-hdf5-parallel/1.12.0.7 gcc/10.3.0 cray-mpich/8.1.4 cray-hdf5-parallel/1.12.0.3 gcc/10.3.0 cray-mpich/8.1.4 cray-hdf5-parallel/1.12.0.7 gcc/10.3.0 cray-mpich/8.1.9 cray-hdf5-parallel/1.12.0.3 gcc/10.3.0 cray-mpich/8.1.9 cray-hdf5-parallel/1.12.0.7 gcc/11.2.0 cray-mpich/8.1.4 cray-hdf5-parallel/1.12.0.3 gcc/11.2.0 cray-mpich/8.1.4 cray-hdf5-parallel/1.12.0.7 gcc/11.2.0 cray-mpich/8.1.9 cray-hdf5-parallel/1.12.0.3 gcc/11.2.0 cray-mpich/8.1.9 cray-hdf5-parallel/1.12.0.7 gcc/9.3.0 cray-mpich/8.1.4 cray-hdf5-parallel/1.12.0.3 gcc/9.3.0 cray-mpich/8.1.4 cray-hdf5-parallel/1.12.0.7 gcc/9.3.0 cray-mpich/8.1.9 cray-hdf5-parallel/1.12.0.3 gcc/9.3.0 cray-mpich/8.1.9 cray-hdf5-parallel/1.12.0.7 Help: Release info: /opt/cray/pe/netcdf-hdf5parallel/4.7.4.7/release_info There is a lot of information here, but what the output is essentailly telling us is that in order to have cray-netcdf-hdf5parallel/4.7.4.3 available to load we need to have loaded a compiler (any version of CCE, GCC or AOCC), an MPI library (any version of cray-mpich) and cray-hdf5-parallel loaded. As we always have a compiler and MPI library loaded, we can satisfy all of the dependencies by loading cray-hdf5-parallel , and then we can use module avail cray-netcdf-hdf5parallel again to show that the module is now available to load: auser@ln03:~> module load cray-hdf5-parallel auser@ln03:~> module avail cray-netcdf-hdf5parallel ----- /opt/cray/pe/lmod/modulefiles/hdf5-parallel/crayclang/10.0/ofi/1.0/cray-mpich/8.0/cray-hdf5-parallel/1.12.0 ------ cray-netcdf-hdf5parallel/4.7.4.3 (D) cray-netcdf-hdf5parallel/4.7.4.7 Where: D: Default Module Use \"module spider\" to find all possible modules and extensions. Use \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\".","title":"module spider"},{"location":"user-guide/sw-environment/#module-help","text":"If you want more info on any of the modules, you can use the module help command: auser@ln03:~> module help gromacs","title":"module help"},{"location":"user-guide/sw-environment/#module-show","text":"The module show command reveals what operations the module actually performs to change your environment when it is loaded. For example, for the default FFTW module: auser@ln03:~> module show gromacs [...]","title":"module show"},{"location":"user-guide/sw-environment/#loading-removing-and-swapping-modules","text":"To change your environment and make different software available you use the following commands which we cover in more detail below. module load module remove module swap","title":"Loading, removing and swapping modules"},{"location":"user-guide/sw-environment/#module-load","text":"To load a module to use the module load command. For example, to load the default version of GROMACS into your environment, use: auser@ln03:~> module load gromacs Once you have done this, your environment will be setup to use GROMACS. The above command will load the default version of GROMACS. If you need a specific version of the software, you can add more information: auser@uan01:~> module load gromacs/2021.2 will load GROMACS version 2021.2 into your environment, regardless of the default.","title":"module load"},{"location":"user-guide/sw-environment/#module-remove","text":"If you want to remove software from your environment, module remove will remove a loaded module: auser@uan01:~> module remove gromacs will unload what ever version of gromacs you might have loaded (even if it is not the default).","title":"module remove"},{"location":"user-guide/sw-environment/#module-swap","text":"There are many situations in which you might want to change the presently loaded version to a different one, such as trying the latest version which is not yet the default or using a legacy version to keep compatibility with old data. This can be achieved most easily by using module swap oldmodule newmodule . For example, to swap from the default CCE (cray) compiler environment to the GCC (gnu) compiler environment, you would use: auser@ln03:~> module swap PrgEnv-cray PrgEnv-gnu You did not need to specify the version of the loaded module in your current environment as this can be inferred as it will be the only one you have loaded. Tip module swap is most commonly used on ARCHER2 to switch between different compiler environments, for example, switching from the HPE Cray Compiler Environment (CCE, PrgEnv-cray) to the Gnu compilers (GCC, PrgEnv-gnu). The available compiler environments are discussed in more detail in the Application Development Environment section.","title":"module swap"},{"location":"user-guide/sw-environment/#shell-environment-overview","text":"When you log in to ARCHER2, you are using the bash shell by default. As with any software, the bash shell has loaded a set of environment variables that can be listed by executing printenv or export . The environment variables listed before are useful to define the behaviour of the software you run. For instance, OMP_NUM_THREADS define the number of threads. To define an environment variable, you need to execute: export OMP_NUM_THREADS=4 Please note there are no blanks between the variable name, the assignation symbol, and the value. If the value is a string, enclose the string in double quotation marks. You can show the value of a specific environment variable if you print it: echo $OMP_NUM_THREADS Do not forget the dollar symbol. To remove an environment variable, just execute: unset OMP_NUM_THREADS Note that the dollar symbol is not included when you use the unset command.","title":"Shell environment overview"},{"location":"user-guide/tuning/","text":"Performance tuning MPI The vast majority of parallel scientific applications use the MPI library as the main way to implement parallelism; it is used so universally that the Cray compiler wrappers on ARCHER2 link to the Cray MPI library by default. Unlike other clusters you may have used, there is no choice of MPI library on ARCHER2: regardless of what compiler you are using, your program will use Cray MPI. This is because the Slingshot network on ARCHER2 is Cray-specific and significant effort has been put in by Cray software engineers to optimise the MPI performance on their Shasta systems. Here we list a number of suggestions for improving the performance of your MPI programs on ARCHER2. Although MPI programs are capable of scaling very well due to the bespoke communications hardware and software, the details of how a program calls MPI can have significant effects on achieved performance. Note Many of these tips are actually quite generic and should be beneficial to any MPI program; however, they all become much more important when running on very large numbers of processes on a machine the size of ARCHER2. Synchronous vs asynchronous communications MPI_Send A standard way to send data in MPI is using MPI_Send (aptly called standard send ). Somewhat confusingly, MPI is allowed to choose how to implement this in two different ways: Synchronously The sending process waits until a matching receive has been posted, i.e. it operates like MPI_Ssend . This clearly has the risk of deadlock if no receive is ever issued. Asynchronously MPI makes a copy of the message into an internal buffer and returns straight away without waiting for a matching receive; the message may actually be delivered later on. This is like the behaviour of the the buffered send routine MPI_Bsend . The rationale is that MPI, rather than the user, should decide how best to send a message. In practice, what typically happens is that MPI tries to use an asynchronous approach via the eager protocol: the message is sent directly to a preallocated buffer on the receiver and the routine returns immediately afterwards. Clearly there is a limit on how much space can be reserved for this, so: small messages will be sent asynchronously; large messages will be sent synchronously. The threshold is often termed the eager limit which is fixed for the entire run of your program. It will have some default setting which varies from system to system, but might be around 8K bytes. Implications An MPI program will typically run faster if MPI_Send is implemented asynchronously using the eager protocol since synchronisation between sender and receive is much reduced. However, you should never assume that MPI_Send buffers your message, so if you have concerns about deadlock you will need to use the non-blocking variant MPI_Isend to guarantee that the send routine returns control to you immediately even if there is no matching receive. It is not enough to say deadlock is an issue in principle, but it runs OK on my laptop so there is no problem in practice . The eager limit is system-dependent so the fact that a message happens to be buffered on your laptop is no guarantee it will be buffered on ARCHER2. To check that you have a correct code, replace all instances of MPI_Send / MPI_Isend with MPI_Ssend / MPI_Issend . A correct MPI program should still run correctly when all references to standard send are replaced by synchronous send (since MPI is allowed to implement standard send as synchronous send). Tuning performance With most MPI libraries you should be able to alter the default value of the eager limit at runtime, perhaps via an environment variable or a command-line argument to mpirun . The advice for tuning the performance of MPI_Send is find out what the distribution of message sizes for MPI_Send is (a profiling tool may be useful here); this applies to MPI_Isend as well: even in the non-blocking form, which can help to weaken synchronisation between sender and receiver, the amount of hand-shaking required is much reduced if the eager protocol is used; find out from the system documentation how to alter the value of the eager limit (there is no standardised way to set it); set the eager limit to a value larger than your typical message size -- you may need to add a small amount, say a few hundred bytes, to allow for any additional header information that is added to each message; measure the performance before and after to check that it has improved. Note It cannot be stressed strongly enough that although the performance may be affected by the value of the eager limit, the functionality of your program should be unaffected. If changing the eager limit affects the correctness of your program (e.g. whether or not it deadlocks) then you have an incorrect MPI program . Setting the eager limit on ARCHER2 On ARCHER2, things are a little more complicated. Although the eager limit defaults to 16KiB, messages up to 256KiB are sent asynchronously because they are actually sent as a number of smaller messages. To send even larger messages asynchronously, alter the value of FI_OFI_RXM_SAR_LIMIT in your job submission script, e.g. to set to 512KiB: export FI_OFI_RXM_SAR_LIMIT=524288 You can also control the size of the smaller messages by altering the value of FI_OFI_RXM_BUFFER_SIZE in your job submission script, e.g. to set to 128KiB: export FI_OFI_RXM_BUFFER_SIZE=131072 A different protocol is used for messages between two processes on the same node. The default eager limit for these is 8K. Although the performance of on-node messages is unlikely to be a limiting factor for your program you can change this value, e.g. to set to 16KiB: export MPICH_SMP_SINGLE_COPY_SIZE=16384 Collective operations Many of the collective operations that are commonly required by parallel scientific programs, i.e. operations that involve a group of processes, are already implemented in MPI. The canonical operation is perhaps adding up a double precision number across all MPI processes, which is best achieved by a reduction operation : MPI_Allreduce(&x, &xsum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD); This will be implemented using an efficient algorithm, for example based on a binary tree. Using such divide-and-conquer approaches typically results in an algorithm whose execution time on P processes scales as log_2(P) ; compare this to a naive approach where every process sends its input to rank 0 where the time will scale as P . This might not be significant on your laptop, but even on as few as 1000 processes the tree-based algorithm will already be around 100 times faster. So, the basic advice is always use a collective routine to implement your communications pattern if at all possible. In real MPI applications, collective operations are often called on a small amount of data, for example a global reduction of a single variable. In these cases, the time taken will be dominated by message latency and the first port of call when looking at performance optimisation is to call them as infrequently as possible! If you are simply printing diagnostics to the screen in an iterative loop, consider doing this less frequently, e.g every ten iterations, or even not at all (although you should easily be able to turn diagnostics on again for future debugging). If you are computing some termination criterion, it may actually be faster overall to compute it and check for convergence infrequently, e.g. every ten iterations, even although this means that your program could run for up to 9 extra iterations. If possible, group data into a single buffer and call a single reduction with count > 1; two reductions with count = 1 will take almost exactly twice as long as a single reduction with count = 2. For example, if you only need to output a sequence of summed data at the end of the run, store the partial totals in an array and do a single reduction right at the end. Sometimes, the collective routines available may not appear to do exactly what you want. However, they can sometimes be used with a small amount of additional programming work: To operate on a subset of processes, create sub-communicators containing the relevant subset(s) and use these communicators instead of MPI_COMM_WORLD . Useful functions for communicator management include: MPI_Comm_split is the most general routine; MPI_Comm_split_type can be used to create a separate communicator for each shared-memory node with split type = MPI_COMM_TYPE_SHARED ; MPI_Cart_sub can divide a Cartesian communicator into regular slices. If the communication pattern is what you want, but the data on each process is not arranged in the required layout, consider using MPI derived data types for the input and/or output. This can be useful, for example, if you want to communicate non-contiguous data such as a subsection of a multidimensional array although care must be taken in defining these types to ensure they have the correct extents. Another example would be using MPI_Allreduce to add up an integer and a double-precision variable using a single call by putting them together into a C struct and defining a matching MPI datatype using MPI_Type_create_struct . Here you would also have to provide MPI with a custom reduction operation using MPI_Op_create . Many MPI programs call MPI_Barrier to explicitly synchronise all the processes. Although this can be useful for getting reliable performance timings, it is rare in practice to find a program where the call is actually needed for correctness. For example, you may see: // Ensure the input x is available on all processes MPI_Barrier(MPI_COMM_WORLD); // Perform a global reduction operation MPI_Allreduce(&x, &xsum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD); // Ensure the result xsum is available on all processes MPI_Barrier(MPI_COMM_WORLD); Neither of these barriers are needed as the reduction operation performs all the required synchronisation. If removing a barrier from your MPI code makes it run incorrectly, then this should ring alarm bells -- it is often a symptom of an underlying bug that is simply being masked by the barrier. For example, if you use non-blocking calls such as MPI_Irecv then it is the programmer's responsibility to ensure that these are completed at some later point, for example by calling MPI_Wait on the returned request object. A common bug is to forget to do this, in which case you might be reading the contents of the receive buffer before the incoming message has arrived (e.g. if the sender is running late). Calling a barrier may mask this bug as it will make all the processes wait for each other, perhaps allowing the late sender to catch up. However, this is not guaranteed so the real solution is to call the non-blocking communications correctly. One of the few times when a barrier may be required is if processes are communicating with each other via some other non-MPI method, e.g. via the file system. If you want processes to sequentially open, append to, then close the same file then barriers are a simple way to achieve this: for (i=0; i < size; i++) { if (rank == i) append_data_to_file(data, filename); MPI_Barrier(comm); } but this is really something of a special case. Global synchronisation may be required if you are using more advanced techniques such as hybrid MPI/OpenMP or single-sided MPI communication with put and get, but typically you should be using specialised routines such as MPI_Win_fence rather than MPI_Barrier . Tip If you run a performance profiler on your code and it shows a lot of time being spent in a collective operation such as MPI_Allreduce , this is not necessarily a sign that the reduction operation itself is the bottleneck. This is often a symptom of load imbalance : even if a reduction operation is efficiently implemented, it may take a long time to complete if the MPI processes do not all call it at the same time. MPI_Allreduce synchronises across processes so will have to wait for all the processes to call it before it can complete. A single slow process will therefore adversely impact the performance of your entire parallel program. OpenMP There are a variety of possible issues that can result in poor performance of OpenMP programs. These include: Sequential code Code outside of parallel regions is executed sequentially by the master thread. Idle threads If different threads have different amounts of computation to do, then threads may be idle whenever a barrier is encountered, for example at the end of parallel regions or the end of worksharing loops. For worksharing loops, choosing a suitable schedule kind may help. For more irregular computation patterns, using OpenMP tasks might offer a solution: the runtime will try to load balance tasks across the threads in the team. Synchronisation mechanisms that enforce mutual exclusion, such as critical regions, atomic statements and locks can also result in idle threads if there is contention - threads have to wait their turn for access. Synchronisation The act of synchronising threads comes at some cost, even if the threads are never idle. In OpenMP, the most common source of synchronisation overheads is the implicit barriers at the end of parallel regions and worksharing loops. The overhead of these barriers depends on the OpenMP implementation being used as well as on the number of threads, but is typically in the range of a few microseconds. This means that for a simple parallel loop such as #pragma omp parallel for reduction(+:sum) for (i=0;i<n;i++){ sum += a[i]; } the number of iterations required to make parallel execution worthwhile may be of the order of 100,000. On ARCHER2, benchmarking has shown that for the AOCC compiler, OpenMP barriers have significantly higher overhead than for either the Cray or GNU compilers. It is possible to suppress the implicit barrier at the end of worksharing loop using a nowait clause, taking care that this does not introduce and race conditions. Atomic statements are designed to be capable of more efficient implementation that the equivalent critical region or lock/unlock pair, so should be used where applicable. Scheduling Whenever we rely on the OpenMP runtime to dynamically assign computation to threads (e.g. dynamic or guided loop schedules, tasks), there is some overhead incurred (some of this cost may actually be internal synchronisation in the runtime). It is often necessary to adjust the granularity of the computation to find a compromise between too many small units (and high scheduling cost) and too few large units (where load imbalance may dominate). For example, we can choose a non-default chunksize for the dynamic schedule, or adjust the amount of computation within each OpenMP task construct. Communication Communication between threads in OpenMP takes place via the cache coherency mechanism. In brief, whenever a thread writes a memory location, all copies of this location which are in a cache belonging to a different core have to be marked as invalid. Subsequent accesses to this location by other threads will result in the up-to-date value being retrieved from the cache where the last write occurred (or possibly from main memory). Due to the fine granularity of memory accesses, these overheads are difficult to analyse or monitor. To minimise communication, we need to write code with good data affinity - i.e. each thread should access the same subset of program data as much as possible. NUMA effects On modern CPU nodes, main memory is often organised in NUMA regions - sections of main memory associated with a subset of the cores on a node. On ARCHER2 nodes, there are 8 NUMA regions per node, each associated with 16 CPU cores. On such systems the location of data in main memory with respect to the cores that are accessing it can be important. The default OS policy is to place data in the NUMA region which first accesses it (first touch policy). For OpenMP programs this can be the worst possible option: if the data is initialised by the master thread, it is all allocated one NUMA region and having all threads accessing data becomes a bandwidth bottleneck. This default policy can be changed using the numactl command, but it is probably better to make use of the first touch policy by explicitly parallelising the data initialisation in the application code. This may be straightforward for large multidimensional arrays, but more challenging for irregular data structures. False sharing The cache coherency mechanism described above operates on units of data corresponding to the size of cache lines - for ARCHER2 CPUs this is 64 bytes. This means that if different threads are accessing neighbouring words in memory, and at least some of the accesses are writes, then communication may be happening even if no individual word is actually being accessed by more than one thread. This means that patterns such as #pragma omp parallel shared(count) private(myid) { myid = omp_get_thread_num(); .... count[myid]++; .... } may give poor performance if the updates to the count array are sufficiently frequent. Hardware resource contention Whenever there are multiple threads (or processes) executing inside a node, they may contend for some hardware resources. The most important of these for many HPC applications is memory bandwidth. This is effect is very evident on ARCHER2 CPUs - it is possible for just 2 threads to almost saturate the available memory bandwidth in a NUMA region which has 16 cores associated with it. For very bandwidth-intensive applications, running more that 2 threads per NUMA region may gain little additional performance. If an OpenMP code is not using all the cores on a node, by default Slurm will spread the threads out across NUMA regions to maximise the available bandwidth. Another resource that threads may contend for is space in shared caches. On ARCHER2, every set of 4 cores shares 16MB of L3 cache. Compiler non-optimisation In rare cases, adding OpenMP directives can adversely affect the compiler's optimisation process. The symptom of this is that the OpenMP code running on 1 thread is slower than the same code compiled without the OpenMP flag. It can be difficult to find a workaround - using the compiler's diagnostic flags to find out which optimisation (e.g. vectorisation, loop unrolling) is being affected and adding compiler-specific directives may help. Hybrid MPI and OpenMP There are two main motivations for using both MPI and OpenMP in the same application code: reducing memory requirements and improving performance. At low core counts, where the pure MPI version of the code is still scaling well, adding OpenMP is unlikely to improve performance. In fact, it can introduce some additional overheads which make performance worse! The benefit is likely to come in the regime where the pure MPI version starts to lose scalability - here adding OpenMP can reduce communication costs, make load balancing easier, or be an effective way of exploiting additional parallelism without excessive code re-writing. An important performance consideration for MPI + OpenMP applications is the choice of the number of OpenMP threads per MPI process. The optimum value will depend on the application, the input data, the number of nodes requested and the choice of compiler, and is hard to predict without experimentation. However, there are some considerations that apply to ARCHER2: Due to NUMA effects, it is likely that running at least one MPI process per NUMA region (i.e. at least 8 MPI processes per node) will be beneficial. The number of MPI processes per node should be a power of 2, so that all OpenMP threads run in the same NUMA region as their parent MPI process. For applications where each process has a small memory footprint (e.g. some molecular dynamics codes), running no more than 4 OpenMP threads per MPI process may be beneficial, so that all the threads in a process share a single L3 cache.","title":"Performance tuning"},{"location":"user-guide/tuning/#performance-tuning","text":"","title":"Performance tuning"},{"location":"user-guide/tuning/#mpi","text":"The vast majority of parallel scientific applications use the MPI library as the main way to implement parallelism; it is used so universally that the Cray compiler wrappers on ARCHER2 link to the Cray MPI library by default. Unlike other clusters you may have used, there is no choice of MPI library on ARCHER2: regardless of what compiler you are using, your program will use Cray MPI. This is because the Slingshot network on ARCHER2 is Cray-specific and significant effort has been put in by Cray software engineers to optimise the MPI performance on their Shasta systems. Here we list a number of suggestions for improving the performance of your MPI programs on ARCHER2. Although MPI programs are capable of scaling very well due to the bespoke communications hardware and software, the details of how a program calls MPI can have significant effects on achieved performance. Note Many of these tips are actually quite generic and should be beneficial to any MPI program; however, they all become much more important when running on very large numbers of processes on a machine the size of ARCHER2.","title":"MPI"},{"location":"user-guide/tuning/#synchronous-vs-asynchronous-communications","text":"","title":"Synchronous vs asynchronous communications"},{"location":"user-guide/tuning/#mpi_send","text":"A standard way to send data in MPI is using MPI_Send (aptly called standard send ). Somewhat confusingly, MPI is allowed to choose how to implement this in two different ways: Synchronously The sending process waits until a matching receive has been posted, i.e. it operates like MPI_Ssend . This clearly has the risk of deadlock if no receive is ever issued. Asynchronously MPI makes a copy of the message into an internal buffer and returns straight away without waiting for a matching receive; the message may actually be delivered later on. This is like the behaviour of the the buffered send routine MPI_Bsend . The rationale is that MPI, rather than the user, should decide how best to send a message. In practice, what typically happens is that MPI tries to use an asynchronous approach via the eager protocol: the message is sent directly to a preallocated buffer on the receiver and the routine returns immediately afterwards. Clearly there is a limit on how much space can be reserved for this, so: small messages will be sent asynchronously; large messages will be sent synchronously. The threshold is often termed the eager limit which is fixed for the entire run of your program. It will have some default setting which varies from system to system, but might be around 8K bytes.","title":"MPI_Send"},{"location":"user-guide/tuning/#implications","text":"An MPI program will typically run faster if MPI_Send is implemented asynchronously using the eager protocol since synchronisation between sender and receive is much reduced. However, you should never assume that MPI_Send buffers your message, so if you have concerns about deadlock you will need to use the non-blocking variant MPI_Isend to guarantee that the send routine returns control to you immediately even if there is no matching receive. It is not enough to say deadlock is an issue in principle, but it runs OK on my laptop so there is no problem in practice . The eager limit is system-dependent so the fact that a message happens to be buffered on your laptop is no guarantee it will be buffered on ARCHER2. To check that you have a correct code, replace all instances of MPI_Send / MPI_Isend with MPI_Ssend / MPI_Issend . A correct MPI program should still run correctly when all references to standard send are replaced by synchronous send (since MPI is allowed to implement standard send as synchronous send).","title":"Implications"},{"location":"user-guide/tuning/#tuning-performance","text":"With most MPI libraries you should be able to alter the default value of the eager limit at runtime, perhaps via an environment variable or a command-line argument to mpirun . The advice for tuning the performance of MPI_Send is find out what the distribution of message sizes for MPI_Send is (a profiling tool may be useful here); this applies to MPI_Isend as well: even in the non-blocking form, which can help to weaken synchronisation between sender and receiver, the amount of hand-shaking required is much reduced if the eager protocol is used; find out from the system documentation how to alter the value of the eager limit (there is no standardised way to set it); set the eager limit to a value larger than your typical message size -- you may need to add a small amount, say a few hundred bytes, to allow for any additional header information that is added to each message; measure the performance before and after to check that it has improved. Note It cannot be stressed strongly enough that although the performance may be affected by the value of the eager limit, the functionality of your program should be unaffected. If changing the eager limit affects the correctness of your program (e.g. whether or not it deadlocks) then you have an incorrect MPI program .","title":"Tuning performance"},{"location":"user-guide/tuning/#setting-the-eager-limit-on-archer2","text":"On ARCHER2, things are a little more complicated. Although the eager limit defaults to 16KiB, messages up to 256KiB are sent asynchronously because they are actually sent as a number of smaller messages. To send even larger messages asynchronously, alter the value of FI_OFI_RXM_SAR_LIMIT in your job submission script, e.g. to set to 512KiB: export FI_OFI_RXM_SAR_LIMIT=524288 You can also control the size of the smaller messages by altering the value of FI_OFI_RXM_BUFFER_SIZE in your job submission script, e.g. to set to 128KiB: export FI_OFI_RXM_BUFFER_SIZE=131072 A different protocol is used for messages between two processes on the same node. The default eager limit for these is 8K. Although the performance of on-node messages is unlikely to be a limiting factor for your program you can change this value, e.g. to set to 16KiB: export MPICH_SMP_SINGLE_COPY_SIZE=16384","title":"Setting the eager limit on ARCHER2"},{"location":"user-guide/tuning/#collective-operations","text":"Many of the collective operations that are commonly required by parallel scientific programs, i.e. operations that involve a group of processes, are already implemented in MPI. The canonical operation is perhaps adding up a double precision number across all MPI processes, which is best achieved by a reduction operation : MPI_Allreduce(&x, &xsum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD); This will be implemented using an efficient algorithm, for example based on a binary tree. Using such divide-and-conquer approaches typically results in an algorithm whose execution time on P processes scales as log_2(P) ; compare this to a naive approach where every process sends its input to rank 0 where the time will scale as P . This might not be significant on your laptop, but even on as few as 1000 processes the tree-based algorithm will already be around 100 times faster. So, the basic advice is always use a collective routine to implement your communications pattern if at all possible. In real MPI applications, collective operations are often called on a small amount of data, for example a global reduction of a single variable. In these cases, the time taken will be dominated by message latency and the first port of call when looking at performance optimisation is to call them as infrequently as possible! If you are simply printing diagnostics to the screen in an iterative loop, consider doing this less frequently, e.g every ten iterations, or even not at all (although you should easily be able to turn diagnostics on again for future debugging). If you are computing some termination criterion, it may actually be faster overall to compute it and check for convergence infrequently, e.g. every ten iterations, even although this means that your program could run for up to 9 extra iterations. If possible, group data into a single buffer and call a single reduction with count > 1; two reductions with count = 1 will take almost exactly twice as long as a single reduction with count = 2. For example, if you only need to output a sequence of summed data at the end of the run, store the partial totals in an array and do a single reduction right at the end. Sometimes, the collective routines available may not appear to do exactly what you want. However, they can sometimes be used with a small amount of additional programming work: To operate on a subset of processes, create sub-communicators containing the relevant subset(s) and use these communicators instead of MPI_COMM_WORLD . Useful functions for communicator management include: MPI_Comm_split is the most general routine; MPI_Comm_split_type can be used to create a separate communicator for each shared-memory node with split type = MPI_COMM_TYPE_SHARED ; MPI_Cart_sub can divide a Cartesian communicator into regular slices. If the communication pattern is what you want, but the data on each process is not arranged in the required layout, consider using MPI derived data types for the input and/or output. This can be useful, for example, if you want to communicate non-contiguous data such as a subsection of a multidimensional array although care must be taken in defining these types to ensure they have the correct extents. Another example would be using MPI_Allreduce to add up an integer and a double-precision variable using a single call by putting them together into a C struct and defining a matching MPI datatype using MPI_Type_create_struct . Here you would also have to provide MPI with a custom reduction operation using MPI_Op_create . Many MPI programs call MPI_Barrier to explicitly synchronise all the processes. Although this can be useful for getting reliable performance timings, it is rare in practice to find a program where the call is actually needed for correctness. For example, you may see: // Ensure the input x is available on all processes MPI_Barrier(MPI_COMM_WORLD); // Perform a global reduction operation MPI_Allreduce(&x, &xsum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD); // Ensure the result xsum is available on all processes MPI_Barrier(MPI_COMM_WORLD); Neither of these barriers are needed as the reduction operation performs all the required synchronisation. If removing a barrier from your MPI code makes it run incorrectly, then this should ring alarm bells -- it is often a symptom of an underlying bug that is simply being masked by the barrier. For example, if you use non-blocking calls such as MPI_Irecv then it is the programmer's responsibility to ensure that these are completed at some later point, for example by calling MPI_Wait on the returned request object. A common bug is to forget to do this, in which case you might be reading the contents of the receive buffer before the incoming message has arrived (e.g. if the sender is running late). Calling a barrier may mask this bug as it will make all the processes wait for each other, perhaps allowing the late sender to catch up. However, this is not guaranteed so the real solution is to call the non-blocking communications correctly. One of the few times when a barrier may be required is if processes are communicating with each other via some other non-MPI method, e.g. via the file system. If you want processes to sequentially open, append to, then close the same file then barriers are a simple way to achieve this: for (i=0; i < size; i++) { if (rank == i) append_data_to_file(data, filename); MPI_Barrier(comm); } but this is really something of a special case. Global synchronisation may be required if you are using more advanced techniques such as hybrid MPI/OpenMP or single-sided MPI communication with put and get, but typically you should be using specialised routines such as MPI_Win_fence rather than MPI_Barrier . Tip If you run a performance profiler on your code and it shows a lot of time being spent in a collective operation such as MPI_Allreduce , this is not necessarily a sign that the reduction operation itself is the bottleneck. This is often a symptom of load imbalance : even if a reduction operation is efficiently implemented, it may take a long time to complete if the MPI processes do not all call it at the same time. MPI_Allreduce synchronises across processes so will have to wait for all the processes to call it before it can complete. A single slow process will therefore adversely impact the performance of your entire parallel program.","title":"Collective operations"},{"location":"user-guide/tuning/#openmp","text":"There are a variety of possible issues that can result in poor performance of OpenMP programs. These include:","title":"OpenMP"},{"location":"user-guide/tuning/#sequential-code","text":"Code outside of parallel regions is executed sequentially by the master thread.","title":"Sequential code"},{"location":"user-guide/tuning/#idle-threads","text":"If different threads have different amounts of computation to do, then threads may be idle whenever a barrier is encountered, for example at the end of parallel regions or the end of worksharing loops. For worksharing loops, choosing a suitable schedule kind may help. For more irregular computation patterns, using OpenMP tasks might offer a solution: the runtime will try to load balance tasks across the threads in the team. Synchronisation mechanisms that enforce mutual exclusion, such as critical regions, atomic statements and locks can also result in idle threads if there is contention - threads have to wait their turn for access.","title":"Idle threads"},{"location":"user-guide/tuning/#synchronisation","text":"The act of synchronising threads comes at some cost, even if the threads are never idle. In OpenMP, the most common source of synchronisation overheads is the implicit barriers at the end of parallel regions and worksharing loops. The overhead of these barriers depends on the OpenMP implementation being used as well as on the number of threads, but is typically in the range of a few microseconds. This means that for a simple parallel loop such as #pragma omp parallel for reduction(+:sum) for (i=0;i<n;i++){ sum += a[i]; } the number of iterations required to make parallel execution worthwhile may be of the order of 100,000. On ARCHER2, benchmarking has shown that for the AOCC compiler, OpenMP barriers have significantly higher overhead than for either the Cray or GNU compilers. It is possible to suppress the implicit barrier at the end of worksharing loop using a nowait clause, taking care that this does not introduce and race conditions. Atomic statements are designed to be capable of more efficient implementation that the equivalent critical region or lock/unlock pair, so should be used where applicable.","title":"Synchronisation"},{"location":"user-guide/tuning/#scheduling","text":"Whenever we rely on the OpenMP runtime to dynamically assign computation to threads (e.g. dynamic or guided loop schedules, tasks), there is some overhead incurred (some of this cost may actually be internal synchronisation in the runtime). It is often necessary to adjust the granularity of the computation to find a compromise between too many small units (and high scheduling cost) and too few large units (where load imbalance may dominate). For example, we can choose a non-default chunksize for the dynamic schedule, or adjust the amount of computation within each OpenMP task construct.","title":"Scheduling"},{"location":"user-guide/tuning/#communication","text":"Communication between threads in OpenMP takes place via the cache coherency mechanism. In brief, whenever a thread writes a memory location, all copies of this location which are in a cache belonging to a different core have to be marked as invalid. Subsequent accesses to this location by other threads will result in the up-to-date value being retrieved from the cache where the last write occurred (or possibly from main memory). Due to the fine granularity of memory accesses, these overheads are difficult to analyse or monitor. To minimise communication, we need to write code with good data affinity - i.e. each thread should access the same subset of program data as much as possible.","title":"Communication"},{"location":"user-guide/tuning/#numa-effects","text":"On modern CPU nodes, main memory is often organised in NUMA regions - sections of main memory associated with a subset of the cores on a node. On ARCHER2 nodes, there are 8 NUMA regions per node, each associated with 16 CPU cores. On such systems the location of data in main memory with respect to the cores that are accessing it can be important. The default OS policy is to place data in the NUMA region which first accesses it (first touch policy). For OpenMP programs this can be the worst possible option: if the data is initialised by the master thread, it is all allocated one NUMA region and having all threads accessing data becomes a bandwidth bottleneck. This default policy can be changed using the numactl command, but it is probably better to make use of the first touch policy by explicitly parallelising the data initialisation in the application code. This may be straightforward for large multidimensional arrays, but more challenging for irregular data structures.","title":"NUMA effects"},{"location":"user-guide/tuning/#false-sharing","text":"The cache coherency mechanism described above operates on units of data corresponding to the size of cache lines - for ARCHER2 CPUs this is 64 bytes. This means that if different threads are accessing neighbouring words in memory, and at least some of the accesses are writes, then communication may be happening even if no individual word is actually being accessed by more than one thread. This means that patterns such as #pragma omp parallel shared(count) private(myid) { myid = omp_get_thread_num(); .... count[myid]++; .... } may give poor performance if the updates to the count array are sufficiently frequent.","title":"False sharing"},{"location":"user-guide/tuning/#hardware-resource-contention","text":"Whenever there are multiple threads (or processes) executing inside a node, they may contend for some hardware resources. The most important of these for many HPC applications is memory bandwidth. This is effect is very evident on ARCHER2 CPUs - it is possible for just 2 threads to almost saturate the available memory bandwidth in a NUMA region which has 16 cores associated with it. For very bandwidth-intensive applications, running more that 2 threads per NUMA region may gain little additional performance. If an OpenMP code is not using all the cores on a node, by default Slurm will spread the threads out across NUMA regions to maximise the available bandwidth. Another resource that threads may contend for is space in shared caches. On ARCHER2, every set of 4 cores shares 16MB of L3 cache.","title":"Hardware resource contention"},{"location":"user-guide/tuning/#compiler-non-optimisation","text":"In rare cases, adding OpenMP directives can adversely affect the compiler's optimisation process. The symptom of this is that the OpenMP code running on 1 thread is slower than the same code compiled without the OpenMP flag. It can be difficult to find a workaround - using the compiler's diagnostic flags to find out which optimisation (e.g. vectorisation, loop unrolling) is being affected and adding compiler-specific directives may help.","title":"Compiler non-optimisation"},{"location":"user-guide/tuning/#hybrid-mpi-and-openmp","text":"There are two main motivations for using both MPI and OpenMP in the same application code: reducing memory requirements and improving performance. At low core counts, where the pure MPI version of the code is still scaling well, adding OpenMP is unlikely to improve performance. In fact, it can introduce some additional overheads which make performance worse! The benefit is likely to come in the regime where the pure MPI version starts to lose scalability - here adding OpenMP can reduce communication costs, make load balancing easier, or be an effective way of exploiting additional parallelism without excessive code re-writing. An important performance consideration for MPI + OpenMP applications is the choice of the number of OpenMP threads per MPI process. The optimum value will depend on the application, the input data, the number of nodes requested and the choice of compiler, and is hard to predict without experimentation. However, there are some considerations that apply to ARCHER2: Due to NUMA effects, it is likely that running at least one MPI process per NUMA region (i.e. at least 8 MPI processes per node) will be beneficial. The number of MPI processes per node should be a power of 2, so that all OpenMP threads run in the same NUMA region as their parent MPI process. For applications where each process has a small memory footprint (e.g. some molecular dynamics codes), running no more than 4 OpenMP threads per MPI process may be beneficial, so that all the threads in a process share a single L3 cache.","title":"Hybrid MPI and OpenMP"}]}