
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../sw-environment/">
      
      
        <link rel="next" href="../io/">
      
      <link rel="icon" href="../../favicon.ico">
      <meta name="generator" content="mkdocs-1.4.3, mkdocs-material-9.1.18">
    
    
      
        <title>Running jobs - ARCHER2 User Documentation</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.26e3688c.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.ecc896b0.min.css">
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../stylesheets/archer2.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      
  


  
  


  <script id="__analytics">function __md_analytics(){function n(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],n("js",new Date),n("config","G-XVNML97F1T"),document.addEventListener("DOMContentLoaded",function(){document.forms.search&&document.forms.search.query.addEventListener("blur",function(){this.value&&n("event","search",{search_term:this.value})}),document$.subscribe(function(){var a=document.forms.feedback;if(void 0!==a)for(var e of a.querySelectorAll("[type=submit]"))e.addEventListener("click",function(e){e.preventDefault();var t=document.location.pathname,e=this.getAttribute("data-md-value");n("event","feedback",{page:t,data:e}),a.firstElementChild.disabled=!0;e=a.querySelector(".md-feedback__note [data-md-value='"+e+"']");e&&(e.hidden=!1)}),a.hidden=!1}),location$.subscribe(function(e){n("config","G-XVNML97F1T",{page_path:e.pathname})})});var e=document.createElement("script");e.async=!0,e.src="https://www.googletagmanager.com/gtag/js?id=G-XVNML97F1T",document.getElementById("__analytics").insertAdjacentElement("afterEnd",e)}</script>

  
    <script>"undefined"!=typeof __md_analytics&&__md_analytics()</script>
  

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="teal">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#running-jobs-on-archer2" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="ARCHER2 User Documentation" class="md-header__button md-logo" aria-label="ARCHER2 User Documentation" data-md-component="logo">
      
  <img src="../../images/archer2_white_transparent.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            ARCHER2 User Documentation
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Running jobs
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/ARCHER2-HPC/archer2-docs" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    ARCHER2-HPC/archer2-docs
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="ARCHER2 User Documentation" class="md-nav__button md-logo" aria-label="ARCHER2 User Documentation" data-md-component="logo">
      
  <img src="../../images/archer2_white_transparent.png" alt="logo">

    </a>
    ARCHER2 User Documentation
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/ARCHER2-HPC/archer2-docs" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    ARCHER2-HPC/archer2-docs
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        Documentation overview
      </a>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
      
      
      
        <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
          Quickstart
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          Quickstart
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../quick-start/overview/" class="md-nav__link">
        Overview
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../quick-start/quickstart-users/" class="md-nav__link">
        Quickstart for users
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../quick-start/quickstart-developers/" class="md-nav__link">
        Quickstart for developers
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../quick-start/quickstart-next-steps/" class="md-nav__link">
        Quickstart next steps
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../known-issues/" class="md-nav__link">
        ARCHER2 Known Issues
      </a>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
      
      
      
        <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
          ARCHER2 Frequently Asked Questions
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          ARCHER2 Frequently Asked Questions
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../faq/" class="md-nav__link">
        General FAQ
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../faq/upgrade-2023/" class="md-nav__link">
        ARCHER2 Upgrade 2023
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" checked>
      
      
      
        <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
          User and Best Practice Guide
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="true">
        <label class="md-nav__title" for="__nav_5">
          <span class="md-nav__icon md-icon"></span>
          User and Best Practice Guide
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../" class="md-nav__link">
        Overview
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../hardware/" class="md-nav__link">
        Hardware
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../connecting/" class="md-nav__link">
        Connecting to ARCHER2
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../data/" class="md-nav__link">
        Data management and transfer
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../sw-environment/" class="md-nav__link">
        Software environment
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Running jobs
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Running jobs
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#resources" class="md-nav__link">
    Resources
  </a>
  
    <nav class="md-nav" aria-label="Resources">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cus" class="md-nav__link">
    CUs
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#checking-available-budget" class="md-nav__link">
    Checking available budget
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#charging" class="md-nav__link">
    Charging
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#basic-slurm-commands" class="md-nav__link">
    Basic Slurm commands
  </a>
  
    <nav class="md-nav" aria-label="Basic Slurm commands">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sinfo-information-on-resources" class="md-nav__link">
    sinfo: information on resources
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbatch-submitting-jobs" class="md-nav__link">
    sbatch: submitting jobs
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#squeue-monitoring-jobs" class="md-nav__link">
    squeue: monitoring jobs
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#scancel-deleting-jobs" class="md-nav__link">
    scancel: deleting jobs
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#resource-limits" class="md-nav__link">
    Resource Limits
  </a>
  
    <nav class="md-nav" aria-label="Resource Limits">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#primary-resource" class="md-nav__link">
    Primary resource
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#partitions" class="md-nav__link">
    Partitions
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#quality-of-service-qos" class="md-nav__link">
    Quality of Service (QoS)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#e-mail-notifications" class="md-nav__link">
    E-mail notifications
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#priority" class="md-nav__link">
    Priority
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#troubleshooting" class="md-nav__link">
    Troubleshooting
  </a>
  
    <nav class="md-nav" aria-label="Troubleshooting">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slurm-error-messages" class="md-nav__link">
    Slurm error messages
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slurm-job-state-codes" class="md-nav__link">
    Slurm job state codes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slurm-queued-reasons" class="md-nav__link">
    Slurm queued reasons
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#output-from-slurm-jobs" class="md-nav__link">
    Output from Slurm jobs
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#specifying-resources-in-job-scripts" class="md-nav__link">
    Specifying resources in job scripts
  </a>
  
    <nav class="md-nav" aria-label="Specifying resources in job scripts">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#additional-options-for-parallel-jobs" class="md-nav__link">
    Additional options for parallel jobs
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#options-for-jobs-on-the-data-analysis-nodes" class="md-nav__link">
    Options for jobs on the data analysis nodes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#srun-launching-parallel-jobs" class="md-nav__link">
    srun: Launching parallel jobs
  </a>
  
    <nav class="md-nav" aria-label="srun: Launching parallel jobs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slurm-definition-of-a-socket" class="md-nav__link">
    Slurm definition of a "socket"
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bolt-job-submission-script-creation-tool" class="md-nav__link">
    bolt: Job submission script creation tool
  </a>
  
    <nav class="md-nav" aria-label="bolt: Job submission script creation tool">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#basic-usage" class="md-nav__link">
    Basic Usage
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#further-help" class="md-nav__link">
    Further help
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#checkscript-job-submission-script-validation-tool" class="md-nav__link">
    checkScript job submission script validation tool
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#checking-scripts-and-estimating-start-time-with-test-only" class="md-nav__link">
    Checking scripts and estimating start time with --test-only
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#example-job-submission-scripts" class="md-nav__link">
    Example job submission scripts
  </a>
  
    <nav class="md-nav" aria-label="Example job submission scripts">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#example-job-submission-script-for-mpi-parallel-job" class="md-nav__link">
    Example: job submission script for MPI parallel job
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#example-job-submission-script-for-mpiopenmp-mixed-mode-parallel-job" class="md-nav__link">
    Example: job submission script for MPI+OpenMP (mixed mode) parallel job
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#job-arrays" class="md-nav__link">
    Job arrays
  </a>
  
    <nav class="md-nav" aria-label="Job arrays">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#job-script-for-a-job-array" class="md-nav__link">
    Job script for a job array
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#submitting-a-job-array" class="md-nav__link">
    Submitting a job array
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#job-chaining" class="md-nav__link">
    Job chaining
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#using-multiple-srun-commands-in-a-single-job-script" class="md-nav__link">
    Using multiple srun commands in a single job script
  </a>
  
    <nav class="md-nav" aria-label="Using multiple srun commands in a single job script">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#running-multiple-full-node-subjobs-within-a-larger-job" class="md-nav__link">
    Running multiple, full-node subjobs within a larger job
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#running-multiple-subjobs-that-each-use-a-fraction-of-a-node" class="md-nav__link">
    Running multiple subjobs that each use a fraction of a node
  </a>
  
    <nav class="md-nav" aria-label="Running multiple subjobs that each use a fraction of a node">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#example-1-128-serial-tasks-running-on-a-single-node" class="md-nav__link">
    Example 1: 128 serial tasks running on a single node
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#example-2-8-subjobs-on-1-node-each-with-8-mpi-processes-and-2-openmp-threads-per-process" class="md-nav__link">
    Example 2: 8 subjobs on 1 node each with 8 MPI processes and 2 OpenMP threads per process
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#example-3-running-inhomogeneous-subjobs-on-one-node" class="md-nav__link">
    Example 3: Running inhomogeneous subjobs on one node
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#example-4-256-serial-tasks-running-across-two-nodes" class="md-nav__link">
    Example 4: 256 serial tasks running across two nodes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#process-placement" class="md-nav__link">
    Process placement
  </a>
  
    <nav class="md-nav" aria-label="Process placement">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#standard-process-placement" class="md-nav__link">
    Standard process placement
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#setting-process-placement-using-slurm-options" class="md-nav__link">
    Setting process placement using Slurm options
  </a>
  
    <nav class="md-nav" aria-label="Setting process placement using Slurm options">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#for-underpopulation-of-nodes-with-processes" class="md-nav__link">
    For underpopulation of nodes with processes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#full-node-population-with-non-sequential-process-placement" class="md-nav__link">
    Full node population with non-sequential process placement
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mpich_rank_reorder_method-for-mpi-process-placement" class="md-nav__link">
    MPICH_RANK_REORDER_METHOD for MPI process placement
  </a>
  
    <nav class="md-nav" aria-label="MPICH_RANK_REORDER_METHOD for MPI process placement">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#grid_order" class="md-nav__link">
    grid_order
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#interactive-jobs" class="md-nav__link">
    Interactive Jobs
  </a>
  
    <nav class="md-nav" aria-label="Interactive Jobs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#using-salloc-to-reserve-resources" class="md-nav__link">
    Using salloc to reserve resources
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#using-srun-directly" class="md-nav__link">
    Using srun directly
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#heterogeneous-jobs" class="md-nav__link">
    Heterogeneous jobs
  </a>
  
    <nav class="md-nav" aria-label="Heterogeneous jobs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#heterogeneous-jobs-for-a-clientserver-model-distinct-mpi_comm_worlds" class="md-nav__link">
    Heterogeneous jobs for a client/server model: distinct MPI_COMM_WORLDs
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#heterogeneous-jobs-for-a-shared-mpi_com_world" class="md-nav__link">
    Heterogeneous jobs for a shared MPI_COM_WORLD
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#heterogeneous-placement-for-mixed-mpiopenmp-work" class="md-nav__link">
    Heterogeneous placement for mixed MPI/OpenMP work
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#low-priority-access" class="md-nav__link">
    Low priority access
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reservations" class="md-nav__link">
    Reservations
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#serial-jobs" class="md-nav__link">
    Serial jobs
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#best-practices-for-job-submission" class="md-nav__link">
    Best practices for job submission
  </a>
  
    <nav class="md-nav" aria-label="Best practices for job submission">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#time-limits" class="md-nav__link">
    Time Limits
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#long-running-jobs" class="md-nav__link">
    Long Running Jobs
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#interconnect-locality" class="md-nav__link">
    Interconnect locality
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#large-jobs" class="md-nav__link">
    Large Jobs
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#huge-pages" class="md-nav__link">
    Huge pages
  </a>
  
    <nav class="md-nav" aria-label="Huge pages">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#when-to-use-huge-pages" class="md-nav__link">
    When to Use Huge Pages
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#when-to-avoid-huge-pages" class="md-nav__link">
    When to Avoid Huge Pages
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../io/" class="md-nav__link">
        I/O and file systems
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../dev-environment/" class="md-nav__link">
        Application development environment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../containers/" class="md-nav__link">
        Containers
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../python/" class="md-nav__link">
        Using Python
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../analysis/" class="md-nav__link">
        Data analysis
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../debug/" class="md-nav__link">
        Debugging
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../profile/" class="md-nav__link">
        Profiling
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../tuning/" class="md-nav__link">
        Performance tuning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../energy/" class="md-nav__link">
        Energy use
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
      
      
      
        <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
          Research Software
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_6">
          <span class="md-nav__icon md-icon"></span>
          Research Software
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../research-software/" class="md-nav__link">
        Overview
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../research-software/casino/" class="md-nav__link">
        CASINO
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../research-software/castep/" class="md-nav__link">
        CASTEP
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../research-software/cesm/" class="md-nav__link">
        CESM2
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../research-software/chemshell/" class="md-nav__link">
        Chemshell
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../research-software/code-saturne/" class="md-nav__link">
        Code_Saturne
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../research-software/cp2k/" class="md-nav__link">
        CP2K
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../research-software/crystal/" class="md-nav__link">
        CRYSTAL
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../research-software/fhi-aims/" class="md-nav__link">
        FHI-aims
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../research-software/gromacs/" class="md-nav__link">
        GROMACS
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../research-software/lammps/" class="md-nav__link">
        LAMMPS
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../research-software/mitgcm/" class="md-nav__link">
        MITgcm
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../research-software/mo-unified-model/" class="md-nav__link">
        Met Office Unified Model
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../research-software/namd/" class="md-nav__link">
        NAMD
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../research-software/nektarplusplus/" class="md-nav__link">
        Nektar++
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../research-software/nemo/" class="md-nav__link">
        NEMO
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../research-software/nwchem/" class="md-nav__link">
        NWChem
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../research-software/onetep/" class="md-nav__link">
        ONETEP
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../research-software/openfoam/" class="md-nav__link">
        OpenFOAM
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../research-software/orca/" class="md-nav__link">
        ORCA
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../research-software/qe/" class="md-nav__link">
        Quantum Espresso
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../research-software/vasp/" class="md-nav__link">
        VASP
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
      
      
      
        <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
          Software Libraries
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_7">
          <span class="md-nav__icon md-icon"></span>
          Software Libraries
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../software-libraries/" class="md-nav__link">
        Overview
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../software-libraries/libsci/" class="md-nav__link">
        HPE Cray LibSci: BLAS, LAPACK, ScaLAPACK
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../software-libraries/fftw/" class="md-nav__link">
        FFTW
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../software-libraries/hdf5/" class="md-nav__link">
        HDF5
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../software-libraries/netcdf/" class="md-nav__link">
        NetCDF
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../software-libraries/adios/" class="md-nav__link">
        ADIOS
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../software-libraries/aocl/" class="md-nav__link">
        AOCL
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../software-libraries/arpack/" class="md-nav__link">
        ARPACK
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../software-libraries/boost/" class="md-nav__link">
        Boost
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../software-libraries/eigen/" class="md-nav__link">
        Eigen
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../software-libraries/glm/" class="md-nav__link">
        GLM
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../software-libraries/hypre/" class="md-nav__link">
        HYPRE
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../software-libraries/matio/" class="md-nav__link">
        Matio
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../software-libraries/mkl/" class="md-nav__link">
        Intel MKL
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../software-libraries/mesa/" class="md-nav__link">
        MESA
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../software-libraries/mumps/" class="md-nav__link">
        MUMPS
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../software-libraries/petsc/" class="md-nav__link">
        PETSc
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../software-libraries/metis/" class="md-nav__link">
        Metis/Parmetis
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../software-libraries/scotch/" class="md-nav__link">
        Scotch/PT-Scotch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../software-libraries/slepc/" class="md-nav__link">
        SLEPc
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../software-libraries/superlu/" class="md-nav__link">
        SuperLU/SuperLU_DIST
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../software-libraries/trilinos/" class="md-nav__link">
        Trilinos
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_8" >
      
      
      
        <label class="md-nav__link" for="__nav_8" id="__nav_8_label" tabindex="0">
          Data Analysis and Tools
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_8_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_8">
          <span class="md-nav__icon md-icon"></span>
          Data Analysis and Tools
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../data-tools/" class="md-nav__link">
        Overview
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../data-tools/amd-uprof/" class="md-nav__link">
        AMD uProf
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../data-tools/arm-forge/" class="md-nav__link">
        Arm Forge
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../data-tools/julia/" class="md-nav__link">
        Julia
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../data-tools/paraview/" class="md-nav__link">
        ParaView
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../data-tools/cray-r/" class="md-nav__link">
        R
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../data-tools/visidata/" class="md-nav__link">
        VisiData
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../essentials/" class="md-nav__link">
        Essential Skills
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../publish/" class="md-nav__link">
        ARCHER2 and Publications
      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#resources" class="md-nav__link">
    Resources
  </a>
  
    <nav class="md-nav" aria-label="Resources">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cus" class="md-nav__link">
    CUs
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#checking-available-budget" class="md-nav__link">
    Checking available budget
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#charging" class="md-nav__link">
    Charging
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#basic-slurm-commands" class="md-nav__link">
    Basic Slurm commands
  </a>
  
    <nav class="md-nav" aria-label="Basic Slurm commands">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sinfo-information-on-resources" class="md-nav__link">
    sinfo: information on resources
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbatch-submitting-jobs" class="md-nav__link">
    sbatch: submitting jobs
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#squeue-monitoring-jobs" class="md-nav__link">
    squeue: monitoring jobs
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#scancel-deleting-jobs" class="md-nav__link">
    scancel: deleting jobs
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#resource-limits" class="md-nav__link">
    Resource Limits
  </a>
  
    <nav class="md-nav" aria-label="Resource Limits">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#primary-resource" class="md-nav__link">
    Primary resource
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#partitions" class="md-nav__link">
    Partitions
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#quality-of-service-qos" class="md-nav__link">
    Quality of Service (QoS)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#e-mail-notifications" class="md-nav__link">
    E-mail notifications
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#priority" class="md-nav__link">
    Priority
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#troubleshooting" class="md-nav__link">
    Troubleshooting
  </a>
  
    <nav class="md-nav" aria-label="Troubleshooting">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slurm-error-messages" class="md-nav__link">
    Slurm error messages
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slurm-job-state-codes" class="md-nav__link">
    Slurm job state codes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slurm-queued-reasons" class="md-nav__link">
    Slurm queued reasons
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#output-from-slurm-jobs" class="md-nav__link">
    Output from Slurm jobs
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#specifying-resources-in-job-scripts" class="md-nav__link">
    Specifying resources in job scripts
  </a>
  
    <nav class="md-nav" aria-label="Specifying resources in job scripts">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#additional-options-for-parallel-jobs" class="md-nav__link">
    Additional options for parallel jobs
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#options-for-jobs-on-the-data-analysis-nodes" class="md-nav__link">
    Options for jobs on the data analysis nodes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#srun-launching-parallel-jobs" class="md-nav__link">
    srun: Launching parallel jobs
  </a>
  
    <nav class="md-nav" aria-label="srun: Launching parallel jobs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slurm-definition-of-a-socket" class="md-nav__link">
    Slurm definition of a "socket"
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bolt-job-submission-script-creation-tool" class="md-nav__link">
    bolt: Job submission script creation tool
  </a>
  
    <nav class="md-nav" aria-label="bolt: Job submission script creation tool">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#basic-usage" class="md-nav__link">
    Basic Usage
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#further-help" class="md-nav__link">
    Further help
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#checkscript-job-submission-script-validation-tool" class="md-nav__link">
    checkScript job submission script validation tool
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#checking-scripts-and-estimating-start-time-with-test-only" class="md-nav__link">
    Checking scripts and estimating start time with --test-only
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#example-job-submission-scripts" class="md-nav__link">
    Example job submission scripts
  </a>
  
    <nav class="md-nav" aria-label="Example job submission scripts">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#example-job-submission-script-for-mpi-parallel-job" class="md-nav__link">
    Example: job submission script for MPI parallel job
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#example-job-submission-script-for-mpiopenmp-mixed-mode-parallel-job" class="md-nav__link">
    Example: job submission script for MPI+OpenMP (mixed mode) parallel job
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#job-arrays" class="md-nav__link">
    Job arrays
  </a>
  
    <nav class="md-nav" aria-label="Job arrays">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#job-script-for-a-job-array" class="md-nav__link">
    Job script for a job array
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#submitting-a-job-array" class="md-nav__link">
    Submitting a job array
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#job-chaining" class="md-nav__link">
    Job chaining
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#using-multiple-srun-commands-in-a-single-job-script" class="md-nav__link">
    Using multiple srun commands in a single job script
  </a>
  
    <nav class="md-nav" aria-label="Using multiple srun commands in a single job script">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#running-multiple-full-node-subjobs-within-a-larger-job" class="md-nav__link">
    Running multiple, full-node subjobs within a larger job
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#running-multiple-subjobs-that-each-use-a-fraction-of-a-node" class="md-nav__link">
    Running multiple subjobs that each use a fraction of a node
  </a>
  
    <nav class="md-nav" aria-label="Running multiple subjobs that each use a fraction of a node">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#example-1-128-serial-tasks-running-on-a-single-node" class="md-nav__link">
    Example 1: 128 serial tasks running on a single node
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#example-2-8-subjobs-on-1-node-each-with-8-mpi-processes-and-2-openmp-threads-per-process" class="md-nav__link">
    Example 2: 8 subjobs on 1 node each with 8 MPI processes and 2 OpenMP threads per process
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#example-3-running-inhomogeneous-subjobs-on-one-node" class="md-nav__link">
    Example 3: Running inhomogeneous subjobs on one node
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#example-4-256-serial-tasks-running-across-two-nodes" class="md-nav__link">
    Example 4: 256 serial tasks running across two nodes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#process-placement" class="md-nav__link">
    Process placement
  </a>
  
    <nav class="md-nav" aria-label="Process placement">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#standard-process-placement" class="md-nav__link">
    Standard process placement
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#setting-process-placement-using-slurm-options" class="md-nav__link">
    Setting process placement using Slurm options
  </a>
  
    <nav class="md-nav" aria-label="Setting process placement using Slurm options">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#for-underpopulation-of-nodes-with-processes" class="md-nav__link">
    For underpopulation of nodes with processes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#full-node-population-with-non-sequential-process-placement" class="md-nav__link">
    Full node population with non-sequential process placement
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mpich_rank_reorder_method-for-mpi-process-placement" class="md-nav__link">
    MPICH_RANK_REORDER_METHOD for MPI process placement
  </a>
  
    <nav class="md-nav" aria-label="MPICH_RANK_REORDER_METHOD for MPI process placement">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#grid_order" class="md-nav__link">
    grid_order
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#interactive-jobs" class="md-nav__link">
    Interactive Jobs
  </a>
  
    <nav class="md-nav" aria-label="Interactive Jobs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#using-salloc-to-reserve-resources" class="md-nav__link">
    Using salloc to reserve resources
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#using-srun-directly" class="md-nav__link">
    Using srun directly
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#heterogeneous-jobs" class="md-nav__link">
    Heterogeneous jobs
  </a>
  
    <nav class="md-nav" aria-label="Heterogeneous jobs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#heterogeneous-jobs-for-a-clientserver-model-distinct-mpi_comm_worlds" class="md-nav__link">
    Heterogeneous jobs for a client/server model: distinct MPI_COMM_WORLDs
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#heterogeneous-jobs-for-a-shared-mpi_com_world" class="md-nav__link">
    Heterogeneous jobs for a shared MPI_COM_WORLD
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#heterogeneous-placement-for-mixed-mpiopenmp-work" class="md-nav__link">
    Heterogeneous placement for mixed MPI/OpenMP work
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#low-priority-access" class="md-nav__link">
    Low priority access
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reservations" class="md-nav__link">
    Reservations
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#serial-jobs" class="md-nav__link">
    Serial jobs
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#best-practices-for-job-submission" class="md-nav__link">
    Best practices for job submission
  </a>
  
    <nav class="md-nav" aria-label="Best practices for job submission">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#time-limits" class="md-nav__link">
    Time Limits
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#long-running-jobs" class="md-nav__link">
    Long Running Jobs
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#interconnect-locality" class="md-nav__link">
    Interconnect locality
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#large-jobs" class="md-nav__link">
    Large Jobs
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#huge-pages" class="md-nav__link">
    Huge pages
  </a>
  
    <nav class="md-nav" aria-label="Huge pages">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#when-to-use-huge-pages" class="md-nav__link">
    When to Use Huge Pages
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#when-to-avoid-huge-pages" class="md-nav__link">
    When to Avoid Huge Pages
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
  


<h1 id="running-jobs-on-archer2">Running jobs on ARCHER2</h1>
<p>As with most HPC services, ARCHER2 uses a scheduler to manage access to
resources and ensure that the thousands of different users of system are
able to share the system and all get access to the resources they
require. ARCHER2 uses the Slurm software to schedule jobs.</p>
<p>Writing a submission script is typically the most convenient way to
submit your job to the scheduler. Example submission scripts (with
explanations) for the most common job types are provided below.</p>
<p>Interactive jobs are also available and can be particularly useful for
developing and debugging applications. More details are available below.</p>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p>If you have any questions on how to run jobs on ARCHER2 do not hesitate
to contact the <a href="mailto:support@archer2.ac.uk">ARCHER2 Service Desk</a>.</p>
</div>
<p>You typically interact with Slurm by issuing Slurm commands from the
login nodes (to submit, check and cancel jobs), and by specifying Slurm
directives that describe the resources required for your jobs in job
submission scripts.</p>
<h2 id="resources">Resources</h2>
<h3 id="cus">CUs</h3>
<p>Time used on ARCHER2 is measured in CUs.<br />
1 CU = 1 Node Hour for a standard 128 core node.</p>
<p>The <a href="https://www.archer2.ac.uk/support-access/cu-calc.html">CU calculator</a> will help you to calculate the CU cost for your jobs.</p>
<h3 id="checking-available-budget">Checking available budget</h3>
<p>You can check in <a href="https://safe.epcc.ed.ac.uk">SAFE</a> by selecting <code>Login accounts</code> from the menu, select the login account you want to query.</p>
<p>Under <code>Login account details</code> you will see each of the budget codes you have access to listed e.g.
<code>e123 resources</code> and then under Resource Pool to the right of this, a note of the remaining budget in CUs. </p>
<p>When logged in to the machine you can also use the command </p>
<pre><code>sacctmgr show assoc where user=$LOGNAME format=account,user,maxtresmins
</code></pre>
<p>This will list all the budget codes that you have access to e.g.</p>
<pre><code>   Account       User   MaxTRESMins
---------- ---------- -------------
      e123      userx         cpu=0
 e123-test      userx
</code></pre>
<p>This shows that <code>userx</code> is a member of budgets <code>e123</code> and <code>e123-test</code>.  However, the <code>cpu=0</code> indicates that the <code>e123</code> budget is empty or disabled.   This user can submit jobs using the <code>e123-test</code> budget.</p>
<p>To see the number of CUs remaining you must check in <a href="https://safe.epcc.ed.ac.uk">SAFE</a>.</p>
<h3 id="charging">Charging</h3>
<p>Jobs run on ARCHER2 are charged for the time they use i.e. from the time the job begins to run until the time the job ends (not the full wall time requested).</p>
<p>Jobs are charged for the full number of nodes which are requested, even if they are not all used.</p>
<p>Charging takes place at the time the job ends, and the job is charged in full to the budget which is live at the end time.</p>
<h2 id="basic-slurm-commands">Basic Slurm commands</h2>
<p>There are four key commands used to interact with the Slurm on the
command line:</p>
<ul>
<li><code>sinfo</code> - Get information on the partitions and resources available</li>
<li><code>sbatch jobscript.slurm</code> - Submit a job submission script (in this
    case called: <code>jobscript.slurm</code>) to the scheduler</li>
<li><code>squeue</code> - Get the current status of jobs submitted to the scheduler</li>
<li><code>scancel 12345</code> - Cancel a job (in this case with the job ID
    <code>12345</code>)</li>
</ul>
<p>We cover each of these commands in more detail below.</p>
<h3 id="sinfo-information-on-resources"><code>sinfo</code>: information on resources</h3>
<p><code>sinfo</code> is used to query information about available resources and
partitions. Without any options, <code>sinfo</code> lists the status of all
resources and partitions, e.g.</p>
<div class="highlight"><pre><span></span><code>auser@ln01:~&gt;<span class="w"> </span>sinfo<span class="w"> </span>

PARTITION<span class="w"> </span>AVAIL<span class="w">  </span>TIMELIMIT<span class="w">  </span>NODES<span class="w">  </span>STATE<span class="w"> </span>NODELIST<span class="w"> </span>
standard<span class="w">     </span>up<span class="w"> </span><span class="m">1</span>-00:00:00<span class="w">    </span><span class="m">105</span><span class="w">  </span>down*<span class="w"> </span>nid<span class="o">[</span><span class="m">001006</span>,...,002014<span class="o">]</span>
standard<span class="w">     </span>up<span class="w"> </span><span class="m">1</span>-00:00:00<span class="w">     </span><span class="m">12</span><span class="w">  </span>drain<span class="w"> </span>nid<span class="o">[</span><span class="m">001016</span>,...,001969<span class="o">]</span>
standard<span class="w">     </span>up<span class="w"> </span><span class="m">1</span>-00:00:00<span class="w">      </span><span class="m">5</span><span class="w">   </span>resv<span class="w"> </span>nid<span class="o">[</span><span class="m">001000</span>,001002-001004,001114<span class="o">]</span><span class="w"> </span>
standard<span class="w">     </span>up<span class="w"> </span><span class="m">1</span>-00:00:00<span class="w">    </span><span class="m">683</span><span class="w">  </span>alloc<span class="w"> </span>nid<span class="o">[</span><span class="m">001001</span>,...,001970-001991<span class="o">]</span><span class="w"> </span>
standard<span class="w">     </span>up<span class="w"> </span><span class="m">1</span>-00:00:00<span class="w">    </span><span class="m">214</span><span class="w">   </span>idle<span class="w"> </span>nid<span class="o">[</span><span class="m">001022</span>-001023,...,002015-002023<span class="o">]</span>
standard<span class="w">     </span>up<span class="w"> </span><span class="m">1</span>-00:00:00<span class="w">      </span><span class="m">2</span><span class="w">   </span>down<span class="w"> </span>nid<span class="o">[</span><span class="m">001021</span>,001050<span class="o">]</span>
</code></pre></div>
<p>Here we see the number of nodes in different states. For example, 683
nodes are allocated (running jobs), and 214 are idle (available to run
jobs).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>that long lists of node IDs have been abbreviated with <code>...</code>.</p>
</div>
<h3 id="sbatch-submitting-jobs"><code>sbatch</code>: submitting jobs</h3>
<p><code>sbatch</code> is used to submit a job script to the job submission system.
The script will typically contain one or more <code>srun</code> commands to launch
parallel tasks.</p>
<p>When you submit the job, the scheduler provides the job ID, which is
used to identify this job in other Slurm commands and when looking at
resource usage in SAFE.</p>
<div class="highlight"><pre><span></span><code>auser@ln01:~&gt;<span class="w"> </span>sbatch<span class="w"> </span>test-job.slurm
Submitted<span class="w"> </span>batch<span class="w"> </span>job<span class="w"> </span><span class="m">12345</span>
</code></pre></div>
<h3 id="squeue-monitoring-jobs"><code>squeue</code>: monitoring jobs</h3>
<p><code>squeue</code> without any options or arguments shows the current status of
all jobs known to the scheduler. For example:</p>
<div class="highlight"><pre><span></span><code>auser@ln01:~&gt;<span class="w"> </span>squeue
</code></pre></div>
<p>will list all jobs on ARCHER2.</p>
<p>The output of this is often overwhelmingly large. You can restrict the
output to just your jobs by adding the <code>-u $USER</code> option:</p>
<div class="highlight"><pre><span></span><code>auser@ln01:~&gt;<span class="w"> </span>squeue<span class="w"> </span>-u<span class="w"> </span><span class="nv">$USER</span>
</code></pre></div>
<h3 id="scancel-deleting-jobs"><code>scancel</code>: deleting jobs</h3>
<p><code>scancel</code> is used to delete a jobs from the scheduler. If the job is
waiting to run it is simply cancelled, if it is a running job then it is
stopped immediately.</p>
<p>If you only want to cancel a specific job you need to provide the job ID
of the job you wish to cancel/stop. For example:</p>
<div class="highlight"><pre><span></span><code>auser@ln01:~&gt;<span class="w"> </span>scancel<span class="w"> </span><span class="m">12345</span>
</code></pre></div>
<p>will cancel (if waiting) or stop (if running) the job with ID <code>12345</code>.</p>
<p><code>scancel</code> can take other options. For example, if you want to cancel all
your pending (queued) jobs but leave the running jobs running, you could
use:</p>
<div class="highlight"><pre><span></span><code>auser@ln01:~&gt;<span class="w"> </span>scancel<span class="w"> </span>--state<span class="o">=</span>PENDING<span class="w"> </span>--user<span class="o">=</span><span class="nv">$USER</span>
</code></pre></div>
<h2 id="resource-limits">Resource Limits</h2>
<p>The ARCHER2 resource limits for any given job are covered by three
separate attributes.</p>
<ul>
<li>The amount of <em>primary resource</em> you require, i.e., number of
    compute nodes.</li>
<li>The <em>partition</em> that you want to use - this specifies the nodes that
    are eligible to run your job.</li>
<li>The <em>Quality of Service (QoS)</em> that you want to use - this specifies
    the job limits that apply.</li>
</ul>
<h3 id="primary-resource">Primary resource</h3>
<p>The <em>primary resource</em> you can request for your job is the compute node.</p>
<div class="admonition information">
<p class="admonition-title">Information</p>
<p>The <code>--exclusive</code> option is enforced on ARCHER2 which means you will
always have access to all of the memory on the compute node regardless
of how many processes are actually running on the node.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You will not generally have access to the full amount of memory resource
on the the node as some is retained for running the operating system and
other system processes.</p>
</div>
<h3 id="partitions">Partitions</h3>
<p>On ARCHER2, compute nodes are grouped into partitions. You will have to
specify a partition using the <code>--partition</code> option in your Slurm
submission script. The following table has a list of active partitions
on ARCHER2.</p>
<div class="tabbed-set tabbed-alternate" data-tabs="1:1"><input checked="checked" id="__tabbed_1_1" name="__tabbed_1" type="radio" /><div class="tabbed-labels"><label for="__tabbed_1_1">Full system</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<table>
<thead>
<tr>
<th>Partition</th>
<th>Description</th>
<th>Max nodes available</th>
</tr>
</thead>
<tbody>
<tr>
<td>standard</td>
<td>CPU nodes with AMD EPYC 7742 64-core processor &times; 2, 256/512 GB memory</td>
<td>5860</td>
</tr>
<tr>
<td>highmem</td>
<td>CPU nodes with AMD EPYC 7742 64-core processor &times; 2, 512 GB memory</td>
<td>584</td>
</tr>
<tr>
<td>serial</td>
<td>CPU nodes with AMD EPYC 7742 64-core processor &times; 2, 512 GB memory</td>
<td>2</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code>standard</code> partition includes both the standard memory and high memory nodes but standard memory
nodes are preferentially chosen for jobs where possible. To guarantee access to high memory nodes
you should specify the <code>highmem</code> partition.</p>
</div>
<h3 id="quality-of-service-qos">Quality of Service (QoS)</h3>
<p>On ARCHER2, job limits are defined by the requested Quality of Service
(QoS), as specified by the <code>--qos</code> Slurm directive. The following table
lists the active QoS on ARCHER2.</p>
<div class="tabbed-set tabbed-alternate" data-tabs="2:1"><input checked="checked" id="__tabbed_2_1" name="__tabbed_2" type="radio" /><div class="tabbed-labels"><label for="__tabbed_2_1">Full system</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<table>
<thead>
<tr>
<th>QoS</th>
<th>Max Nodes Per Job</th>
<th>Max Walltime</th>
<th>Jobs Queued</th>
<th>Jobs Running</th>
<th>Partition(s)</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>standard</td>
<td>1024</td>
<td>24 hrs</td>
<td>64</td>
<td>16</td>
<td>standard</td>
<td>Maximum of 1024 nodes in use by any one user at any time</td>
</tr>
<tr>
<td>highmem</td>
<td>256</td>
<td>24 hrs</td>
<td>16</td>
<td>16</td>
<td>highmem</td>
<td>Maximum of 512 nodes in use by any one user at any time</td>
</tr>
<tr>
<td>taskfarm</td>
<td>16</td>
<td>24 hrs</td>
<td>128</td>
<td>32</td>
<td>standard</td>
<td>Maximum of 256 nodes in use by any one user at any time</td>
</tr>
<tr>
<td>short</td>
<td>32</td>
<td>20 mins</td>
<td>16</td>
<td>4</td>
<td>standard</td>
<td></td>
</tr>
<tr>
<td>long</td>
<td>64</td>
<td>48 hrs</td>
<td>16</td>
<td>16</td>
<td>standard</td>
<td>Minimum walltime of 24 hrs, maximum 512 nodes in use by any one user at any time, maximum of 2048 nodes in use by QoS</td>
</tr>
<tr>
<td>largescale</td>
<td>5860</td>
<td>12 hrs</td>
<td>8</td>
<td>1</td>
<td>standard</td>
<td>Minimum job size of 1025 nodes</td>
</tr>
<tr>
<td>lowpriority</td>
<td>2048</td>
<td>24 hrs</td>
<td>16</td>
<td>16</td>
<td>standard</td>
<td>Jobs not charged but requires at least 1 CU in budget to use.</td>
</tr>
<tr>
<td>serial</td>
<td>32 cores and/or 128 GB memory</td>
<td>24 hrs</td>
<td>32</td>
<td>4</td>
<td>serial</td>
<td>Jobs not charged but requires at least 1 CU in budget to use. Maximum of 32 cores and/or 128 GB in use by any one user at any time.</td>
</tr>
<tr>
<td>reservation</td>
<td>Size of reservation</td>
<td>Length of reservation</td>
<td>No limit</td>
<td>no limit</td>
<td>standard</td>
<td></td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<p>You can find out the QoS that you can use by running the following
command:</p>
<div class="tabbed-set tabbed-alternate" data-tabs="3:1"><input checked="checked" id="__tabbed_3_1" name="__tabbed_3" type="radio" /><div class="tabbed-labels"><label for="__tabbed_3_1">Full system</label></div>
<div class="tabbed-content">
<div class="tabbed-block"></div>
</div>
</div>
<div class="highlight"><pre><span></span><code>auser@ln01:~&gt;<span class="w"> </span>sacctmgr<span class="w"> </span>show<span class="w"> </span>assoc<span class="w"> </span><span class="nv">user</span><span class="o">=</span><span class="nv">$USER</span><span class="w"> </span><span class="nv">cluster</span><span class="o">=</span>archer2<span class="w"> </span><span class="nv">format</span><span class="o">=</span>cluster,account,user,qos%50
</code></pre></div>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p>If you have needs which do not fit within the current QoS, please
<a href="https://www.archer2.ac.uk/support-access/servicedesk.html">contact the Service
Desk</a> and we
can discuss how to accommodate your requirements.</p>
</div>
<h3 id="e-mail-notifications">E-mail notifications</h3>
<p>E-mail notifications from the scheduler are not currently available
on ARCHER2.</p>
<h3 id="priority">Priority</h3>
<p>Job priority on ARCHER2 depends on a number of different factors:</p>
<ul>
<li>The QoS your job has specified</li>
<li>The amount of time you have been queuing for</li>
<li>The number of nodes you have requested (job size)</li>
<li>Your current fairshare factor</li>
</ul>
<p>Each of these factors is normalised to a value between 0 and 1, is multiplied
with a weight and the resulting values combined to produce a priority for the job. 
The current job priority formula on ARCHER2 is:</p>
<div class="highlight"><pre><span></span><code>Priority = [10000 * P(QoS)] + [500 * P(Age)] + [300 * P(Fairshare)] + [100 * P(size)]
</code></pre></div>
<p>The priority factors are:</p>
<ul>
<li>P(QoS) - The QoS priority normalised to a value between 0 and 1. The maximum raw
  value is 10000 and the minimum is 0. Most QoS on ARCHER2 have a raw prioity of 500; the
  <code>lowpriority</code> QoS has a raw priority of 1.</li>
<li>P(Age) - The priority based on the job age normalised to a value between 0 and 1.
  The maximum raw value is 14 days (where P(Age) = 1).</li>
<li>P(Fairshare) - The fairshare priority normalised to a value between 0 and 1. Your
  fairshare priority is determined by a combination of your budget code fairshare 
  value and your user fairshare value within that budget code. The more use that 
  the budget code you are using has made of the system recently relative to other 
  budget codes on the system, the lower the budget code fairshare value will be; and the more
  use you have made of the system recently relative to other users within your
  budget code, the lower your user fairshare value will be. The decay half life 
  for fairshare on ARCHER2 is set to 2 days. <a href="https://slurm.schedmd.com/fair_tree.html">More information on the Slurm fairshare
  algorithm</a>.</li>
<li>P(Size) - The priority based on the job size normalised to a value between 0 and 1.
  The maximum size is the total number of ARCHER2 compute nodes.</li>
</ul>
<p>You can view the priorities for current queued jobs on the system with the <code>sprio</code>
command:</p>
<div class="highlight"><pre><span></span><code>auser@ln04:~&gt; sprio -l
          JOBID PARTITION   PRIORITY       SITE        AGE  FAIRSHARE    JOBSIZE        QOS
         828764 standard        1049          0         45          0          4       1000
         828765 standard        1049          0         45          0          4       1000
         828770 standard        1049          0         45          0          4       1000
         828771 standard        1012          0          8          0          4       1000
         828773 standard        1012          0          8          0          4       1000
         828791 standard        1012          0          8          0          4       1000
         828797 standard        1118          0        115          0          4       1000
         828800 standard        1154          0        150          0          4       1000
         828801 standard        1154          0        150          0          4       1000
         828805 standard        1118          0        115          0          4       1000
         828806 standard        1154          0        150          0          4       1000
</code></pre></div>
<h2 id="troubleshooting">Troubleshooting</h2>
<h3 id="slurm-error-messages">Slurm error messages</h3>
<p>An incorrect submission will cause Slurm to return an error.
Some common problems are listed below, with a suggestion about
the likely cause:</p>
<ul>
<li>
<p><code>sbatch: unrecognized option &lt;text&gt;</code></p>
<p>One of your options is invalid or has a typo. <code>man sbatch</code> to help.</p>
</li>
<li>
<p><code>error: Batch job submission failed: No partition specified or system default partition</code></p>
<p>A <code>--partition=</code> option is missing. You must specify the partition
(see the list above). This is most often <code>--partition=standard</code>.</p>
</li>
<li>
<p><code>error: invalid partition specified: &lt;partition&gt;</code></p>
<p><code>error: Batch job submission failed: Invalid partition name specified</code></p>
<p>Check the partition exists and check the spelling is correct.</p>
</li>
<li>
<p><code>error: Batch job submission failed: Invalid account or account/partition combination specified</code></p>
<p>This probably means an invalid account has been given. Check the
<code>--account=</code> options against valid accounts in SAFE.</p>
</li>
<li>
<p><code>error: Batch job submission failed: Invalid qos specification</code></p>
<p>A QoS option is either missing or invalid. Check the script has a
<code>--qos=</code> option and that the option is a valid one from the
table above. (Check the spelling of the QoS is correct.)</p>
</li>
<li>
<p><code>error: Your job has no time specification (--time=)...</code></p>
<p>Add an option of the form <code>--time=hours:minutes:seconds</code> to the
submission script. E.g., <code>--time=01:30:00</code> gives a time limit of
90 minutes.</p>
</li>
<li>
<p><code>error: QOSMaxWallDurationPerJobLimit</code>
    <code>error: Batch job submission failed: Job violates accounting/QOS policy</code>
    <code>(job submit limit, user's size and/or time limits)</code></p>
<p>The script has probably specified a time limit which is too long for
the corresponding QoS. E.g., the time limit for the short QoS
is 20 minutes.</p>
</li>
</ul>
<h3 id="slurm-job-state-codes">Slurm job state codes</h3>
<p>The <code>squeue</code> command allows users to view information for jobs managed by Slurm. Jobs
typically go through the following states: PENDING, RUNNING, COMPLETING, and COMPLETED.
The first table provides a description of some job state codes. The second table provides a description
of the <a href="#slurm-queued-reasons">reasons</a> that cause a job to be in a state.</p>
<table>
<thead>
<tr>
<th>Status</th>
<th>Code</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>PENDING</td>
<td>PD</td>
<td>Job is awaiting resource allocation.</td>
</tr>
<tr>
<td>RUNNING</td>
<td>R</td>
<td>Job currently has an allocation.</td>
</tr>
<tr>
<td>SUSPENDED</td>
<td>S</td>
<td>Job currently has an allocation.</td>
</tr>
<tr>
<td>COMPLETING</td>
<td>CG</td>
<td>Job is in the process of completing. Some processes on some nodes may still be active.</td>
</tr>
<tr>
<td>COMPLETED</td>
<td>CD</td>
<td>Job has terminated all processes on all nodes with an exit code of zero.</td>
</tr>
<tr>
<td>TIMEOUT</td>
<td>TO</td>
<td>Job terminated upon reaching its time limit.</td>
</tr>
<tr>
<td>STOPPED</td>
<td>ST</td>
<td>Job has an allocation, but execution has been stopped with SIGSTOP signal. CPUS have been retained by this job.</td>
</tr>
<tr>
<td>OUT_OF_MEMORY</td>
<td>OOM</td>
<td>Job experienced out of memory error.</td>
</tr>
<tr>
<td>FAILED</td>
<td>F</td>
<td>Job terminated with non-zero exit code or other failure condition.</td>
</tr>
<tr>
<td>NODE_FAIL</td>
<td>NF</td>
<td>Job terminated due to failure of one or more allocated nodes.</td>
</tr>
<tr>
<td>CANCELLED</td>
<td>CA</td>
<td>Job was explicitly cancelled by the user or system administrator. The job may or may not have been initiated.</td>
</tr>
</tbody>
</table>
<p>For a full list of see <a href="https://slurm.schedmd.com/squeue.html#lbAG">Job State Codes</a>.</p>
<h3 id="slurm-queued-reasons">Slurm queued reasons</h3>
<table>
<thead>
<tr>
<th>Reason</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Priority</td>
<td>One or more higher priority jobs exist for this partition or advanced reservation.</td>
</tr>
<tr>
<td>Resources</td>
<td>The job is waiting for resources to become available.</td>
</tr>
<tr>
<td>BadConstraints</td>
<td>The job's constraints can not be satisfied.</td>
</tr>
<tr>
<td>BeginTime</td>
<td>The job's earliest start time has not yet been reached.</td>
</tr>
<tr>
<td>Dependency</td>
<td>This job is waiting for a dependent job to complete.</td>
</tr>
<tr>
<td>Licenses</td>
<td>The job is waiting for a license.</td>
</tr>
<tr>
<td>WaitingForScheduling</td>
<td>No reason has been set for this job yet. Waiting for the scheduler to determine the appropriate reason.</td>
</tr>
<tr>
<td>Prolog</td>
<td>Its PrologSlurmctld program is still running.</td>
</tr>
<tr>
<td>JobHeldAdmin</td>
<td>The job is held by a system administrator.</td>
</tr>
<tr>
<td>JobHeldUser</td>
<td>The job is held by the user.</td>
</tr>
<tr>
<td>JobLaunchFailure</td>
<td>The job could not be launched. This may be due to a file system problem, invalid program name, etc.</td>
</tr>
<tr>
<td>NonZeroExitCode</td>
<td>The job terminated with a non-zero exit code.</td>
</tr>
<tr>
<td>InvalidAccount</td>
<td>The job's account is invalid.</td>
</tr>
<tr>
<td>InvalidQOS</td>
<td>The job's QOS is invalid.</td>
</tr>
<tr>
<td>QOSUsageThreshold</td>
<td>Required QOS threshold has been breached.</td>
</tr>
<tr>
<td>QOSJobLimit</td>
<td>The job's QOS has reached its maximum job count.</td>
</tr>
<tr>
<td>QOSResourceLimit</td>
<td>The job's QOS has reached some resource limit.</td>
</tr>
<tr>
<td>QOSTimeLimit</td>
<td>The job's QOS has reached its time limit.</td>
</tr>
<tr>
<td>NodeDown</td>
<td>A node required by the job is down.</td>
</tr>
<tr>
<td>TimeLimit</td>
<td>The job exhausted its time limit.</td>
</tr>
<tr>
<td>ReqNodeNotAvail</td>
<td>Some node specifically required by the job is not currently available. The node may currently be in use, reserved for another job, in an advanced reservation, DOWN, DRAINED, or not responding. Nodes which are DOWN, DRAINED, or not responding will be identified as part of the job's "reason" field as "UnavailableNodes". Such nodes will typically require the intervention of a system administrator to make available.</td>
</tr>
</tbody>
</table>
<p>For a full list of see <a href="https://slurm.schedmd.com/squeue.html#lbAF">Job Reasons</a>.</p>
<h2 id="output-from-slurm-jobs">Output from Slurm jobs</h2>
<p>Slurm places standard output (STDOUT) and standard error (STDERR) for
each job in the file <code>slurm_&lt;JobID&gt;.out</code>. This file appears in the job's
working directory once your job starts running.</p>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p>Output may be buffered - to enable live output, e.g. for monitoring
job status, add <code>--unbuffered</code> to the <code>srun</code> command in your Slurm
script.</p>
</div>
<h2 id="specifying-resources-in-job-scripts">Specifying resources in job scripts</h2>
<p>You specify the resources you require for your job using directives at
the top of your job submission script using lines that start with the
directive <code>#SBATCH</code>.</p>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p>Most options provided using <code>#SBATCH</code> directives can also be specified as
command line options to <code>srun</code>.</p>
</div>
<p>If you do not specify any options, then the default for each option will
be applied. As a minimum, all job submissions must specify the budget
that they wish to charge the job too with the option:</p>
<ul>
<li><code>--account=&lt;budgetID&gt;</code> your budget ID is usually something like
     <code>t01</code> or <code>t01-test</code>. You can see which budget codes you can charge
     to in SAFE.</li>
</ul>
<p>Other common options that are used are:</p>
<ul>
<li><code>--time=&lt;hh:mm:ss&gt;</code> the maximum walltime for your job. <em>e.g.</em> For
     a 6.5 hour walltime, you would use <code>--time=6:30:0</code>.</li>
<li><code>--job-name=&lt;jobname&gt;</code> set a name for the job to help identify it
     in Slurm</li>
</ul>
<p>To prevent the behaviour of batch scripts being dependent on the user
environment at the point of submission, the option</p>
<ul>
<li><code>--export=none</code> prevents the user environment from being exported
     to the batch system.</li>
</ul>
<p>Using the <code>--export=none</code> means that the behaviour of batch submissions
should be repeatable. We strongly recommend its use.</p>
<h3 id="additional-options-for-parallel-jobs">Additional options for parallel jobs</h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For parallel jobs, ARCHER2 operates in a <em>node exclusive</em> way. This
means that you are assigned resources in the units of full compute nodes
for your jobs (<em>i.e.</em> 128 cores) and that no other user can share those
compute nodes with you. Hence, the minimum amount of resource you can
request for a parallel job is 1 node (or 128 cores).</p>
</div>
<p>In addition, parallel jobs will also need to specify how many nodes,
parallel processes and threads they require.</p>
<ul>
<li><code>--nodes=&lt;nodes&gt;</code> the number of nodes to use for the job.</li>
<li><code>--ntasks-per-node=&lt;processes per node&gt;</code> the number of parallel
     processes (e.g. MPI ranks) per node.</li>
<li><code>--cpus-per-task=1</code> if you are using parallel processes only with
     no threading and you want to use all 128 cores on the node then you
     should set the number of CPUs (cores) per parallel process to 1.
     <strong>Important:</strong> if you are using threading (e.g. with OpenMP) or
     you want to use less than 128 cores per node (e.g. to access more 
     memory or memory bandwidth per core) then you will need to change
     this option as described below.</li>
<li><code>--cpu-freq=&lt;freq. in kHz&gt;</code> set the CPU frequency for the compute 
     nodes. Valid values are <code>2250000</code> (2.25 GHz), <code>2000000</code> (2.0 GHz),
     <code>1500000</code> (1.5 GHz). For more information on CPU frequency settings
     and energy use see the <a href="../energy/">Energy use</a> section.</li>
</ul>
<p>For parallel jobs that use threading (e.g. OpenMP) or when you want to
use less than 128 cores per node (e.g. to access more memory or memory
bandwidth per core), you will also need to change the <code>--cpus-per-task</code>
option.</p>
<p>For jobs using threading:
   - <code>--cpus-per-task=&lt;threads per task&gt;</code> the number of threads per
     parallel process (e.g. number of OpenMP threads per MPI task for
     hybrid MPI/OpenMP jobs). <strong>Important:</strong> you must also set the
     <code>OMP_NUM_THREADS</code> environment variable if using OpenMP in your
     job.</p>
<p>For jobs using less than 128 cores per node:
   - <code>--cpus-per-task=&lt;stride between placement of processes&gt;</code> the stride
     between the parallel processes. For example, if oyu want to double the
     memory and memory bandwidth per process on an ARCHER2 compute node you
     would want to place 64 processes per node and leave an empty core between
     each process you would set <code>--cpus-per-task=2</code> and <code>--ntasks-per-node=64</code>.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>You must also add <code>export SRUN_CPUS_PER_TASK=$SLURM_CPUS_PER_TASK</code> to
your job submission script to pass the <code>--cpus-per-task</code> setting from
the job script to the <code>srun</code> command. (Alternatively, you could use
the <code>--cpus-per-task</code> option in the srun command itself.) If you do not do
this then the placement of processes/threads will be incorrect and you 
will likely see poor performance of your application. </p>
</div>
<h3 id="options-for-jobs-on-the-data-analysis-nodes">Options for jobs on the data analysis nodes</h3>
<p>The data analysis nodes are shared between all users and can be used to 
run jobs that require small numbers of cores and/or access to an external
network to transfer data. These jobs are often <strong>serial jobs</strong> that only
require a single core.</p>
<p>To run jobs on the data analysis node you require the following options:</p>
<ul>
<li><code>--partition=serial</code> to select the data analysis nodes</li>
<li><code>--qos=serial</code> to select the data analysis QoS (see above for QoS limits)</li>
<li><code>--ntasks=&lt;number of cores&gt;</code> to select the number of cores you want
      to use in this job (up to the maximum defined in the QoS)</li>
<li><code>--mem=&lt;amount of memory&gt;</code> to select the amount of memory you require
      (up to the maximum defined in the QoS).</li>
</ul>
<p>More information on using the data analysis nodes (including example job
submission scripts) can be found in the
<a href="../analysis/">Data Analysis section</a> of the User and Best Practice Guide.</p>
<h2 id="srun-launching-parallel-jobs"><code>srun</code>: Launching parallel jobs</h2>
<p>If you are running parallel jobs, your job submission script should
contain one or more <code>srun</code> commands to launch the parallel executable
across the compute nodes. In most cases you will want to add the options
<code>--distribution=block:block</code> and <code>--hint=nomultithread</code> to your 
<code>srun</code> command to ensure you get the correct pinning of processes to 
cores on a compute node.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If you do not add the <code>--distribution=block:block</code> and <code>--hint=nomultithread</code>
options to your <code>srun</code> command the default process placement 
may lead to a drop in performance for your jobs on ARCHER2.</p>
</div>
<p>A brief explanation of these options:
 - <code>--hint=nomultithread</code> - do not use hyperthreads/SMP
 - <code>--distribution=block:block</code> - the first <code>block</code> means use a block distribution
   of processes across nodes (i.e. fill nodes before moving onto the next one) and
   the second <code>block</code> means use a block distribution of processes across "sockets"
   within a node (i.e. fill a "socket" before moving on to the next one).</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>The Slurm definition of a "socket" does not correspond to a physical CPU socket.
On ARCHER2 it corresponds to a 4-core CCX (Core CompleX).</p>
</div>
<h3 id="slurm-definition-of-a-socket">Slurm definition of a "socket"</h3>
<p>On ARCHER2, Slurm is configured with the following setting:</p>
<div class="highlight"><pre><span></span><code>SlurmdParameters=l3cache_as_socket
</code></pre></div>
<p>The effect of this setting is to define a Slurm socket as a unit that has a shared
L3 cache. On ARCHER2, this means that each Slurm "socket" corresponds to a 4-core 
CCX (Core CompleX). For a more detailed discussion on the hardware and the memory/cache
layout see <a href="../hardware/">the Hardware section</a>.</p>
<p>The effect of this setting can be illustrated by using the <code>xthi</code> program to report
placement when we select a cyclic distribution of processes across sockets from srun
(<code>--distribution=block:cyclic</code>). As you can see from the output from <code>xthi</code> included 
below, the <code>cyclic</code> per-socket distribution results in sequential MPI processes being 
placed on every 4th core (i.e. cyclic placement across CCX).</p>
<div class="highlight"><pre><span></span><code>Node summary for    1 nodes:
Node    0, hostname nid000006, mpi 128, omp   1, executable xthi_mpi
MPI summary: 128 ranks 
Node    0, rank    0, thread   0, (affinity =    0) 
Node    0, rank    1, thread   0, (affinity =    4) 
Node    0, rank    2, thread   0, (affinity =    8) 
Node    0, rank    3, thread   0, (affinity =   12) 
Node    0, rank    4, thread   0, (affinity =   16) 
Node    0, rank    5, thread   0, (affinity =   20) 
Node    0, rank    6, thread   0, (affinity =   24) 
Node    0, rank    7, thread   0, (affinity =   28) 
Node    0, rank    8, thread   0, (affinity =   32) 
Node    0, rank    9, thread   0, (affinity =   36) 
Node    0, rank   10, thread   0, (affinity =   40) 
Node    0, rank   11, thread   0, (affinity =   44) 
Node    0, rank   12, thread   0, (affinity =   48) 
Node    0, rank   13, thread   0, (affinity =   52) 
Node    0, rank   14, thread   0, (affinity =   56) 
Node    0, rank   15, thread   0, (affinity =   60) 
Node    0, rank   16, thread   0, (affinity =   64) 
Node    0, rank   17, thread   0, (affinity =   68) 
Node    0, rank   18, thread   0, (affinity =   72) 
Node    0, rank   19, thread   0, (affinity =   76) 
Node    0, rank   20, thread   0, (affinity =   80) 
Node    0, rank   21, thread   0, (affinity =   84) 
Node    0, rank   22, thread   0, (affinity =   88) 
Node    0, rank   23, thread   0, (affinity =   92) 
Node    0, rank   24, thread   0, (affinity =   96) 
Node    0, rank   25, thread   0, (affinity =  100) 
Node    0, rank   26, thread   0, (affinity =  104) 
Node    0, rank   27, thread   0, (affinity =  108) 
Node    0, rank   28, thread   0, (affinity =  112) 
Node    0, rank   29, thread   0, (affinity =  116) 
Node    0, rank   30, thread   0, (affinity =  120) 
Node    0, rank   31, thread   0, (affinity =  124) 
Node    0, rank   32, thread   0, (affinity =    1) 
Node    0, rank   33, thread   0, (affinity =    5) 
Node    0, rank   34, thread   0, (affinity =    9) 
Node    0, rank   35, thread   0, (affinity =   13) 
Node    0, rank   36, thread   0, (affinity =   17) 
Node    0, rank   37, thread   0, (affinity =   21) 
Node    0, rank   38, thread   0, (affinity =   25) 

...output trimmed...
</code></pre></div>
<h2 id="bolt-job-submission-script-creation-tool">bolt: Job submission script creation tool</h2>
<p>The bolt job submission script creation tool has been written by EPCC to
simplify the process of writing job submission scripts for modern
multicore architectures. Based on the options you supply, bolt will
generate a job submission script that uses ARCHER2 in a reasonable way.</p>
<p>MPI, OpenMP and hybrid MPI/OpenMP jobs are supported.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The tool will allow you to generate scripts for jobs that use the <code>long</code>
QoS but you will need to manually modify the resulting script to change
the QoS to <code>long</code>.</p>
</div>
<p>If there are problems or errors in your job parameter specifications
then bolt will print warnings or errors. However, bolt cannot detect all
problems.</p>
<h3 id="basic-usage">Basic Usage</h3>
<p>The basic syntax for using bolt is:</p>
<pre><code>bolt -n [parallel tasks] -N [parallel tasks per node] -d [number of threads per task] \
     -t [wallclock time (h:m:s)] -o [script name] -j [job name] -A [project code]  [arguments...]
</code></pre>
<p>Example 1: to generate a job script to run an executable called
<code>my_prog.x</code> for 24 hours using 8192 parallel (MPI) processes and 128
(MPI) processes per compute node you would use something like:</p>
<pre><code>bolt -n 8192 -N 128 -t 24:0:0 -o my_job.bolt -j my_job -A z01-budget my_prog.x arg1 arg2
</code></pre>
<p>(remember to substitute <code>z01-budget</code> for your actual budget code.)</p>
<p>Example 2: to generate a job script to run an executable called
<code>my_prog.x</code> for 3 hours using 2048 parallel (MPI) processes and 64 (MPI)
processes per compute node (i.e. using half of the cores on a compute
node), you would use:</p>
<pre><code>bolt -n 2048 -N 64 -t 3:0:0 -o my_job.bolt -j my_job -A z01-budget my_prog.x arg1 arg2
</code></pre>
<p>These examples generate the job script <code>my_job.bolt</code> with the correct
options to run <code>my_prog.x</code> with command line arguments <code>arg1</code> and
<code>arg2</code>. The project code against which the job will be charged is
specified with the ' -A ' option. As usual, the job script is submitted
as follows:</p>
<pre><code>sbatch my_job.bolt
</code></pre>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p>If you do not specify the script name with the '-o' option then your
script will be a file called <code>a.bolt</code>.</p>
</div>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p>If you do not specify the number of parallel tasks then bolt will try to
generate a serial job submission script (and throw an error on the
ARCHER2 4 cabinet system as serial jobs are not supported).</p>
</div>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p>If you do not specify a project code, bolt will use your default project
code (set by your login account).</p>
</div>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p>If you do not specify a job name, bolt will use either <code>bolt_ser_job</code>
(for serial jobs) or <code>bolt_par_job</code> (for parallel jobs).</p>
</div>
<h3 id="further-help">Further help</h3>
<p>You can access further help on using bolt on ARCHER2 with the ' -h '
option:</p>
<pre><code>bolt -h
</code></pre>
<p>A selection of other useful options are:</p>
<ul>
<li><code>-s</code> Write and submit the job script rather than just writing the
     job script.</li>
<li><code>-p</code> Force the job to be parallel even if it only uses a single
     parallel task.</li>
</ul>
<h2 id="checkscript-job-submission-script-validation-tool">checkScript job submission script validation tool</h2>
<p>The checkScript tool has been written to allow users to validate their
job submission scripts before submitting their jobs. The tool will read
your job submission script and try to identify errors, problems or
inconsistencies.</p>
<p>An example of the sort of output the tool can give would be:</p>
<pre><code>auser@ln01:/work/t01/t01/auser&gt; checkScript submit.slurm

===========================================================================
checkScript
---------------------------------------------------------------------------
Copyright 2011-2020  EPCC, The University of Edinburgh
This program comes with ABSOLUTELY NO WARRANTY.
This is free software, and you are welcome to redistribute it
under certain conditions.
===========================================================================

Script details
---------------
       User: auser
Script file: submit.slurm
  Directory: /work/t01/t01/auser (ok)
   Job name: test (ok)
  Partition: standard (ok)
        QoS: standard (ok)
Combination:          (ok)

Requested resources
-------------------
         nodes =              3                     (ok)
tasks per node =             16
 cpus per task =              8
cores per node =            128                     (ok)
OpenMP defined =           True                     (ok)
      walltime =          1:0:0                     (ok)

CU Usage Estimate (if full job time used)
------------------------------------------
                      CU =          3.000



checkScript finished: 0 warning(s) and 0 error(s).
</code></pre>
<h2 id="checking-scripts-and-estimating-start-time-with-test-only">Checking scripts and estimating start time with <code>--test-only</code></h2>
<p><code>sbatch --test-only</code> validates the batch script and returns an estimate of when the job would be scheduled to run given the current scheduler state. Please note that it is just an estimate, the actual start time may differ as the scheduler status when the start time was estimated may be different once the job is actually submitted and due to subsequent changes to the scheduler state. The job is not actually submitted.</p>
<div class="highlight"><pre><span></span><code>auser@ln01:~&gt; sbatch --test-only submit.slurm
sbatch: Job 1039497 to start at 2022-02-01T23:20:51 using 256 processors on nodes nid002836
in partition standard
</code></pre></div>
<h2 id="example-job-submission-scripts">Example job submission scripts</h2>
<p>A subset of example job submission scripts are included in full below.
Examples are provided for both the full system and the 4-cabinet system.</p>
<h3 id="example-job-submission-script-for-mpi-parallel-job">Example: job submission script for MPI parallel job</h3>
<p>A simple MPI job submission script to submit a job using 4 compute nodes
and 128 MPI ranks per node for 20 minutes would look like:</p>
<div class="highlight"><pre><span></span><code><span class="ch">#!/bin/bash</span>

<span class="c1"># Slurm job options (job-name, compute nodes, job time)</span>
<span class="kp">#SBATCH --job-name=Example_MPI_Job</span>
<span class="kp">#SBATCH --time=0:20:0</span>
<span class="kp">#SBATCH --nodes=4</span>
<span class="kp">#SBATCH --ntasks-per-node=128</span>
<span class="kp">#SBATCH --cpus-per-task=1</span>

<span class="c1"># Replace [budget code] below with your budget code (e.g. t01)</span>
<span class="kp">#SBATCH --account=[budget code]             </span>
<span class="kp">#SBATCH --partition=standard</span>
<span class="kp">#SBATCH --qos=standard</span>

<span class="c1"># Set the number of threads to 1</span>
<span class="c1">#   This prevents any threaded system libraries from automatically </span>
<span class="c1">#   using threading.</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="m">1</span>

<span class="c1"># Propagate the cpus-per-task setting from script to srun commands</span>
<span class="c1">#    By default, Slurm does not propagate this setting from the sbatch</span>
<span class="c1">#    options to srun commands in the job script. If this is not done,</span>
<span class="c1">#    process/thread pinning may be incorrect leading to poor performance</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">SRUN_CPUS_PER_TASK</span><span class="o">=</span><span class="nv">$SLURM_CPUS_PER_TASK</span>

<span class="c1"># Launch the parallel job</span>
<span class="c1">#   Using 512 MPI processes and 128 MPI processes per node</span>
<span class="c1">#   srun picks up the distribution from the sbatch options</span>

<span class="nb">srun</span><span class="w"> </span>--distribution<span class="o">=</span>block:block<span class="w"> </span>--hint<span class="o">=</span>nomultithread<span class="w"> </span>./my_mpi_executable.x
</code></pre></div>
<p>This will run your executable "my_mpi_executable.x" in parallel on 512
MPI processes using 4 nodes (128 cores per node, i.e. not using
hyper-threading). Slurm will allocate 4 nodes to your job and srun will
place 128 MPI processes on each node (one per physical core).</p>
<p>See above for a more detailed discussion of the different <code>sbatch</code>
options</p>
<h3 id="example-job-submission-script-for-mpiopenmp-mixed-mode-parallel-job">Example: job submission script for MPI+OpenMP (mixed mode) parallel job</h3>
<p>Mixed mode codes that use both MPI (or another distributed memory
parallel model) and OpenMP should take care to ensure that the shared
memory portion of the process/thread placement does not span more than
one NUMA region. Nodes on ARCHER2 are made up of two sockets each
containing 4 NUMA regions of 16 cores, i.e. there are 8 NUMA regions in
total. Therefore the total number of threads should ideally not be
greater than 16, and also needs to be a factor of 16. Sensible choices
for the number of threads are therefore 1 (single-threaded), 2, 4, 8,
and 16. More information about using OpenMP and MPI+OpenMP can be found
in the Tuning chapter.</p>
<p>To ensure correct placement of MPI processes the number of cpus-per-task
needs to match the number of OpenMP threads, and the number of
tasks-per-node should be set to ensure the entire node is filled with
MPI tasks.</p>
<p>In the example below, we are using 4 nodes for 6 hours. There are 32 MPI
processes in total (8 MPI processes per node) and 16 OpenMP threads per
MPI process. This results in all 128 physical cores per node being used.</p>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p>Note the use of the <code>export OMP_PLACES=cores</code> environment option to
generate the correct thread pinning.</p>
</div>
<div class="highlight"><pre><span></span><code><span class="ch">#!/bin/bash</span>

<span class="c1"># Slurm job options (job-name, compute nodes, job time)</span>
<span class="kp">#SBATCH --job-name=Example_MPI_Job</span>
<span class="kp">#SBATCH --time=0:20:0</span>
<span class="kp">#SBATCH --nodes=4</span>
<span class="kp">#SBATCH --ntasks-per-node=8</span>
<span class="kp">#SBATCH --cpus-per-task=16</span>

<span class="c1"># Replace [budget code] below with your project code (e.g. t01)</span>
<span class="kp">#SBATCH --account=[budget code] </span>
<span class="kp">#SBATCH --partition=standard</span>
<span class="kp">#SBATCH --qos=standard</span>

<span class="c1"># Propagate the cpus-per-task setting from script to srun commands</span>
<span class="c1">#    By default, Slurm does not propagate this setting from the sbatch</span>
<span class="c1">#    options to srun commands in the job script. If this is not done,</span>
<span class="c1">#    process/thread pinning may be incorrect leading to poor performance</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">SRUN_CPUS_PER_TASK</span><span class="o">=</span><span class="nv">$SLURM_CPUS_PER_TASK</span>

<span class="c1"># Set the number of threads to 16 and specify placement</span>
<span class="c1">#   There are 16 OpenMP threads per MPI process</span>
<span class="c1">#   We want one thread per physical core</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="m">16</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">OMP_PLACES</span><span class="o">=</span>cores

<span class="c1"># Launch the parallel job</span>
<span class="c1">#   Using 32 MPI processes</span>
<span class="c1">#   8 MPI processes per node</span>
<span class="c1">#   16 OpenMP threads per MPI process</span>
<span class="c1">#   Additional srun options to pin one thread per physical core</span>
<span class="nb">srun</span><span class="w"> </span>--hint<span class="o">=</span>nomultithread<span class="w"> </span>--distribution<span class="o">=</span>block:block<span class="w"> </span>./my_mixed_executable.x<span class="w"> </span>arg1<span class="w"> </span>arg2
</code></pre></div>
<h2 id="job-arrays">Job arrays</h2>
<p>The Slurm job scheduling system offers the <em>job array</em> concept, for
running collections of almost-identical jobs. For example, running the
same program several times with different arguments or input data.</p>
<p>Each job in a job array is called a <em>subjob</em>. The subjobs of a job array
can be submitted and queried as a unit, making it easier and cleaner to
handle the full set, compared to individual jobs.</p>
<p>All subjobs in a job array are started by running the same job script.
The job script also contains information on the number of jobs to be
started, and Slurm provides a subjob index which can be passed to the
individual subjobs or used to select the input data per subjob.</p>
<h3 id="job-script-for-a-job-array">Job script for a job array</h3>
<p>As an example, the following script runs 56 subjobs, with the subjob
index as the only argument to the executable. Each subjob requests a
single node and uses all 128 cores on the node by placing 1 MPI process
per core and specifies 4 hours maximum runtime per subjob:</p>
<div class="highlight"><pre><span></span><code><span class="ch">#!/bin/bash</span>
<span class="c1"># Slurm job options (job-name, compute nodes, job time)</span>
<span class="kp">#SBATCH --job-name=Example_Array_Job</span>
<span class="kp">#SBATCH --time=04:00:00</span>
<span class="kp">#SBATCH --nodes=1</span>
<span class="kp">#SBATCH --ntasks-per-node=128</span>
<span class="kp">#SBATCH --cpus-per-task=1</span>
<span class="kp">#SBATCH --array=0-55</span>

<span class="c1"># Replace [budget code] below with your budget code (e.g. t01)</span>
<span class="kp">#SBATCH --account=[budget code]  </span>
<span class="kp">#SBATCH --partition=standard</span>
<span class="kp">#SBATCH --qos=standard</span>

<span class="c1"># Propagate the cpus-per-task setting from script to srun commands</span>
<span class="c1">#    By default, Slurm does not propagate this setting from the sbatch</span>
<span class="c1">#    options to srun commands in the job script. If this is not done,</span>
<span class="c1">#    process/thread pinning may be incorrect leading to poor performance</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">SRUN_CPUS_PER_TASK</span><span class="o">=</span><span class="nv">$SLURM_CPUS_PER_TASK</span>

<span class="c1"># Set the number of threads to 1</span>
<span class="c1">#   This prevents any threaded system libraries from automatically </span>
<span class="c1">#   using threading.</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="m">1</span>

<span class="nb">srun</span><span class="w"> </span>--distribution<span class="o">=</span>block:block<span class="w"> </span>--hint<span class="o">=</span>nomultithread<span class="w"> </span>/path/to/exe<span class="w"> </span><span class="nv">$SLURM_ARRAY_TASK_ID</span>
</code></pre></div>
<h3 id="submitting-a-job-array">Submitting a job array</h3>
<p>Job arrays are submitted using <code>sbatch</code> in the same way as for standard
jobs:</p>
<div class="highlight"><pre><span></span><code>sbatch job_script.pbs 
</code></pre></div>
<h2 id="job-chaining">Job chaining</h2>
<p>Job dependencies can be used to construct complex pipelines or chain
together long simulations requiring multiple steps.</p>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p>The <code>--parsable</code> option to <code>sbatch</code> can simplify working with job
dependencies. It returns the job ID in a format that can be used as the
input to other commands.</p>
</div>
<p>For example:</p>
<div class="highlight"><pre><span></span><code>jobid=$(sbatch --parsable first_job.sh)
sbatch --dependency=afterok:$jobid second_job.sh
</code></pre></div>
<p>or for a longer chain:</p>
<div class="highlight"><pre><span></span><code>jobid1=$(sbatch --parsable first_job.sh)
jobid2=$(sbatch --parsable --dependency=afterok:$jobid1 second_job.sh)
jobid3=$(sbatch --parsable --dependency=afterok:$jobid1 third_job.sh)
sbatch --dependency=afterok:$jobid2,afterok:$jobid3 last_job.sh
</code></pre></div>
<h2 id="using-multiple-srun-commands-in-a-single-job-script">Using multiple <code>srun</code> commands in a single job script</h2>
<p>You can use multiple <code>srun</code> commands within in a Slurm job submission script
to allow you to use the resource requested more flexibly. For example, you 
could run a collection of smaller jobs within the requested resources or
you could even subdivide nodes if your individual calculations do not scale
up to use all 128 cores on a node.</p>
<p>In this guide we will cover two scenarios:</p>
<ol>
<li>Subdividing the job into multiple full-node or multi-node subjobs, e.g.
    requesting 100 nodes and running 100, 1-node subjobs or 50, 2-node 
    subjobs.</li>
<li>Subdividing the job into multiple subjobs that each use a fraction of a
    node, e.g. requesting 2 nodes and running 256, 1-core subjobs or 16,
    16-core subjobs.</li>
</ol>
<h3 id="running-multiple-full-node-subjobs-within-a-larger-job">Running multiple, full-node subjobs within a larger job</h3>
<p>When subdivding a larger job into smaller subjobs you typically need to 
overwrite the <code>--nodes</code> option to <code>srun</code> and add the <code>--ntasks</code> option
to ensure that each subjob runs on the correct number of nodes and that
subjobs are placed correctly onto separate nodes.</p>
<p>For example, we will show how to request 100 nodes and then run 100
separate 1-node jobs, each of which use 128 MPI processes and which
run on a different compute node. We start by showing 
the job script that would achieve this and then explain how this works
and the options used. In our case, we will run 100 copies of the <code>xthi</code> 
program that prints the process placement on the node it is running on.</p>
<div class="highlight"><pre><span></span><code><span class="ch">#!/bin/bash</span>

<span class="c1"># Slurm job options (job-name, compute nodes, job time)</span>
<span class="kp">#SBATCH --job-name=multi_xthi</span>
<span class="kp">#SBATCH --time=0:20:0</span>
<span class="kp">#SBATCH --nodes=100</span>
<span class="kp">#SBATCH --ntasks-per-node=128</span>
<span class="kp">#SBATCH --cpus-per-task=1</span>

<span class="c1"># Replace [budget code] below with your budget code (e.g. t01)</span>
<span class="kp">#SBATCH --account=[budget code]             </span>
<span class="kp">#SBATCH --partition=standard</span>
<span class="kp">#SBATCH --qos=standard</span>

<span class="c1"># Load the xthi module</span>
module<span class="w"> </span>load<span class="w"> </span>xthi

<span class="c1"># Propagate the cpus-per-task setting from script to srun commands</span>
<span class="c1">#    By default, Slurm does not propagate this setting from the sbatch</span>
<span class="c1">#    options to srun commands in the job script. If this is not done,</span>
<span class="c1">#    process/thread pinning may be incorrect leading to poor performance</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">SRUN_CPUS_PER_TASK</span><span class="o">=</span><span class="nv">$SLURM_CPUS_PER_TASK</span>

<span class="c1"># Set the number of threads to 1</span>
<span class="c1">#   This prevents any threaded system libraries from automatically </span>
<span class="c1">#   using threading.</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="m">1</span>

<span class="c1"># Loop over 100 subjobs starting each of them on a separate node</span>
<span class="k">for</span><span class="w"> </span>i<span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="k">$(</span>seq<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">100</span><span class="k">)</span>
<span class="k">do</span>
<span class="c1"># Launch this subjob on 1 node, note nodes and ntasks options and &amp; to place subjob in the background</span>
<span class="w">    </span><span class="nb">srun</span><span class="w"> </span>--nodes<span class="o">=</span><span class="m">1</span><span class="w"> </span>--ntasks<span class="o">=</span><span class="m">128</span><span class="w"> </span>--distribution<span class="o">=</span>block:block<span class="w"> </span>--hint<span class="o">=</span>nomultithread<span class="w"> </span>xthi<span class="w"> </span>&gt;<span class="w"> </span>placement<span class="si">${</span><span class="nv">i</span><span class="si">}</span>.txt<span class="w"> </span><span class="p">&amp;</span>
<span class="k">done</span>
<span class="c1"># Wait for all background subjobs to finish</span>
<span class="nb">wait</span>
</code></pre></div>
<p>Key points from the example job script:</p>
<ul>
<li>The <code>#SBATCH</code> options select 100 full nodes in the usual way.</li>
<li>Each subjob <code>srun</code> command sets the following:<ul>
<li><code>--nodes=1</code> We need override this setting from the main job so that each subjob only uses 1 node</li>
<li><code>--ntasks=128</code> For normal jobs, the number of parallel tasks (MPI processes) is calculated from
  the number of nodes you request and the number of tasks per node. We need to explicitly tell <code>srun</code>
  how many we require for this subjob.</li>
<li><code>--distribution=block:block --hint=nomultithread</code> These options ensure correct placement of
  processes within the compute nodes.</li>
<li><code>&amp;</code> Each subjob <code>srun</code> command ends with an ampersand to place the process in the background
  and move on to the next loop iteration (and subjob submission). Without this, the script would
  wait for this subjob to complete before moving on to submit the next.</li>
</ul>
</li>
<li>Finally, there is the <code>wait</code> command to tell the script to wait for all the background subjobs
to complete before exiting. If we did not have this in place, the script would exit as soon as the
last subjob was submitted and kill all running subjobs.</li>
</ul>
<h3 id="running-multiple-subjobs-that-each-use-a-fraction-of-a-node">Running multiple subjobs that each use a fraction of a node</h3>
<p>As the ARCHER2 nodes contain a large number of cores (128 per node) it
may sometimes be useful to be able to run multiple executables on a single
node. For example, you may want to run 128 copies of a serial executable or
Python script; or, you may want to run multiple copies of parallel executables
that use fewer than 128 cores each. This use model is possible using 
multiple <code>srun</code> commands in a job script on ARCHER2</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You can never share a compute node with another user. Although you can
use <code>srun</code> to place multiple copies of an executable or script on a 
compute node, you still have exclusive use of that node. The minimum
amount of resources you can reserve for your use on ARCHER2 is a single
node.</p>
</div>
<p>When using <code>srun</code> to place multiple executables or scripts on a compute 
node you must be aware of a few things:</p>
<ul>
<li>The <code>srun</code> command must specify any Slurm options that differ in value
   from those specified to <code>sbatch</code>. This typically means that you need 
   to specify the <code>--nodes</code>, <code>--ntasks</code> and <code>--ntasks-per-node</code> options to <code>srun</code>.</li>
<li>You will need to include the <code>--exact</code> flag to your <code>srun</code> command. With 
   this flag on, Slurm will ensure that the resources you request are assigned 
   to your subjob. Furthermore, if the resources are not currently available, 
   Slurm will output a message letting you know that this is the case and 
   stall the launch of this subjob until enough of your previous subjobs have 
   completed to free up the resources for this subjob.</li>
<li>You will need to define the memory required by each subjob with the 
   <code>--mem=&lt;amount of memory&gt;</code> flag. The amount of memory is given in MiB 
   by default but other units can be specified. If you do not know how 
   much memory to specify, we recommend that you specify 1500M (1,500 MiB) 
   per core being used.</li>
<li>You will need to place each <code>srun</code> command into the background and 
   then use the <code>wait</code> command at the end of the submission script to
   make sure it does not exit before the commands are complete.</li>
<li>If you want to use more than one node in the job and use multiple <code>srun</code>
   per node (e.g. 256 single core processes across 2 nodes) then you need
   to pass the node ID to the <code>srun</code> commands otherwise Slurm will oversubscribe
   cores on the first node.</li>
</ul>
<p>Below, we provide four examples or running multiple subjobs in a node: 
one that runs 128 serial processes across a single node; one that runs 8 
subjobs each of which use 8 MPI processes with 2 OpenMP threads per MPI 
process; one that runs four inhomogeneous jobs, each of which requires a 
different number of MPI processes and OpenMP threads per process; and one 
that runs 256 serial processes across two nodes.</p>
<h4 id="example-1-128-serial-tasks-running-on-a-single-node">Example 1: 128 serial tasks running on a single node</h4>
<p>For our first example, we will run 128 single-core copies of the <code>xthi</code> program (which
prints process/thread placement) on a single ARCHER2 compute node with each
copy of <code>xthi</code> pinned to a different core. The job submission script for
this example would look like:</p>
<div class="highlight"><pre><span></span><code><span class="ch">#!/bin/bash</span>
<span class="c1"># Slurm job options (job-name, compute nodes, job time)</span>
<span class="kp">#SBATCH --job-name=MultiSerialOnCompute</span>
<span class="kp">#SBATCH --time=0:10:0</span>
<span class="kp">#SBATCH --nodes=1</span>
<span class="kp">#SBATCH --ntasks-per-node=128</span>
<span class="kp">#SBATCH --cpus-per-task=1</span>
<span class="kp">#SBATCH --hint=nomultithread</span>
<span class="kp">#SBATCH --distribution=block:block</span>

<span class="c1"># Replace [budget code] below with your budget code (e.g. t01)</span>
<span class="kp">#SBATCH --account=[budget code]  </span>
<span class="kp">#SBATCH --partition=standard</span>
<span class="kp">#SBATCH --qos=standard</span>

<span class="c1"># Make xthi available</span>
module<span class="w"> </span>load<span class="w"> </span>xthi

<span class="c1"># Set the number of threads to 1</span>
<span class="c1">#   This prevents any threaded system libraries from automatically </span>
<span class="c1">#   using threading.</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="m">1</span>

<span class="c1"># Propagate the cpus-per-task setting from script to srun commands</span>
<span class="c1">#    By default, Slurm does not propagate this setting from the sbatch</span>
<span class="c1">#    options to srun commands in the job script. If this is not done,</span>
<span class="c1">#    process/thread pinning may be incorrect leading to poor performance</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">SRUN_CPUS_PER_TASK</span><span class="o">=</span><span class="nv">$SLURM_CPUS_PER_TASK</span>

<span class="c1"># Loop over 128 subjobs pinning each to a different core</span>
<span class="k">for</span><span class="w"> </span>i<span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="k">$(</span>seq<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">128</span><span class="k">)</span>
<span class="k">do</span>
<span class="c1"># Launch subjob overriding job settings as required and in the background</span>
<span class="c1"># Make sure to change the amount specified by the `--mem=` flag to the amount </span>
<span class="c1"># of memory required. The amount of memory is given in MiB by default but other</span>
<span class="c1"># units can be specified. If you do not know how much memory to specify, we </span>
<span class="c1"># recommend that you specify `--mem=1500M` (1,500 MiB).</span>
<span class="nb">srun</span><span class="w"> </span>--nodes<span class="o">=</span><span class="m">1</span><span class="w"> </span>--ntasks<span class="o">=</span><span class="m">1</span><span class="w"> </span>--ntasks-per-node<span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--exact<span class="w"> </span>--mem<span class="o">=</span>1500M<span class="w"> </span>xthi<span class="w"> </span>&gt;<span class="w"> </span>placement<span class="si">${</span><span class="nv">i</span><span class="si">}</span>.txt<span class="w"> </span><span class="p">&amp;</span>
<span class="k">done</span>

<span class="c1"># Wait for all subjobs to finish</span>
<span class="nb">wait</span>
</code></pre></div>
<h4 id="example-2-8-subjobs-on-1-node-each-with-8-mpi-processes-and-2-openmp-threads-per-process">Example 2: 8 subjobs on 1 node each with 8 MPI processes and 2 OpenMP threads per process</h4>
<p>For our second example, we will run 8 subjobs, each running the <code>xthi</code> program (which
prints process/thread placement) across 1 node. Each subjob will use 8 MPI processes
and 2 OpenMP threads per process. The job submission script for
this example would look like:</p>
<div class="highlight"><pre><span></span><code><span class="ch">#!/bin/bash</span>
<span class="c1"># Slurm job options (job-name, compute nodes, job time)</span>
<span class="kp">#SBATCH --job-name=MultiParallelOnCompute</span>
<span class="kp">#SBATCH --time=0:10:0</span>
<span class="kp">#SBATCH --nodes=1</span>
<span class="kp">#SBATCH --ntasks-per-node=64</span>
<span class="kp">#SBATCH --cpus-per-task=2</span>
<span class="kp">#SBATCH --hint=nomultithread</span>
<span class="kp">#SBATCH --distribution=block:block</span>

<span class="c1"># Replace [budget code] below with your budget code (e.g. t01)</span>
<span class="kp">#SBATCH --account=[budget code]  </span>
<span class="kp">#SBATCH --partition=standard</span>
<span class="kp">#SBATCH --qos=standard</span>

<span class="c1"># Make xthi available</span>
module<span class="w"> </span>load<span class="w"> </span>xthi

<span class="c1"># Set the number of threads to 2 as required by all subjobs</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="m">2</span>

<span class="c1"># Loop over 8 subjobs</span>
<span class="k">for</span><span class="w"> </span>i<span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="k">$(</span>seq<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">8</span><span class="k">)</span>
<span class="k">do</span>
<span class="w">    </span><span class="nb">echo</span><span class="w"> </span><span class="nv">$j</span><span class="w"> </span><span class="nv">$i</span>
<span class="w">    </span><span class="c1"># Launch subjob overriding job settings as required and in the background</span>
<span class="w">    </span><span class="c1"># Make sure to change the amount specified by the `--mem=` flag to the amount </span>
<span class="w">    </span><span class="c1"># of memory required. The amount of memory is given in MiB by default but other</span>
<span class="w">    </span><span class="c1"># units can be specified. If you do not know how much memory to specify, we </span>
<span class="w">    </span><span class="c1"># recommend that you specify `--mem=12500M` (12,500 MiB).</span>
<span class="w">    </span><span class="nb">srun</span><span class="w"> </span>--nodes<span class="o">=</span><span class="m">1</span><span class="w"> </span>--ntasks<span class="o">=</span><span class="m">8</span><span class="w"> </span>--ntasks-per-node<span class="o">=</span><span class="m">8</span><span class="w"> </span>--cpus-per-task<span class="o">=</span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--exact<span class="w"> </span>--mem<span class="o">=</span>12500M<span class="w"> </span>xthi<span class="w"> </span>&gt;<span class="w"> </span>placement<span class="si">${</span><span class="nv">i</span><span class="si">}</span>.txt<span class="w"> </span><span class="p">&amp;</span>
<span class="k">done</span>

<span class="c1"># Wait for all subjobs to finish</span>
<span class="nb">wait</span>
</code></pre></div>
<h4 id="example-3-running-inhomogeneous-subjobs-on-one-node">Example 3: Running inhomogeneous subjobs on one node</h4>
<p>For our third example, we will run 4 subjobs, each running the <code>xthi</code> program 
(which prints process/thread placement) across 1 node. Our subjobs will each 
run with a different number of MPI processes and OpenMP threads. We will run:
one job with 64 MPI processes and 1 OpenMP process per thread; one job with 
16 MPI processes and 2 threads per process; one job with 4 MPI processes and 
4 OpenMP threads per job; and, one job with 1 MPI process and 16 OpenMP 
threads  per job.</p>
<p>To be able to change the number of MPI processes and OpenMP threads per 
process, we will need to forgo using the <code>#SBATCH --ntasks-per-node</code> and the 
<code>#SBATCH cpus-per-task</code> commands -- if you set these Slurm will not let you 
alter the <code>OMP_NUM_THREADS</code> variable and you will not be able to change the 
number of OpenMP threads per process between each job.</p>
<p>Before each <code>srun</code> command, you will need to define the number of OpenMP 
threads per process you want by changing the <code>OMP_NUM_THREADS</code> variable. 
Furthermore, for each <code>srun</code> command, you will need to set the <code>--ntasks</code> flag 
to equal the number of MPI processes you want to use. You will also need to 
set the <code>--cpus-per-task</code> flag to equal the number of OpenMP threads per 
process you want to use.</p>
<div class="highlight"><pre><span></span><code><span class="ch">#!/bin/bash</span>
<span class="c1"># Slurm job options (job-name, compute nodes, job time)</span>
<span class="kp">#SBATCH --job-name=MultiParallelOnCompute</span>
<span class="kp">#SBATCH --time=0:10:0</span>
<span class="kp">#SBATCH --nodes=1</span>
<span class="kp">#SBATCH --hint=nomultithread</span>
<span class="kp">#SBATCH --distribution=block:block</span>

<span class="c1"># Replace [budget code] below with your budget code (e.g. t01)</span>
<span class="kp">#SBATCH --account=[budget code]  </span>
<span class="kp">#SBATCH --partition=standard</span>
<span class="kp">#SBATCH --qos=standard</span>

<span class="c1"># Make xthi available</span>
module<span class="w"> </span>load<span class="w"> </span>xthi

<span class="c1"># Set the number of threads to value required by the first job</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="m">1</span>
<span class="nb">srun</span><span class="w"> </span>--ntasks<span class="o">=</span><span class="m">64</span><span class="w"> </span>--cpus-per-task<span class="o">=</span><span class="si">${</span><span class="nv">OMP_NUM_THREADS</span><span class="si">}</span><span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--exact<span class="w"> </span>--mem<span class="o">=</span>12500M<span class="w"> </span>xthi<span class="w"> </span>&gt;<span class="w"> </span>placement<span class="si">${</span><span class="nv">OMP_NUM_THREADS</span><span class="si">}</span>.txt<span class="w"> </span><span class="p">&amp;</span>

<span class="c1"># Set the number of threads to the value required by the second job</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="m">2</span>
<span class="nb">srun</span><span class="w"> </span>--ntasks<span class="o">=</span><span class="m">16</span><span class="w"> </span>--cpus-per-task<span class="o">=</span><span class="si">${</span><span class="nv">OMP_NUM_THREADS</span><span class="si">}</span><span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--exact<span class="w"> </span>--mem<span class="o">=</span>12500M<span class="w"> </span>xthi<span class="w"> </span>&gt;<span class="w"> </span>placement<span class="si">${</span><span class="nv">OMP_NUM_THREADS</span><span class="si">}</span>.txt<span class="w"> </span><span class="p">&amp;</span>

<span class="c1"># Set the number of threads to the value required by the second job</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="m">4</span>
<span class="nb">srun</span><span class="w"> </span>--ntasks<span class="o">=</span><span class="m">4</span><span class="w"> </span>--cpus-per-task<span class="o">=</span><span class="si">${</span><span class="nv">OMP_NUM_THREADS</span><span class="si">}</span><span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--exact<span class="w"> </span>--mem<span class="o">=</span>12500M<span class="w"> </span>xthi<span class="w"> </span>&gt;<span class="w"> </span>placement<span class="si">${</span><span class="nv">OMP_NUM_THREADS</span><span class="si">}</span>.txt<span class="w"> </span><span class="p">&amp;</span>

<span class="c1"># Set the number of threads to the value required by the second job</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="m">16</span>
<span class="nb">srun</span><span class="w"> </span>--ntasks<span class="o">=</span><span class="m">1</span><span class="w"> </span>--cpus-per-task<span class="o">=</span><span class="si">${</span><span class="nv">OMP_NUM_THREADS</span><span class="si">}</span><span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--exact<span class="w"> </span>--mem<span class="o">=</span>12500M<span class="w"> </span>xthi<span class="w"> </span>&gt;<span class="w"> </span>placement<span class="si">${</span><span class="nv">OMP_NUM_THREADS</span><span class="si">}</span>.txt<span class="w"> </span><span class="p">&amp;</span>

<span class="c1"># Wait for all subjobs to finish</span>
<span class="nb">wait</span>
</code></pre></div>
<h4 id="example-4-256-serial-tasks-running-across-two-nodes">Example 4: 256 serial tasks running across two nodes</h4>
<p>For our fourth example, we will run 256 single-core copies of the <code>xthi</code> program (which
prints process/thread placement) across two ARCHER2 compute nodes with each
copy of <code>xthi</code> pinned to a different core. We will illustrate a mechanism for getting the
node IDs to pass to <code>srun</code> as this is required to ensure that the individual subjobs are
assigned to the correct node. This mechanism uses the <code>scontrol</code> command to turn the 
nodelist from <code>sbatch</code> into a format we can use as input to <code>srun</code>. The job submission
script for this example would look like:</p>
<div class="highlight"><pre><span></span><code><span class="ch">#!/bin/bash</span>
<span class="c1"># Slurm job options (job-name, compute nodes, job time)</span>
<span class="kp">#SBATCH --job-name=MultiSerialOnComputes</span>
<span class="kp">#SBATCH --time=0:10:0</span>
<span class="kp">#SBATCH --nodes=2</span>
<span class="kp">#SBATCH --ntasks-per-node=128</span>
<span class="kp">#SBATCH --cpus-per-task=1</span>

<span class="c1"># Replace [budget code] below with your budget code (e.g. t01)</span>
<span class="kp">#SBATCH --account=[budget code]  </span>
<span class="kp">#SBATCH --partition=standard</span>
<span class="kp">#SBATCH --qos=standard</span>

<span class="c1"># Make xthi available</span>
module<span class="w"> </span>load<span class="w"> </span>xthi

<span class="c1"># Set the number of threads to 1</span>
<span class="c1">#   This prevents any threaded system libraries from automatically </span>
<span class="c1">#   using threading.</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="m">1</span>

<span class="c1"># Propagate the cpus-per-task setting from script to srun commands</span>
<span class="c1">#    By default, Slurm does not propagate this setting from the sbatch</span>
<span class="c1">#    options to srun commands in the job script. If this is not done,</span>
<span class="c1">#    process/thread pinning may be incorrect leading to poor performance</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">SRUN_CPUS_PER_TASK</span><span class="o">=</span><span class="nv">$SLURM_CPUS_PER_TASK</span>

<span class="c1"># Get a list of the nodes assigned to this job in a format we can use.</span>
<span class="c1">#   scontrol converts the condensed node IDs in the sbatch environment</span>
<span class="c1">#   variable into a list of full node IDs that we can use with srun to</span>
<span class="c1">#   ensure the subjobs are placed on the correct node. e.g. this converts</span>
<span class="c1">#   &quot;nid[001234,002345]&quot; to &quot;nid001234 nid002345&quot;</span>
<span class="nv">nodelist</span><span class="o">=</span><span class="k">$(</span>scontrol<span class="w"> </span>show<span class="w"> </span>hostnames<span class="w"> </span><span class="nv">$SLURM_JOB_NODELIST</span><span class="k">)</span>

<span class="c1"># Loop over the nodes assigned to the job</span>
<span class="k">for</span><span class="w"> </span>nodeid<span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nv">$nodelist</span>
<span class="k">do</span>
<span class="w">    </span><span class="c1"># Loop over 128 subjobs on each node pinning each to a different core</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span>i<span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="k">$(</span>seq<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">128</span><span class="k">)</span>
<span class="w">    </span><span class="k">do</span>
<span class="w">        </span><span class="c1"># Launch subjob overriding job settings as required and in the background</span>
<span class="w">        </span><span class="c1"># Make sure to change the amount specified by the `--mem=` flag to the amount </span>
<span class="w">        </span><span class="c1"># of memory required. The amount of memory is given in MiB by default but other</span>
<span class="w">        </span><span class="c1"># units can be specified. If you do not know how much memory to specify, we </span>
<span class="w">        </span><span class="c1"># recommend that you specify `--mem=1500M` (1,500 MiB).</span>
<span class="w">        </span><span class="nb">srun</span><span class="w"> </span>--nodelist<span class="o">=</span><span class="si">${</span><span class="nv">nodeid</span><span class="si">}</span><span class="w"> </span>--nodes<span class="o">=</span><span class="m">1</span><span class="w"> </span>--ntasks<span class="o">=</span><span class="m">1</span><span class="w"> </span>--ntasks-per-node<span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--exact<span class="w"> </span>--mem<span class="o">=</span>1500M<span class="w"> </span>xthi<span class="w"> </span>&gt;<span class="w"> </span>placement_<span class="si">${</span><span class="nv">nodeid</span><span class="si">}</span>_<span class="si">${</span><span class="nv">i</span><span class="si">}</span>.txt<span class="w"> </span><span class="p">&amp;</span>
<span class="w">    </span><span class="k">done</span>
<span class="k">done</span>

<span class="c1"># Wait for all subjobs to finish</span>
<span class="nb">wait</span>
</code></pre></div>
<h2 id="process-placement">Process placement</h2>
<p>There are many occasions where you may want to control (usually, MPI) process placement and change
it from the default, for example:</p>
<ul>
<li>You may want to place processes to different NUMA regions in a round-robin way rather than
   the default sequential placement</li>
<li>You may be using fewer than 128 processes per node and want to ensure that processes
   are placed evenly across NUMA regions (16-core blocks) or core complexes (4-core blocks that
   share an L3 cache)</li>
</ul>
<p>There are a number of different methods for defining process placement, below we cover two different
options: using Slurm options and using the <code>MPICH_RANK_REORDER_METHOD</code> environment variable. Most users will
likely use the Slurm options approach.</p>
<h3 id="standard-process-placement">Standard process placement</h3>
<p>The standard approach recommended on ARCHER2 is to place processes sequentially on nodes until the
maximum number of tasks is reached. You can use the <code>xthi</code> program
to verify this for MPI process placement:</p>
<div class="highlight"><pre><span></span><code>auser@ln04:/work/t01/t01/auser&gt; salloc --nodes=2 --ntasks-per-node=128 \
     --cpus-per-task=1 --time=0:10:0 --partition=standard --qos=short \
     --account=[your account]

salloc: Pending job allocation 1170365
salloc: job 1170365 queued and waiting for resources
salloc: job 1170365 has been allocated resources
salloc: Granted job allocation 1170365
salloc: Waiting for resource configuration
salloc: Nodes nid[002526-002527] are ready for job

auser@ln04:/work/t01/t01/auser&gt; module load xthi
auser@ln04:/work/t01/t01/auser&gt; export OMP_NUM_THREADS=1
auser@ln04:/work/t01/t01/auser&gt; export SRUN_CPUS_PER_TASK=$SLURM_CPUS_PER_TASK
auser@ln04:/work/t01/t01/auser&gt; srun --distribution=block:block --hint=nomultithread xthi

Node summary for    2 nodes:
Node    0, hostname nid002526, mpi 128, omp   1, executable xthi
Node    1, hostname nid002527, mpi 128, omp   1, executable xthi
MPI summary: 256 ranks 
Node    0, rank    0, thread   0, (affinity =    0) 
Node    0, rank    1, thread   0, (affinity =    1) 
Node    0, rank    2, thread   0, (affinity =    2) 
Node    0, rank    3, thread   0, (affinity =    3) 

...output trimmed...

Node    0, rank  124, thread   0, (affinity =  124) 
Node    0, rank  125, thread   0, (affinity =  125) 
Node    0, rank  126, thread   0, (affinity =  126) 
Node    0, rank  127, thread   0, (affinity =  127) 
Node    1, rank  128, thread   0, (affinity =    0) 
Node    1, rank  129, thread   0, (affinity =    1) 
Node    1, rank  130, thread   0, (affinity =    2) 
Node    1, rank  131, thread   0, (affinity =    3) 

...output trimmed...
</code></pre></div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For MPI programs on ARCHER2, each <em>rank</em> corresponds to a <em>process</em>.</p>
</div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>To get good performance out of MPI collective operations, MPI processes
should be placed sequentially on cores as in the standard placement described
above.</p>
</div>
<h3 id="setting-process-placement-using-slurm-options">Setting process placement using Slurm options</h3>
<h4 id="for-underpopulation-of-nodes-with-processes">For underpopulation of nodes with processes</h4>
<p>When you are using fewer processes than cores on compute nodes (i.e. &lt; 128 processes
per node) the basic Slurm options (usually supplied in your script as options to <code>sbatch</code>) for 
process placement are:</p>
<ul>
<li><code>--ntasks-per-node=X</code> Place <em>X</em> processes on each node</li>
<li><code>--cpus-per-task=Y</code> Set a stride of <em>Y</em> cores between each placed process. If you specify this 
  option in a job submission script (queued using <code>sbatch</code>) or via <code>salloc</code> they you will also need
  to set <code>export SRUN_CPUS_PER_TASK=$SLURM_CPUS_PER_TASK</code> to ensure the setting is passed to <code>srun</code>
  commands in the script or allocation.</li>
</ul>
<p>In addition, the following options are added to your <code>srun</code> commands in your job
submission script:</p>
<ul>
<li><code>--hint=nomultithread</code> Only use physical cores (avoids use of SMT/hyperthreads)</li>
<li><code>--distribution=block:block</code> Allocate processes to cores in a sequential fashion</li>
</ul>
<p>For example, to place 32 processes per node and have 1 process per 4-core block (corresponding
to a CCX, <em>Core CompleX</em>, that shares an L3 cache), you would set:</p>
<ul>
<li><code>--ntasks-per-node=32</code> Place 32 processes on each node</li>
<li><code>--cpus-per-task=4</code> Set a stride of 4 cores between each placed process</li>
</ul>
<p>Here is the output from <code>xthi</code>:</p>
<div class="highlight"><pre><span></span><code>auser@ln04:/work/t01/t01/auser&gt; salloc --nodes=2 --ntasks-per-node=32 \
     --cpus-per-task=4 --time=0:10:0 --partition=standard --qos=short \
     --account=[your account]

salloc: Pending job allocation 1170383
salloc: job 1170383 queued and waiting for resources
salloc: job 1170383 has been allocated resources
salloc: Granted job allocation 1170383
salloc: Waiting for resource configuration
salloc: Nodes nid[002526-002527] are ready for job

auser@ln04:/work/t01/t01/auser&gt; module load xthi
auser@ln04:/work/t01/t01/auser&gt; export OMP_NUM_THREADS=1
auser@ln04:/work/t01/t01/auser&gt; export SRUN_CPUS_PER_TASK=$SLURM_CPUS_PER_TASK
auser@ln04:/work/t01/t01/auser&gt; srun --distribution=block:block --hint=nomultithread xthi

Node summary for    2 nodes:
Node    0, hostname nid002526, mpi  32, omp   1, executable xthi
Node    1, hostname nid002527, mpi  32, omp   1, executable xthi
MPI summary: 64 ranks 
Node    0, rank    0, thread   0, (affinity =  0-3) 
Node    0, rank    1, thread   0, (affinity =  4-7) 
Node    0, rank    2, thread   0, (affinity = 8-11) 
Node    0, rank    3, thread   0, (affinity = 12-15) 
Node    0, rank    4, thread   0, (affinity = 16-19) 
Node    0, rank    5, thread   0, (affinity = 20-23) 
Node    0, rank    6, thread   0, (affinity = 24-27) 
Node    0, rank    7, thread   0, (affinity = 28-31) 
Node    0, rank    8, thread   0, (affinity = 32-35) 
Node    0, rank    9, thread   0, (affinity = 36-39) 
Node    0, rank   10, thread   0, (affinity = 40-43) 
Node    0, rank   11, thread   0, (affinity = 44-47) 
Node    0, rank   12, thread   0, (affinity = 48-51) 
Node    0, rank   13, thread   0, (affinity = 52-55) 
Node    0, rank   14, thread   0, (affinity = 56-59) 
Node    0, rank   15, thread   0, (affinity = 60-63) 
Node    0, rank   16, thread   0, (affinity = 64-67) 
Node    0, rank   17, thread   0, (affinity = 68-71) 
Node    0, rank   18, thread   0, (affinity = 72-75) 
Node    0, rank   19, thread   0, (affinity = 76-79) 
Node    0, rank   20, thread   0, (affinity = 80-83) 
Node    0, rank   21, thread   0, (affinity = 84-87) 
Node    0, rank   22, thread   0, (affinity = 88-91) 
Node    0, rank   23, thread   0, (affinity = 92-95) 
Node    0, rank   24, thread   0, (affinity = 96-99) 
Node    0, rank   25, thread   0, (affinity = 100-103) 
Node    0, rank   26, thread   0, (affinity = 104-107) 
Node    0, rank   27, thread   0, (affinity = 108-111) 
Node    0, rank   28, thread   0, (affinity = 112-115) 
Node    0, rank   29, thread   0, (affinity = 116-119) 
Node    0, rank   30, thread   0, (affinity = 120-123) 
Node    0, rank   31, thread   0, (affinity = 124-127) 
Node    1, rank   32, thread   0, (affinity =  0-3) 
Node    1, rank   33, thread   0, (affinity =  4-7) 
Node    1, rank   34, thread   0, (affinity = 8-11) 
Node    1, rank   35, thread   0, (affinity = 12-15) 
Node    1, rank   36, thread   0, (affinity = 16-19) 
Node    1, rank   37, thread   0, (affinity = 20-23) 
Node    1, rank   38, thread   0, (affinity = 24-27) 
Node    1, rank   39, thread   0, (affinity = 28-31) 
Node    1, rank   40, thread   0, (affinity = 32-35) 
Node    1, rank   41, thread   0, (affinity = 36-39) 
Node    1, rank   42, thread   0, (affinity = 40-43) 
Node    1, rank   43, thread   0, (affinity = 44-47) 
Node    1, rank   44, thread   0, (affinity = 48-51) 
Node    1, rank   45, thread   0, (affinity = 52-55) 
Node    1, rank   46, thread   0, (affinity = 56-59) 
Node    1, rank   47, thread   0, (affinity = 60-63) 
Node    1, rank   48, thread   0, (affinity = 64-67) 
Node    1, rank   49, thread   0, (affinity = 68-71) 
Node    1, rank   50, thread   0, (affinity = 72-75) 
Node    1, rank   51, thread   0, (affinity = 76-79) 
Node    1, rank   52, thread   0, (affinity = 80-83) 
Node    1, rank   53, thread   0, (affinity = 84-87) 
Node    1, rank   54, thread   0, (affinity = 88-91) 
Node    1, rank   55, thread   0, (affinity = 92-95) 
Node    1, rank   56, thread   0, (affinity = 96-99) 
Node    1, rank   57, thread   0, (affinity = 100-103) 
Node    1, rank   58, thread   0, (affinity = 104-107) 
Node    1, rank   59, thread   0, (affinity = 108-111) 
Node    1, rank   60, thread   0, (affinity = 112-115) 
Node    1, rank   61, thread   0, (affinity = 116-119) 
Node    1, rank   62, thread   0, (affinity = 120-123) 
Node    1, rank   63, thread   0, (affinity = 124-127) 
</code></pre></div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>You usually only want to use physical cores on ARCHER2, so
(<code>ntasks-per-node</code>) &times; (<code>cpus-per-task</code>) should generally be equal to 128.</p>
</div>
<h4 id="full-node-population-with-non-sequential-process-placement">Full node population with non-sequential process placement</h4>
<p>If you want to change the order processes are placed on nodes and cores using Slurm options
then you should use the <code>--distribution</code> option to <code>srun</code> to change this.</p>
<p>For example, to place processes sequentially on nodes but round-robin on the 16-core NUMA regions in a
single node, you would use the <code>--distribution=block:cyclic</code> option to <code>srun</code>. This type
of process placement can be beneficial when a code is memory bound.</p>
<div class="highlight"><pre><span></span><code>auser@ln04:/work/t01/t01/auser&gt; salloc --nodes=2 --ntasks-per-node=128 \
     --cpus-per-task=1 --time=0:10:0 --partition=standard --qos=short \
     --account=[your account]

salloc: Pending job allocation 1170594
salloc: job 1170594 queued and waiting for resources
salloc: job 1170594 has been allocated resources
salloc: Granted job allocation 1170594
salloc: Waiting for resource configuration
salloc: Nodes nid[002616,002621] are ready for job

auser@ln04:/work/t01/t01/auser&gt; module load xthi
auser@ln04:/work/t01/t01/auser&gt; export OMP_NUM_THREADS=1
auser@ln04:/work/t01/t01/auser&gt; srun --distribution=block:cyclic --hint=nomultithread xthi

Node summary for    2 nodes:
Node    0, hostname nid002616, mpi 128, omp   1, executable xthi
Node    1, hostname nid002621, mpi 128, omp   1, executable xthi
MPI summary: 256 ranks 
Node    0, rank    0, thread   0, (affinity =    0) 
Node    0, rank    1, thread   0, (affinity =   16) 
Node    0, rank    2, thread   0, (affinity =   32) 
Node    0, rank    3, thread   0, (affinity =   48) 
Node    0, rank    4, thread   0, (affinity =   64) 
Node    0, rank    5, thread   0, (affinity =   80) 
Node    0, rank    6, thread   0, (affinity =   96) 
Node    0, rank    7, thread   0, (affinity =  112) 
Node    0, rank    8, thread   0, (affinity =    1) 
Node    0, rank    9, thread   0, (affinity =   17) 
Node    0, rank   10, thread   0, (affinity =   33) 
Node    0, rank   11, thread   0, (affinity =   49) 
Node    0, rank   12, thread   0, (affinity =   65) 
Node    0, rank   13, thread   0, (affinity =   81) 
Node    0, rank   14, thread   0, (affinity =   97) 
Node    0, rank   15, thread   0, (affinity =  113

...output trimmed...

Node    0, rank  120, thread   0, (affinity =   15) 
Node    0, rank  121, thread   0, (affinity =   31) 
Node    0, rank  122, thread   0, (affinity =   47) 
Node    0, rank  123, thread   0, (affinity =   63) 
Node    0, rank  124, thread   0, (affinity =   79) 
Node    0, rank  125, thread   0, (affinity =   95) 
Node    0, rank  126, thread   0, (affinity =  111) 
Node    0, rank  127, thread   0, (affinity =  127) 
Node    1, rank  128, thread   0, (affinity =    0) 
Node    1, rank  129, thread   0, (affinity =   16) 
Node    1, rank  130, thread   0, (affinity =   32) 
Node    1, rank  131, thread   0, (affinity =   48) 
Node    1, rank  132, thread   0, (affinity =   64) 
Node    1, rank  133, thread   0, (affinity =   80) 
Node    1, rank  134, thread   0, (affinity =   96) 
Node    1, rank  135, thread   0, (affinity =  112) 

...output trimmed...
</code></pre></div>
<p>If you wish to place processes round robin on <em>both</em> nodes and 16-core regions
(cores that share access to a DRAM single memory controller) within in a node
you would use <code>--distribution=cyclic:cyclic</code>:</p>
<div class="highlight"><pre><span></span><code>auser@ln04:/work/t01/t01/auser&gt; salloc --nodes=2 --ntasks-per-node=128 \
     --cpus-per-task=1 --time=0:10:0 --partition=standard --qos=short \
     --account=[your account]

salloc: Pending job allocation 1170594
salloc: job 1170594 queued and waiting for resources
salloc: job 1170594 has been allocated resources
salloc: Granted job allocation 1170594
salloc: Waiting for resource configuration
salloc: Nodes nid[002616,002621] are ready for job

auser@ln04:/work/t01/t01/auser&gt; module load xthi
auser@ln04:/work/t01/t01/auser&gt; export OMP_NUM_THREADS=1
auser@ln04:/work/t01/t01/auser&gt; export SRUN_CPUS_PER_TASK=$SLURM_CPUS_PER_TASK
auser@ln04:/work/t01/t01/auser&gt; srun --distribution=cyclic:cyclic --hint=nomultithread xthi

Node summary for    2 nodes:
Node    0, hostname nid002616, mpi 128, omp   1, executable xthi
Node    1, hostname nid002621, mpi 128, omp   1, executable xthi
MPI summary: 256 ranks 
Node    0, rank    0, thread   0, (affinity =    0) 
Node    0, rank    2, thread   0, (affinity =   16) 
Node    0, rank    4, thread   0, (affinity =   32) 
Node    0, rank    6, thread   0, (affinity =   48) 
Node    0, rank    8, thread   0, (affinity =   64) 
Node    0, rank   10, thread   0, (affinity =   80) 
Node    0, rank   12, thread   0, (affinity =   96) 
Node    0, rank   14, thread   0, (affinity =  112) 
Node    0, rank   16, thread   0, (affinity =    1) 
Node    0, rank   18, thread   0, (affinity =   17) 
Node    0, rank   20, thread   0, (affinity =   33) 
Node    0, rank   22, thread   0, (affinity =   49) 
Node    0, rank   24, thread   0, (affinity =   65) 
Node    0, rank   26, thread   0, (affinity =   81) 
Node    0, rank   28, thread   0, (affinity =   97) 
Node    0, rank   30, thread   0, (affinity =  113) 

...output trimmed...

Node    1, rank    1, thread   0, (affinity =    0) 
Node    1, rank    3, thread   0, (affinity =   16) 
Node    1, rank    5, thread   0, (affinity =   32) 
Node    1, rank    7, thread   0, (affinity =   48) 
Node    1, rank    9, thread   0, (affinity =   64) 
Node    1, rank   11, thread   0, (affinity =   80) 
Node    1, rank   13, thread   0, (affinity =   96) 
Node    1, rank   15, thread   0, (affinity =  112) 
Node    1, rank   17, thread   0, (affinity =    1) 
Node    1, rank   19, thread   0, (affinity =   17) 
Node    1, rank   21, thread   0, (affinity =   33) 
Node    1, rank   23, thread   0, (affinity =   49) 
Node    1, rank   25, thread   0, (affinity =   65) 
Node    1, rank   27, thread   0, (affinity =   81) 
Node    1, rank   29, thread   0, (affinity =   97) 
Node    1, rank   31, thread   0, (affinity =  113) 

...output trimmed...
</code></pre></div>
<p>Remember, MPI collective performance is generally much worse if processes are not placed
sequentially on a node (so adjacent MPI ranks are as close to each other as possible). This
is the reason that the default recommended placement on ARCHER2 is sequential rather than 
round-robin.</p>
<h3 id="mpich_rank_reorder_method-for-mpi-process-placement"><code>MPICH_RANK_REORDER_METHOD</code> for MPI process placement</h3>
<!-- TODO Need to check this -->

<p>The <code>MPICH_RANK_REORDER_METHOD</code> environment variable can also be used to specify
other types of MPI task placement. For example, setting it to "0" results
in a round-robin placement on both nodes and NUMA regions in a node (equivalent to
the <code>--distribution=cyclic:cyclic</code> option to <code>srun</code>). Note, we do not specify the <code>--distribution</code>
option to <code>srun</code> in this case as the environment variable is controlling placement:</p>
<div class="highlight"><pre><span></span><code>salloc --nodes=8 --ntasks-per-node=2 --cpus-per-task=1 --time=0:10:0 --account=t01

salloc: Granted job allocation 24236
salloc: Waiting for resource configuration
salloc: Nodes cn13 are ready for job

module load xthi
export OMP_NUM_THREADS=1
export MPICH_RANK_REORDER_METHOD=0
srun --hint=nomultithread xthi

Node summary for    2 nodes:
Node    0, hostname nid002616, mpi 128, omp   1, executable xthi
Node    1, hostname nid002621, mpi 128, omp   1, executable xthi
MPI summary: 256 ranks 
Node    0, rank    0, thread   0, (affinity =    0) 
Node    0, rank    2, thread   0, (affinity =   16) 
Node    0, rank    4, thread   0, (affinity =   32) 
Node    0, rank    6, thread   0, (affinity =   48) 
Node    0, rank    8, thread   0, (affinity =   64) 
Node    0, rank   10, thread   0, (affinity =   80) 
Node    0, rank   12, thread   0, (affinity =   96) 
Node    0, rank   14, thread   0, (affinity =  112) 
Node    0, rank   16, thread   0, (affinity =    1) 
Node    0, rank   18, thread   0, (affinity =   17) 
Node    0, rank   20, thread   0, (affinity =   33) 
Node    0, rank   22, thread   0, (affinity =   49) 
Node    0, rank   24, thread   0, (affinity =   65) 
Node    0, rank   26, thread   0, (affinity =   81) 
Node    0, rank   28, thread   0, (affinity =   97) 
Node    0, rank   30, thread   0, (affinity =  113) 

...output trimmed...
</code></pre></div>
<p>There are other modes available with the <code>MPICH_RANK_REORDER_METHOD</code>
environment variable, including one which lets the user provide a file
called <code>MPICH_RANK_ORDER</code> which contains a list of each task's placement
on each node. These options are described in detail in the <code>intro_mpi</code>
man page.</p>
<h4 id="grid_order">grid_order</h4>
<p>For MPI applications which perform a large amount of nearest-neighbor
communication, e.g., stencil-based applications on structured grids,
HPE provide a tool in the <code>perftools-base</code> module (Loaded by default for
all users) called <code>grid_order</code>
which can generate a <code>MPICH_RANK_ORDER</code> file automatically by taking as
parameters the dimensions of the grid, core count, etc. For example, to
place 256 MPI parameters in row-major order on a Cartesian grid of size $(8, 8,
4)$, using 128 cores per node:</p>
<div class="highlight"><pre><span></span><code>grid_order -R -c 128 -g 8,8,4

# grid_order -R -Z -c 128 -g 8,8,4
# Region 3: 0,0,1 (0..255)
0,1,2,3,32,33,34,35,64,65,66,67,96,97,98,99,128,129,130,131,160,161,162,163,192,193,194,195,224,225,226,227,4,5,6,7,36,37,38,39,68,69,70,71,100,101,102,103,132,133,134,135,164,165,166,167,196,197,198,199,228,229,230,231,8,9,10,11,40,41,42,43,72,73,74,75,104,105,106,107,136,137,138,139,168,169,170,171,200,201,202,203,232,233,234,235,12,13,14,15,44,45,46,47,76,77,78,79,108,109,110,111,140,141,142,143,172,173,174,175,204,205,206,207,236,237,238,239
16,17,18,19,48,49,50,51,80,81,82,83,112,113,114,115,144,145,146,147,176,177,178,179,208,209,210,211,240,241,242,243,20,21,22,23,52,53,54,55,84,85,86,87,116,117,118,119,148,149,150,151,180,181,182,183,212,213,214,215,244,245,246,247,24,25,26,27,56,57,58,59,88,89,90,91,120,121,122,123,152,153,154,155,184,185,186,187,216,217,218,219,248,249,250,251,28,29,30,31,60,61,62,63,92,93,94,95,124,125,126,127,156,157,158,159,188,189,190,191,220,221,222,223,252,253,254,255
</code></pre></div>
<p>One can then save this output to a file called <code>MPICH_RANK_ORDER</code> and
then set <code>MPICH_RANK_REORDER_METHOD=3</code> before running the job, which
tells Cray MPI to read the <code>MPICH_RANK_ORDER</code> file to set the MPI task
placement. For more information, please see the man page <code>man grid_order</code>.</p>
<h2 id="interactive-jobs">Interactive Jobs</h2>
<h3 id="using-salloc-to-reserve-resources">Using <code>salloc</code> to reserve resources</h3>
<p>When you are developing or debugging code you often want to run many
short jobs with a small amount of editing the code between runs. This
can be achieved by using the login nodes to run MPI but you may want to
test on the compute nodes (e.g. you may want to test running on multiple
nodes across the high performance interconnect). One of the best ways to
achieve this on ARCHER2 is to use interactive jobs.</p>
<p>An interactive job allows you to issue <code>srun</code> commands directly from the
command line without using a job submission script, and to see the
output from your program directly in the terminal.</p>
<p>You use the <code>salloc</code> command to reserve compute nodes for interactive
jobs.</p>
<p>To submit a request for an interactive job reserving 8 nodes (1024
physical cores) for 20 minutes on the short QoS you would issue the
following command from the command line:</p>
<div class="highlight"><pre><span></span><code>auser@ln01:&gt;<span class="w"> </span>salloc<span class="w"> </span>--nodes<span class="o">=</span><span class="m">8</span><span class="w"> </span>--ntasks-per-node<span class="o">=</span><span class="m">128</span><span class="w"> </span>--cpus-per-task<span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">                </span>--time<span class="o">=</span><span class="m">00</span>:20:00<span class="w"> </span>--partition<span class="o">=</span>standard<span class="w"> </span>--qos<span class="o">=</span>short<span class="w"> </span><span class="se">\</span>
<span class="w">                </span>--account<span class="o">=[</span>budget<span class="w"> </span>code<span class="o">]</span>
</code></pre></div>
<p>When you submit this job your terminal will display something like:</p>
<div class="highlight"><pre><span></span><code>salloc:<span class="w"> </span>Granted<span class="w"> </span>job<span class="w"> </span>allocation<span class="w"> </span><span class="m">24236</span>
salloc:<span class="w"> </span>Waiting<span class="w"> </span><span class="k">for</span><span class="w"> </span>resource<span class="w"> </span>configuration
salloc:<span class="w"> </span>Nodes<span class="w"> </span>nid000002<span class="w"> </span>are<span class="w"> </span>ready<span class="w"> </span><span class="k">for</span><span class="w"> </span>job
auser@ln01:&gt;
</code></pre></div>
<p>It may take some time for your interactive job to start. Once it runs
you will enter a standard interactive terminal session (a new shell).
Note that this shell is still on the front end (the prompt has not
change). Whilst the interactive session lasts you will be able to run
parallel jobs on the compute nodes by issuing the <code>srun
--distribution=block:block --hint=nomultithread</code> command directly at 
your command prompt using the same syntax as you would inside a job
script. The maximum number of nodes you can use is limited by resources
requested in the <code>salloc</code> command.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>If you wish the <code>cpus-per-task</code> option to <code>salloc</code> to propagate to <code>srun</code>
commands in the allocation, you will need to use the command 
<code>export SRUN_CPUS_PER_TASK=$SLURM_CPUS_PER_TASK</code> before you issue any
<code>srun</code> commands.</p>
</div>
<p>If you know you will be doing a lot of intensive debugging you may find
it useful to request an interactive session lasting the expected length
of your working session, say a full day.</p>
<p>Your session will end when you hit the requested walltime. If you wish
to finish before this you should use the <code>exit</code> command - this will
return you to your prompt before you issued the <code>salloc</code> command.</p>
<h3 id="using-srun-directly">Using <code>srun</code> directly</h3>
<p>A second way to run an interactive job is to use <code>srun</code> directly in the
following way (here using the <code>short</code> QoS):</p>
<div class="highlight"><pre><span></span><code>auser@ln01:/work/t01/t01/auser&gt; srun --nodes=1 --exclusive --time=00:20:00 \
                --partition=standard --qos=short --account=[budget code] \
    --pty /bin/bash
auser@nid001261:/work/t01/t01/auser&gt; hostname
nid001261
</code></pre></div>
<p>The <code>--pty /bin/bash</code> will cause a new shell to be started on the first
node of a new allocation . This is perhaps closer to what
many people consider an 'interactive' job than the method using <code>salloc</code>
appears.</p>
<p>One can now issue shell commands in the usual way. A further invocation
of <code>srun</code> is required to launch a parallel job in the allocation.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When using <code>srun</code> within an interactive <code>srun</code> session, you will need to 
include the <code>--oversubscribe</code> flag and specify the number of cores you want 
to use:
<div class="highlight"><pre><span></span><code>auser@nid001261:/work/t01/t01/auser&gt; srun --oversubscribe --distribution=block:block \
                --hint=nomultithread --ntasks=128 ./my_mpi_executable.x
</code></pre></div></p>
</div>
<p>When finished, type <code>exit</code> to relinquish the allocation and control will
be returned to the front end.</p>
<p>By default, the interactive shell will retain the environment of the
parent. If you want a clean shell, remember to specify <code>--export=none</code>.</p>
<h2 id="heterogeneous-jobs">Heterogeneous jobs</h2>
<p>Most of the Slurm submissions discussed above involve running a single
executable. However, there are situations where two or more distinct
executables are coupled and need to be run at the same time, potentially
using the same MPI communicator. This is most easily handled via the
Slurm heterogeneous job mechanism.</p>
<p>Two common cases are discussed below: first, a client server model in
which client and server each have a different <code>MPI_COMM_WORLD</code>, and second
the case were two or more executables share <code>MPI_COMM_WORLD</code>.</p>
<h3 id="heterogeneous-jobs-for-a-clientserver-model-distinct-mpi_comm_worlds">Heterogeneous jobs for a client/server model: distinct <code>MPI_COMM_WORLDs</code></h3>
<p>The essential feature of a heterogeneous job here is to create a single batch
submission which specifies the resource requirements for the individual
components. Schematically, we would use</p>
<p><div class="highlight"><pre><span></span><code><span class="ch">#!/bin/bash</span>

<span class="c1"># Slurm specifications for the first component</span>

<span class="kp">#SBATCH --partition=standard</span>

...

<span class="kp">#SBATCH hetjob</span>

<span class="c1"># Slurm specifications for the second component</span>

<span class="kp">#SBATCH --partition=standard</span>

...
</code></pre></div>
where new each component beyond the first is introduced by the special
token <code>#SBATCH hetjob</code> (note this is not a normal option and is not
<code>--hetjob</code>). Each component must specify a partition.</p>
<p>Such a job will appear in the scheduler as, e.g.,
<div class="highlight"><pre><span></span><code>           50098+0  standard qscript-    user  PD       0:00      1 (None) 
           50098+1  standard qscript-    user  PD       0:00      2 (None) 
</code></pre></div>
and counts as (in this case) two separate jobs from the point of
QoS limits.</p>
<p>Consider a case where we have two executables which may both be parallel (in
that they use MPI), both run at the same time, and communicate with each
other using MPI or by some other means. In the following example, we run two
different executables, <code>xthi-a</code> and <code>xthi-b</code>, both of which must finish
before the jobs completes.</p>
<p><div class="highlight"><pre><span></span><code><span class="ch">#!/bin/bash</span>

<span class="kp">#SBATCH --time=00:20:00</span>
<span class="kp">#SBATCH --exclusive</span>
<span class="kp">#SBATCH --export=none</span>

<span class="kp">#SBATCH --partition=standard</span>
<span class="kp">#SBATCH --qos=standard</span>

<span class="kp">#SBATCH --nodes=1</span>
<span class="kp">#SBATCH --ntasks-per-node=8</span>

<span class="kp">#SBATCH hetjob</span>

<span class="kp">#SBATCH --partition=standard</span>
<span class="kp">#SBATCH --qos=standard</span>

<span class="kp">#SBATCH --nodes=2</span>
<span class="kp">#SBATCH --ntasks-per-node=4</span>

<span class="c1"># Run two executables with separate MPI_COMM_WORLD</span>

<span class="nb">srun</span><span class="w"> </span>--distribution<span class="o">=</span>block:block<span class="w"> </span>--hint<span class="o">=</span>nomultithread<span class="w"> </span>--het-group<span class="o">=</span><span class="m">0</span><span class="w"> </span>./xthi-a<span class="w"> </span><span class="p">&amp;</span>
<span class="nb">srun</span><span class="w"> </span>--distribution<span class="o">=</span>block:block<span class="w"> </span>--hint<span class="o">=</span>nomultithread<span class="w"> </span>--het-group<span class="o">=</span><span class="m">1</span><span class="w"> </span>./xthi-b<span class="w"> </span><span class="p">&amp;</span>
<span class="nb">wait</span>
</code></pre></div>
In this case, each executable is launched with a separate call to
<code>srun</code> but specifies a different heterogeneous group via the
<code>--het-group</code> option. The first group is <code>--het-group=0</code>.
Both are run in the background with <code>&amp;</code> and the <code>wait</code> is required
to ensure both executables have completed before the job submission
exits.</p>
<p>The above is a rather artificial example using two executables which
are in fact just symbolic links in the job directory to <code>xthi</code>,
used without loading the module. You can test this script yourself
by creating symbolic links to the original executable before
submitting the job:</p>
<div class="highlight"><pre><span></span><code>auser@ln04:/work/t01/t01/auser/job-dir&gt;<span class="w"> </span>module<span class="w"> </span>load<span class="w"> </span>xthi
auser@ln04:/work/t01/t01/auser/job-dir&gt;<span class="w"> </span>which<span class="w"> </span>xthi
/work/y07/shared/utils/core/xthi/1.2/CRAYCLANG/11.0/bin/xthi
auser@ln04:/work/t01/t01/auser/job-dir&gt;<span class="w"> </span>ln<span class="w"> </span>-s<span class="w"> </span>/work/y07/shared/utils/core/xthi/1.2/CRAYCLANG/11.0/bin/xthi<span class="w"> </span>xthi-a
auser@ln04:/work/t01/t01/auser/job-dir&gt;<span class="w"> </span>ln<span class="w"> </span>-s<span class="w"> </span>/work/y07/shared/utils/core/xthi/1.2/CRAYCLANG/11.0/bin/xthi<span class="w"> </span>xthi-b
</code></pre></div>
<p>The example job will produce two reports showing the placement of the
MPI tasks from the two instances of <code>xthi</code> running in each of the
heterogeneous groups. For example, the output might be</p>
<p><div class="highlight"><pre><span></span><code>Node summary for    1 nodes:
Node    0, hostname nid002400, mpi   8, omp   1, executable xthi-a
MPI summary: 8 ranks
Node    0, rank    0, thread   0, (affinity =    0)
Node    0, rank    1, thread   0, (affinity =    1)
Node    0, rank    2, thread   0, (affinity =    2)
Node    0, rank    3, thread   0, (affinity =    3)
Node    0, rank    4, thread   0, (affinity =    4)
Node    0, rank    5, thread   0, (affinity =    5)
Node    0, rank    6, thread   0, (affinity =    6)
Node    0, rank    7, thread   0, (affinity =    7)
Node summary for    2 nodes:
Node    0, hostname nid002146, mpi   4, omp   1, executable xthi-b
Node    1, hostname nid002149, mpi   4, omp   1, executable xthi-b
MPI summary: 8 ranks
Node    0, rank    0, thread   0, (affinity =    0)
Node    0, rank    1, thread   0, (affinity =    1)
Node    0, rank    2, thread   0, (affinity =    2)
Node    0, rank    3, thread   0, (affinity =    3)
Node    1, rank    4, thread   0, (affinity =    0)
Node    1, rank    5, thread   0, (affinity =    1)
Node    1, rank    6, thread   0, (affinity =    2)
Node    1, rank    7, thread   0, (affinity =    3)
</code></pre></div>
Here we have the first executable running on one node with
a communicator size 8 (ranks 0-7). The second executable runs on
two nodes also with communicator size 8 (ranks 0-7, 4 ranks per node).
Further examples of placement for heterogenenous jobs are given below.</p>
<p>Finally, if your workflow requires the different heterogeneous jobs to
communicate via MPI, but without sharing their <code>MPI_COM_WORLD</code>, you will need
to export two new variables before your <code>srun</code> commands as defined below:</p>
<div class="highlight"><pre><span></span><code><span class="nb">export</span><span class="w"> </span><span class="nv">PMI_UNIVERSE_SIZE</span><span class="o">=</span><span class="m">3</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MPICH_SINGLE_HOST_ENABLED</span><span class="o">=</span><span class="m">0</span>
</code></pre></div>
<h3 id="heterogeneous-jobs-for-a-shared-mpi_com_world">Heterogeneous jobs for a shared <code>MPI_COM_WORLD</code></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The directive <code>SBATCH hetjob</code> can no longer be used for jobs requiring
a shared <code>MPI_COMM_WORLD</code></p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In this approach, each <code>hetjob</code> component must be on its own set of nodes.
You cannot use this approach to place different <code>hetjob</code> components on 
the same node.</p>
</div>
<p>If two or more heterogeneous components need to share a unique
<code>MPI_COMM_WORLD</code>, a single <code>srun</code> invocation with the differrent
components separated by a colon <code>:</code> should be used. Arguements
to the individual components of the <code>srun</code> control the placement of
the tasks and threads for each component. For example, running the
same <code>xthi-a</code> and <code>xthi-b</code> executables as above but now in a shared
communicator, we might run:</p>
<div class="highlight"><pre><span></span><code><span class="ch">#!/bin/bash</span>

<span class="kp">#SBATCH --time=00:20:00</span>
<span class="kp">#SBATCH --export=none</span>
<span class="kp">#SBATCH --account=[...]</span>

<span class="kp">#SBATCH --partition=standard</span>
<span class="kp">#SBATCH --qos=standard</span>

<span class="c1"># We must specify correctly the total number of nodes required.</span>
<span class="kp">#SBATCH --nodes=3</span>

<span class="nv">SHARED_ARGS</span><span class="o">=</span><span class="s2">&quot;--distribution=block:block --hint=nomultithread&quot;</span>

<span class="nb">srun</span><span class="w"> </span>--het-group<span class="o">=</span><span class="m">0</span><span class="w"> </span>--nodes<span class="o">=</span><span class="m">1</span><span class="w"> </span>--ntasks-per-node<span class="o">=</span><span class="m">8</span><span class="w"> </span><span class="si">${</span><span class="nv">SHARED_ARGS</span><span class="si">}</span><span class="w"> </span>./xthi-a<span class="w"> </span>:<span class="w"> </span><span class="se">\</span>
<span class="w"> </span>--het-group<span class="o">=</span><span class="m">1</span><span class="w"> </span>--nodes<span class="o">=</span><span class="m">2</span><span class="w"> </span>--ntasks-per-node<span class="o">=</span><span class="m">4</span><span class="w"> </span><span class="si">${</span><span class="nv">SHARED_ARGS</span><span class="si">}</span><span class="w"> </span>./xthi-b
</code></pre></div>
<p>The output should confirm we have a single <code>MPI_COMM_WORLD</code> with
a total of three nodes, <code>xthi-a</code> running on one and <code>xthi-b</code> on two,
with ranks 0-15 extending across both executables.</p>
<div class="highlight"><pre><span></span><code>Node summary for    3 nodes:
Node    0, hostname nid002668, mpi   8, omp   1, executable xthi-a
Node    1, hostname nid002669, mpi   4, omp   1, executable xthi-b
Node    2, hostname nid002670, mpi   4, omp   1, executable xthi-b
MPI summary: 16 ranks 
Node    0, rank    0, thread   0, (affinity =    0) 
Node    0, rank    1, thread   0, (affinity =    1) 
Node    0, rank    2, thread   0, (affinity =    2) 
Node    0, rank    3, thread   0, (affinity =    3) 
Node    0, rank    4, thread   0, (affinity =    4) 
Node    0, rank    5, thread   0, (affinity =    5) 
Node    0, rank    6, thread   0, (affinity =    6) 
Node    0, rank    7, thread   0, (affinity =    7) 
Node    1, rank    8, thread   0, (affinity =    0) 
Node    1, rank    9, thread   0, (affinity =    1) 
Node    1, rank   10, thread   0, (affinity =    2) 
Node    1, rank   11, thread   0, (affinity =    3) 
Node    2, rank   12, thread   0, (affinity =    0) 
Node    2, rank   13, thread   0, (affinity =    1) 
Node    2, rank   14, thread   0, (affinity =    2) 
Node    2, rank   15, thread   0, (affinity =    3) 
</code></pre></div>
<h3 id="heterogeneous-placement-for-mixed-mpiopenmp-work">Heterogeneous placement for mixed MPI/OpenMP work</h3>
<p>Some care may be required for placement of tasks/threads in heterogeneous
jobs in which the number of threads needs to be specified differently
for different components.</p>
<p>In the following we have two components, again using <code>xthi-a</code> and
<code>xthi-b</code> as our two separate executables. The first component runs
8 MPI tasks each with 16 OpenMP threads on one node. The second component
runs 8 MPI tasks with one task per NUMA region on a second node; each
task has one thread. An appropriate Slurm submission might be:</p>
<div class="highlight"><pre><span></span><code><span class="ch">#!/bin/bash</span>

<span class="kp">#SBATCH --time=00:20:00</span>
<span class="kp">#SBATCH --export=none</span>
<span class="kp">#SBATCH --account=[...]</span>

<span class="kp">#SBATCH --partition=standard</span>
<span class="kp">#SBATCH --qos=standard</span>

<span class="kp">#SBATCH --nodes=2</span>

<span class="nv">SHARED_ARGS</span><span class="o">=</span><span class="s2">&quot;--distribution=block:block --hint=nomultithread \</span>
<span class="s2">              --nodes=1 --ntasks-per-node=8 --cpus-per-task=16&quot;</span>

<span class="c1"># Do not set OMP_NUM_THREADS in the calling environment</span>

<span class="nb">unset</span><span class="w"> </span>OMP_NUM_THREADS
<span class="nb">export</span><span class="w"> </span><span class="nv">OMP_PROC_BIND</span><span class="o">=</span>spread

<span class="nb">srun</span><span class="w"> </span>--het-group<span class="o">=</span><span class="m">0</span><span class="w"> </span><span class="si">${</span><span class="nv">SHARED_ARGS</span><span class="si">}</span><span class="w"> </span>--export<span class="o">=</span>all,OMP_NUM_THREADS<span class="o">=</span><span class="m">16</span><span class="w"> </span>./xthi-a<span class="w"> </span>:<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--het-group<span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="si">${</span><span class="nv">SHARED_ARGS</span><span class="si">}</span><span class="w"> </span>--export<span class="o">=</span>all,OMP_NUM_THREADS<span class="o">=</span><span class="m">1</span><span class="w">  </span>./xthi-b
</code></pre></div>
<p>The important point here is that <code>OMP_NUM_THREADS</code> must not be set
in the environment that calls <code>srun</code> in order that the different
specifications for the separate groups via <code>--export</code> on the <code>srun</code>
command line take effect. If <code>OMP_NUM_THREADS</code> is set in the calling
environment, then that value takes precedence, and each component will
see the same value of <code>OMP_NUM_THREADS</code>.</p>
<p>The output might then be:</p>
<div class="highlight"><pre><span></span><code>Node    0, hostname nid001111, mpi   8, omp  16, executable xthi-a
Node    1, hostname nid001126, mpi   8, omp   1, executable xthi-b
Node    0, rank    0, thread   0, (affinity =    0)
Node    0, rank    0, thread   1, (affinity =    1)
Node    0, rank    0, thread   2, (affinity =    2)
Node    0, rank    0, thread   3, (affinity =    3)
Node    0, rank    0, thread   4, (affinity =    4)
Node    0, rank    0, thread   5, (affinity =    5)
Node    0, rank    0, thread   6, (affinity =    6)
Node    0, rank    0, thread   7, (affinity =    7)
Node    0, rank    0, thread   8, (affinity =    8)
Node    0, rank    0, thread   9, (affinity =    9)
Node    0, rank    0, thread  10, (affinity =   10)
Node    0, rank    0, thread  11, (affinity =   11)
Node    0, rank    0, thread  12, (affinity =   12)
Node    0, rank    0, thread  13, (affinity =   13)
Node    0, rank    0, thread  14, (affinity =   14)
Node    0, rank    0, thread  15, (affinity =   15)
Node    0, rank    1, thread   0, (affinity =   16)
Node    0, rank    1, thread   1, (affinity =   17)
...
Node    0, rank    7, thread  14, (affinity =  126)
Node    0, rank    7, thread  15, (affinity =  127)
Node    1, rank    8, thread   0, (affinity =    0)
Node    1, rank    9, thread   0, (affinity =   16)
Node    1, rank   10, thread   0, (affinity =   32)
Node    1, rank   11, thread   0, (affinity =   48)
Node    1, rank   12, thread   0, (affinity =   64)
Node    1, rank   13, thread   0, (affinity =   80)
Node    1, rank   14, thread   0, (affinity =   96)
Node    1, rank   15, thread   0, (affinity =  112)
</code></pre></div>
<p>Here we can see the eight MPI tasks from <code>xthi-a</code> each running with
sixteen OpenMP threads. Then the 8 MPI tasks with no threading from
<code>xthi-b</code> are spaced across the cores on the second node, one per NUMA region.</p>
<h2 id="low-priority-access">Low priority access</h2>
<p>Low priority jobs are not charged against your allocation but will only run when
other, higher-priority, jobs cannot be run. Although low priority jobs are not
charged, you do need a valid, positive budget to be able to submit and run low
priority jobs, i.e. you need at least 1 CU in your budget.</p>
<p>Low priority access is always available and has the following limits:</p>
<ul>
<li>1024 node maximum job size</li>
<li>Maximum 16 low priority jobs submitted (including running) per user</li>
<li>Maximum 16 low priority job running per user</li>
<li>Maximum runtime of 24 hours</li>
</ul>
<p>You submit a low priority job on ARCHER2 by using the <code>lowpriority</code> QoS. For example,
you would usually have the following line in your job submission script sbatch 
options:</p>
<div class="highlight"><pre><span></span><code><span class="kp">#SBATCH --qos=lowpriority</span>
</code></pre></div>
<h2 id="reservations">Reservations</h2>
<p>Reservations are available on ARCHER2. These allow users to reserve a number of nodes
for a specified length of time starting at a particular time on the system.</p>
<p>Reservations require justification. They will only be approved if the request could not
be fulfilled with the normal QoS's. For instance, you require a job/jobs to run at a
particular time e.g. for a demonstration or course.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Reservation requests must be submitted at least 60 hours in advance of the reservation
start time. If requesting a reservation for a Monday at 18:00, please ensure this is
received by the Friday at 12:00 the latest. The same applies over Service Holidays.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Reservations are only valid for standard compute nodes, high memory compute nodes
and/or PP nodes cannot be included in reservations.</p>
</div>
<p>Reservations will be charged at 1.5 times the usual CU rate and our policy is that they
will be charged the full rate for the entire reservation at the time of booking, whether
or not you use the nodes for the full time. In addition, you will not be refunded the
CUs if you fail to use them due to a job issue unless this issue is due to a system failure.</p>
<p>To request a reservation you complete a form on SAFE:</p>
<ol>
<li><a href="https://safe.epcc.ed.ac.uk">Log into SAFE</a></li>
<li>Under the "Login accounts" menu, choose the "Request reservation" option</li>
</ol>
<p>On the first page, you need to provide the following:</p>
<ul>
<li>The start time and date of the reservation.</li>
<li>The end time and date of the reservation.</li>
<li>Your justification for the reservation -- this must be provided or the request will be rejected.</li>
<li>The number of nodes required.</li>
</ul>
<p>On the second page, you will need to specify which username you wish the reservation to be charged against
and, once the username has been selected, the budget you want to charge the reservation to.
(The selected username will be charged for the reservation but the reservation can be used by all members of the selected budget.)</p>
<p>Your request will be checked by the ARCHER2 User Administration team and, if approved, you will be provided a reservation ID which can be used on the system. To submit jobs to a reservation, you need to add <code>--reservation=&lt;reservation ID&gt;</code> and <code>--qos=reservation</code> options to your job submission script or command.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>You must have at least 1 CU in the budget to submit a job on ARCHER2, even to a pre-paid reservation.</p>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>You can submit jobs to a reservation as soon as the reservation has been set up; jobs will remain queued until the reservation starts.</p>
</div>
<h2 id="serial-jobs">Serial jobs</h2>
<p>You can run serial jobs on the shared data analysis nodes. More information
on using the data analysis nodes (including example job submission scripts)
can be found in the <a href="../analysis/">Data Analysis section</a> of the User and Best
Practice Guide.</p>
<h2 id="best-practices-for-job-submission">Best practices for job submission</h2>
<p>This guidance is adapted from <a href="https://docs.nersc.gov/jobs/best-practices/">the advice provided by
NERSC</a></p>
<h3 id="time-limits">Time Limits</h3>
<p>Due to backfill scheduling, short and variable-length jobs generally
start quickly resulting in much better job throughput. You can specify a
minimum time for your job with the <code>--time-min</code> option to SBATCH:</p>
<div class="highlight"><pre><span></span><code><span class="kp">#SBATCH --time-min=&lt;lower_bound&gt;</span>
<span class="kp">#SBATCH --time=&lt;upper_bound&gt;</span>
</code></pre></div>
<p>Within your job script, you can get the time remaining in the job with
<code>squeue -h -j ${Slurm_JOBID} -o %L</code> to allow you to deal with
potentially varying runtimes when using this option.</p>
<h3 id="long-running-jobs">Long Running Jobs</h3>
<p>Simulations which must run for a long period of time achieve the best
throughput when composed of many small jobs using a checkpoint and
restart method chained together (see above for how to chain jobs
together). However, this method does occur a startup and shutdown
overhead for each job as the state is saved and loaded so you should
experiment to find the best balance between runtime (long runtimes
minimise the checkpoint/restart overheads) and throughput (short
runtimes maximise throughput).</p>
<h3 id="interconnect-locality">Interconnect locality</h3>
<p>For jobs which are sensitive to interconnect (MPI) performance and
utilise 128 nodes or less it is possible to request that all nodes
are in a single Slingshot dragonfly group. The maximum number of nodes in
a group on ARCHER2 is 128.</p>
<p>Slurm has a concept of "switches" which on ARCHER2 are configured to map
to Slingshot electrical groups; where all compute nodes have all-to-all
electrical connections which minimises latency. Since this places an
additional constraint on the scheduler a maximum time to wait for the
requested topology can be specified - after this time, the job will be
placed without the constraint.</p>
<p>For example, to specify that all requested nodes should come from one
electrical group and to wait for up to 6 hours (360 minutes) for that
placement, you would use the following option in your job:</p>
<div class="highlight"><pre><span></span><code><span class="kp">#SBATCH --switches=1@360</span>
</code></pre></div>
<p>You can request multiple groups using this option if you are using 
more nodes than are in a single group to maximise the number of
nodes that share electrical connetions in the job. For example, to
request 4 groups (maximum of 512 nodes) and have this as an absolute
constraint with no timeout, you would use:</p>
<div class="highlight"><pre><span></span><code><span class="kp">#SBATCH --switches=4</span>
</code></pre></div>
<div class="admonition danger">
<p class="admonition-title">Danger</p>
<p>When specifying the number of groups take care to request enough
groups to satisfy the requested number of nodes. If the number
is too low then an unneccesary delay will be added due to the
unsatisfiable request.</p>
<p>A useful heuristic to ensure this is the case is to ensure that
the total nodes requested is less than or equal to the number
of groups multiplied by 128.</p>
</div>
<h3 id="large-jobs">Large Jobs</h3>
<p>Large jobs may take longer to start up. The <code>sbcast</code> command is
recommended for large jobs requesting over 1500 MPI tasks. By default,
Slurm reads the executable on the allocated compute nodes from the
location where it is installed; this may take long time when the file
system (where the executable resides) is slow or busy. The <code>sbcast</code>
command, the executable can be copied to the <code>/tmp</code> directory on each of
the compute nodes. Since <code>/tmp</code> is part of the memory on the compute
nodes, it can speed up the job startup time.</p>
<div class="highlight"><pre><span></span><code>sbcast --compress=none /path/to/exe /tmp/exe
srun /tmp/exe
</code></pre></div>
<h3 id="huge-pages">Huge pages</h3>
<p>Huge pages are virtual memory pages which are bigger than the default
page size of 4K bytes. Huge pages can improve memory performance for
common access patterns on large data sets since it helps to reduce the
number of virtual to physical address translations when compared to
using the default 4KB.</p>
<p>To use huge pages for an application (with the 2 MB huge pages as an
example):</p>
<div class="highlight"><pre><span></span><code>module load craype-hugepages2M
cc -o mycode.exe mycode.c
</code></pre></div>
<p>And also load the same huge pages module at runtime.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Due to the huge pages memory fragmentation issue, applications may get
<em>Cannot allocate memory</em> warnings or errors when there are not enough
hugepages on the compute node, such as:</p>
<p>libhugetlbfs [nid0000xx:xxxxx]: WARNING: New heap segment map at 0x10000000 failed: Cannot allocate memory``</p>
</div>
<p>By default, The verbosity level of libhugetlbfs <code>HUGETLB_VERBOSE</code> is set
to <code>0</code> on ARCHER2 to surpress debugging messages. Users can adjust this
value to obtain more information on huge pages use.</p>
<h4 id="when-to-use-huge-pages">When to Use Huge Pages</h4>
<ul>
<li>For MPI applications, map the static data and/or heap onto huge
    pages.</li>
<li>For an application which uses shared memory, which needs to be
    concurrently registered with the high speed network drivers for
    remote communication.</li>
<li>For SHMEM applications, map the static data and/or private heap onto
    huge pages.</li>
<li>For applications written in Unified Parallel C, Coarray Fortran, and
    other languages based on the PGAS programming model, map the static
    data and/or private heap onto huge pages.</li>
<li>For an application doing heavy I/O.</li>
<li>To improve memory performance for common access patterns on large
    data sets.</li>
</ul>
<h4 id="when-to-avoid-huge-pages">When to Avoid Huge Pages</h4>
<ul>
<li>Applications sometimes consist of many steering programs in addition
    to the core application. Applying huge page behavior to all
    processes would not provide any benefit and would consume huge pages
    that would otherwise benefit the core application. The runtime
    environment variable <code>HUGETLB_RESTRICT_EXE</code> can be used to specify
    the susbset of the programs to use hugepages.</li>
<li>For certain applications if using hugepages either causes issues or
    slows down performance. One such example is that when an application
    forks more subprocesses (such as pthreads) and these threads
    allocate memory, the newly allocated memory are the default 4 KB
    pages.</li>
</ul>


  




                
              </article>
            </div>
          
          
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/ARCHER2-HPC" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://twitter.com/ARCHER2_HPC" target="_blank" rel="noopener" title="twitter.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://www.linkedin.com/groups/13848871/" target="_blank" rel="noopener" title="www.linkedin.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["tabs"], "search": "../../assets/javascripts/workers/search.74e28a9f.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.220ee61c.min.js"></script>
      
    
  </body>
</html>